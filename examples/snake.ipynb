{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training DQN Agent on Snake using Jumanji API\n",
        "This notebook is an example of how to use the Jumanji API to train a Deep-RL agent. We will train a [DQN](https://arxiv.org/abs/1312.5602) agent on the `\"Snake-v1\"` Jumanji environment under the Anakin framework. The Anakin design was developed by Matteo Hessel, Manuel Kroiss, Fabio Viola and Hado van Hasselt in [Podracer architectures for scalable Reinforcement Learning](https://arxiv.org/abs/2104.06272).\n",
        "\n",
        "**Aims:**\n",
        "* Understand the basics of the Jumanji API.\n",
        "* Solve the `\"Snake-v1\"` environment with a DQN using the Anakin framework.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Familiarity with [JAX](https://jax.readthedocs.io/en/latest/beginner_guide.html#beginner-guide).\n",
        "* Familiarity with some Deep Reinforcement Learning algorithms. ([DQN Tutorial](https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc)) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAIUh72K5Ws1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip3 install -U pip\n",
        "! pip install --upgrade \"jax[cuda11_local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html #gpu\n",
        "#! pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html # tpu\n",
        "! pip install jaxlib\n",
        "! pip3 install dm-haiku\n",
        "! pip3 install optax \n",
        "! pip3 install matplotlib\n",
        "! pip3 install jumanji\n",
        "! pip3 install tensorflow\n",
        "! pip3 install tensorrt\n",
        "! pip3 install chex\n",
        "! pip3 install rlax\n",
        "! pip3 install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8ML35o5thM",
        "outputId": "a0ff60f5-a882-4b16-dea4-14647bf4ba84"
      },
      "outputs": [],
      "source": [
        "# ensure GPU is not preallocating space\n",
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]='false'\n",
        "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "import jax\n",
        "import warnings\n",
        "\n",
        "accelerator_type = jax.devices()[0].platform\n",
        "\n",
        "if accelerator_type == 'tpu':\n",
        "    # setup TPU\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "    print(\"Running with TPU!\")\n",
        "\n",
        "elif accelerator_type == \"gpu\":\n",
        "    print(\"Running with GPU!\")\n",
        "\n",
        "else:\n",
        "    print(\"Running with CPU!\")\n",
        "    warnings.warn(\n",
        "        \"Running with CPU. We reccomend running this notebook in Colab with TPU enabled,\"\n",
        "        \"or GPU if TPU is not available.\"\n",
        "    )\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import jaxlib\n",
        "from jax.lib import xla_bridge\n",
        "from jax import lax\n",
        "import tensorrt\n",
        "import tensorflow as tf\n",
        "\n",
        "import haiku as hk\n",
        "import optax\n",
        "import rlax\n",
        "import timeit\n",
        "import chex\n",
        "\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "from jumanji.environments.routing.snake import State, Observation, Snake\n",
        "\n",
        "# prevent TensorFlow from allocating GPU memory.\n",
        "tf.config.set_visible_devices([], \"GPU\")\n",
        "\n",
        "# for VScode output\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usage of Jumanji\n",
        "We implement a 10 step episode to illustrate the basic usage of a Jumanji environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fcr6upF-6Cc",
        "outputId": "7998c730-1926-49dc-f926-8fd9aabc3b5e"
      },
      "outputs": [],
      "source": [
        "# initialise environment\n",
        "env = jumanji.make(\"Snake-v1\")\n",
        "num_actions = env.action_spec().num_values\n",
        "\n",
        "# generate PRNG key for randomness\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "# reset the environment and receive the initial state and timestep\n",
        "state, timestep = env.reset(key)\n",
        "\n",
        "# jit environment step function\n",
        "env_step = jax.jit(env.step)\n",
        "\n",
        "# run episode for 10 steps\n",
        "done = False\n",
        "transitions = 0\n",
        "while not done and transitions < 10:\n",
        "\n",
        "    # generate new key\n",
        "    key, subkey = jax.random.split(key)\n",
        "\n",
        "    # select action randomly\n",
        "    action = jax.random.randint(subkey, (), 0, num_actions)\n",
        "\n",
        "    # transition to next state\n",
        "    state, timestep = env_step(state, action)\n",
        "    transitions += 1\n",
        "\n",
        "    # validate action\n",
        "    done = not timestep.discount\n",
        "\n",
        "    # render environment\n",
        "    env.render(state)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Q-Network\n",
        "Here, we define the DQN's architecture and get the `q_values` of some `dummy_observation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpeCf7UQ96ar",
        "outputId": "68bec1fd-f238-41c0-f88e-57b7ceec91c4"
      },
      "outputs": [],
      "source": [
        "def build_dqn(\n",
        "    num_outputs: int,\n",
        "    mlp_units: Tuple[int] = (128,64),\n",
        "    conv_n_channels: int = 32,\n",
        "    time_limit: int = 5000,\n",
        "):\n",
        "    \"\"\"Builds and returns Deep Q-Network\"\"\"\n",
        "\n",
        "    def network_fn(observation: Observation) -> chex.Array:\n",
        "        torso = hk.Sequential(\n",
        "            [\n",
        "                hk.Conv2D(conv_n_channels, (2, 2), 2),\n",
        "                jax.nn.relu,\n",
        "                hk.Conv2D(conv_n_channels, (2, 2), 1),\n",
        "                jax.nn.relu,\n",
        "                hk.Flatten(),\n",
        "            ]\n",
        "        )\n",
        "        flat = hk.Flatten(preserve_dims=-2)\n",
        "\n",
        "        # convolve the grid and flatten the output\n",
        "        embedding = flat(torso(observation.grid))\n",
        "\n",
        "        # concatenate the embedding with the step count\n",
        "        norm_step_count = jnp.expand_dims(observation.step_count / time_limit, axis=-1)\n",
        "        embedding = jnp.concatenate((embedding, norm_step_count), axis=-1)\n",
        "\n",
        "        # pass the embedding through an MLP\n",
        "        head = hk.nets.MLP((*mlp_units, num_outputs), activate_final=False)\n",
        "\n",
        "        if num_outputs == 1:\n",
        "            value = jnp.squeeze(head(embedding), axis=-1)\n",
        "            return value\n",
        "        else:\n",
        "            logits = head(embedding)\n",
        "            logits = jnp.where(\n",
        "                observation.action_mask, logits, jnp.finfo(jnp.float32).min\n",
        "            )\n",
        "            return logits\n",
        "\n",
        "    return hk.without_apply_rng(hk.transform(network_fn))\n",
        "\n",
        "\n",
        "# build DQN and get dummy q-values\n",
        "DQN = build_dqn(num_outputs=num_actions)\n",
        "\n",
        "# initial DQN with a dummy observation\n",
        "initial_parameters = DQN.init(\n",
        "    key, timestep.observation\n",
        ")\n",
        "\n",
        "# jit apply method\n",
        "model_apply = jax.jit(DQN.apply)\n",
        "\n",
        "# get q_values\n",
        "q_values = model_apply(initial_parameters, timestep.observation)\n",
        "\n",
        "print(q_values)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anakin Framework\n",
        "The majority of the code below is copied verbatim from the [Podracer Architectures for Scalable RL](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing#scrollTo=mGSmAiCHJsas).\n",
        "\n",
        "**Additions**\n",
        "* $\\epsilon$-greedy action selection as apposed to a greedy selection\n",
        "* Record metrics such as max and mean episode return per batch\n",
        "* An evaluation step per batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pSq6mIM_2Hg"
      },
      "outputs": [],
      "source": [
        "eval_batch_size = 5\n",
        "\n",
        "# TODO:\n",
        "# env is not used\n",
        "# action_key is not used\n",
        "# where does this eval_env comes from?\n",
        "# add comments in the code\n",
        "\n",
        "def evaluate(env, params: hk.Params, key: chex.PRNGKey):\n",
        "    \"\"\"Evaluates currently policy\"\"\"\n",
        "\n",
        "    def evaluate_one_episode(key: chex.PRNGKey):\n",
        "        def one_step(\n",
        "            state: State,\n",
        "            timestep: TimeStep,\n",
        "            return_: chex.Numeric,\n",
        "            key: chex.PRNGKey,\n",
        "        ):\n",
        "            key, action_key = jax.random.split(key)\n",
        "            action = jnp.argmax(model_apply(params, timestep.observation))\n",
        "            state, timestep = eval_env.step(state, action)\n",
        "            return_ += timestep.reward\n",
        "            return state, timestep, return_, key\n",
        "\n",
        "        state, timestep = eval_env.reset(key)\n",
        "        *_, return_, _ = jax.lax.while_loop(\n",
        "            lambda carry: ~carry[1].last(),\n",
        "            lambda carry: one_step(*carry),\n",
        "            (state, timestep, jnp.array(0, float), key),\n",
        "        )\n",
        "        return return_\n",
        "\n",
        "    keys = jax.random.split(key, eval_batch_size)\n",
        "    returns = jax.vmap(evaluate_one_episode)(keys)\n",
        "    return jnp.mean(returns)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define useful containers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeIt:\n",
        "    def __init__(self, tag, frames=None):\n",
        "        self.tag = tag\n",
        "        self.frames = frames\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = timeit.default_timer()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.elapsed_secs = timeit.default_timer() - self.start\n",
        "        msg = self.tag + (\": Elapsed time=%.2fs\" % self.elapsed_secs)\n",
        "        if self.frames:\n",
        "            msg += \", FPS=%.2e\" % (self.frames / self.elapsed_secs)\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class TimeStep:\n",
        "    q_values: chex.Array\n",
        "    action: chex.Array\n",
        "    discount: chex.Array\n",
        "    reward: chex.Array\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class EpisodeMetrics:\n",
        "    \"\"\"Metrics that we use to keep track of the episode return and length\n",
        "    throughout interation with the environment.\"\"\"\n",
        "\n",
        "    episode_return: chex.Array\n",
        "    episode_length: chex.Array\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Learner Function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method `get_learner_fn` returns a method `learner_fn` which:\n",
        "* Collects a batch of experiences from a trajectory while recording any useful information. \n",
        "* Computes the squared TD($\\lambda$) loss of the batch of experiences.\n",
        "* Computes the gradient of the loss with respect to the network's parameters.\n",
        "* Aggregates the gradient across a batch of trajectories, and across multiple devices (if using TPU).\n",
        "* Updates the network's parameters with the Adam optimizer.\n",
        "* Runs this update step multiple times without going back to python. The original Anakin implementation uses `jax.lax.fori` however we would like to obtain information from each step of the algoirthm. Therefore, we use `jax.lax.scan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smY4U05D-V2u",
        "outputId": "89a621e5-5515-41fd-acad-4edd9c1df3eb"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# add typing\n",
        "# add comments in the code\n",
        "# rename rng to key for consistency\n",
        "# make sure to be consistent in the way your import modules\n",
        "# here, use jax.randon, not random directly\n",
        "\n",
        "def get_learner_fn(\n",
        "    env_step,\n",
        "    forward_pass,\n",
        "    opt_update,\n",
        "    rollout_len,\n",
        "    agent_discount,\n",
        "    lambda_,\n",
        "    iterations,\n",
        "    epsilon,\n",
        "):\n",
        "    \"\"\"Define the minimal unit of computation in Anakin.\"\"\"\n",
        "\n",
        "    def loss_fn(params, outer_rng, env_state, env_timestep, episode_metrics):\n",
        "        \"\"\"Compute the loss on a single trajectory.\"\"\"\n",
        "\n",
        "        def step_fn(carry, rng):\n",
        "            env_state, env_timestep, episode_metrics = carry\n",
        "\n",
        "            q_values = forward_pass(params, env_timestep.observation)\n",
        "            action = rlax.epsilon_greedy(epsilon).sample(rng, q_values)\n",
        "            next_env_state, next_env_timestep = env_step(env_state, action)\n",
        "            reward = next_env_timestep.reward\n",
        "            discount = next_env_timestep.discount\n",
        "\n",
        "            info = {\n",
        "                \"episode_return\": jnp.where(\n",
        "                    discount == 0.0, reward + episode_metrics.episode_return, jnp.nan\n",
        "                ),\n",
        "                \"episode_length\": jnp.where(\n",
        "                    discount == 0.0, 1 + episode_metrics.episode_length, jnp.nan\n",
        "                ),\n",
        "            }\n",
        "\n",
        "            episode_metrics = EpisodeMetrics(\n",
        "                episode_return=jnp.where(\n",
        "                    discount == 0.0,\n",
        "                    jnp.array(0.0),\n",
        "                    reward + episode_metrics.episode_return,\n",
        "                ),\n",
        "                episode_length=jnp.where(\n",
        "                    discount == 0.0, jnp.array(0), 1 + episode_metrics.episode_length\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            carry = next_env_state, next_env_timestep, episode_metrics\n",
        "            return carry, (\n",
        "                TimeStep(\n",
        "                    q_values=q_values, action=action, discount=discount, reward=reward\n",
        "                ),\n",
        "                info,\n",
        "            )\n",
        "\n",
        "        step_rngs = random.split(outer_rng, rollout_len)\n",
        "        (env_state, env_timestep, episode_metrics), (rollout, info) = lax.scan(\n",
        "            step_fn, (env_state, env_timestep, episode_metrics), step_rngs\n",
        "        )  # trajectory.\n",
        "\n",
        "        qa_tm1 = rlax.batched_index(rollout.q_values[:-1], rollout.action[:-1])\n",
        "\n",
        "        # compute multi-step temporal diff error\n",
        "        td_error = rlax.td_lambda(\n",
        "            v_tm1=qa_tm1,  # predictions\n",
        "            r_t=rollout.reward[1:],  # rewards\n",
        "            discount_t=agent_discount * rollout.discount[1:],  # discount\n",
        "            v_t=jnp.max(rollout.q_values[1:], axis=-1),  # bootstrap values\n",
        "            lambda_=lambda_, # mixing hyper-parameter lambda\n",
        "        )\n",
        "\n",
        "        info = {\n",
        "            \"episode_return\": jnp.nanmean(info[\"episode_return\"], axis=0),\n",
        "            \"episode_length\": jnp.nanmean(info[\"episode_length\"], axis=0),\n",
        "            \"reward\": jnp.mean(rollout.reward),\n",
        "            \"q_values\": jnp.mean(qa_tm1),\n",
        "            \"max_episode_return\": jnp.nanmax(info[\"episode_return\"], axis=0),\n",
        "        }\n",
        "\n",
        "        return jnp.mean(td_error**2), (env_state, env_timestep, episode_metrics, info)\n",
        "\n",
        "    def update_fn(params, opt_state, rng, env_state, env_timestep, episode_metrics):\n",
        "        \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
        "\n",
        "        rng, loss_rng = random.split(rng)\n",
        "        grads, (new_env_state, new_env_timestep, episode_metrics, info) = jax.grad(\n",
        "            loss_fn, has_aux=True\n",
        "        )(\n",
        "            params, loss_rng, env_state, env_timestep, episode_metrics\n",
        "        )  # compute grad for single traj.\n",
        "\n",
        "        grads = lax.pmean(grads, axis_name=\"j\")  # reduce mean across cores.\n",
        "        grads = lax.pmean(grads, axis_name=\"i\")  # reduce mean across batch.\n",
        "\n",
        "        updates, new_opt_state = opt_update(grads, opt_state)  # transform grads.\n",
        "        new_params = optax.apply_updates(params, updates)  # update parameters.\n",
        "\n",
        "        return (\n",
        "            new_params,\n",
        "            new_opt_state,\n",
        "            rng,\n",
        "            new_env_state,\n",
        "            new_env_timestep,\n",
        "            episode_metrics,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def learner_fn(params, opt_state, rngs, env_states, env_timestep, episode_metrics):\n",
        "        \"\"\"Vectorise and repeat the update.\"\"\"\n",
        "        batched_update_fn = jax.vmap(\n",
        "            update_fn, axis_name=\"j\"\n",
        "        )  # vectorize across batch.\n",
        "\n",
        "        def iterate_fn(carry, _):  # repeat many times to avoid going back to Python.\n",
        "            params, opt_state, rngs, env_states, env_timestep, episode_metrics = carry\n",
        "            (\n",
        "                new_params,\n",
        "                new_opt_state,\n",
        "                rng,\n",
        "                new_env_state,\n",
        "                new_env_timestep,\n",
        "                episode_metrics,\n",
        "                info,\n",
        "            ) = batched_update_fn(\n",
        "                params, opt_state, rngs, env_states, env_timestep, episode_metrics\n",
        "            )\n",
        "            carry = (\n",
        "                new_params,\n",
        "                new_opt_state,\n",
        "                rng,\n",
        "                new_env_state,\n",
        "                new_env_timestep,\n",
        "                episode_metrics,\n",
        "            )\n",
        "\n",
        "            max_return_info = {\n",
        "                \"max_episode_return\": jnp.nanmax(info[\"max_episode_return\"], axis=0)\n",
        "            }\n",
        "\n",
        "            info = jax.tree_util.tree_map(lambda x: jnp.nanmean(x, axis=0), info)\n",
        "            info.update(max_return_info)\n",
        "            return carry, info\n",
        "\n",
        "        init = params, opt_state, rngs, env_states, env_timestep, episode_metrics\n",
        "\n",
        "        return jax.lax.scan(iterate_fn, init, xs=None, length=iterations)\n",
        "\n",
        "    return learner_fn\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define experiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a set of hyper-parameters, `run_experiment` initialises and executes the experiment. The experiement runs multiped pmapped iterations of `learner_fn` until the desired number of total training iterations is reached. `run_experiment` returns the recorded training information and the most recently updated network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myEDYuYAD9tp"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# add typing\n",
        "# epsilon and time_limit are not used\n",
        "# comments should be the line before when possible (not on the side)\n",
        "# d_state is not used\n",
        "# use \"key\" rather than \"rng\" for consistency\n",
        "# add space and comments\n",
        "\n",
        "\n",
        "def run_experiment(\n",
        "    env,\n",
        "    eval_env,\n",
        "    batch_size,\n",
        "    rollout_len,\n",
        "    step_size,\n",
        "    iterations,\n",
        "    discount_factor,\n",
        "    epsilon,\n",
        "    seed,\n",
        "    time_limit,\n",
        "):\n",
        "    \"\"\"Runs experiment.\"\"\"\n",
        "    cores_count = len(jax.devices())  # get available TPU cores.\n",
        "    network = build_dqn(env.action_spec().num_values)  # define network.\n",
        "\n",
        "    inner_iter_length = 100\n",
        "\n",
        "    rng, rng_e, rng_p, rng_eval = random.split(\n",
        "        random.PRNGKey(seed), num=4\n",
        "    )  # prng keys.\n",
        "\n",
        "    d_state, d_timestep = env.reset(rng_e)\n",
        "    dummy_obs = d_timestep.observation  # dummy for net init.\n",
        "\n",
        "    env_step = jax.jit(env.step)\n",
        "\n",
        "    params = network.init(rng_p, dummy_obs)  # initialise params.\n",
        "\n",
        "    optim = optax.adam(step_size)  # define optimiser.\n",
        "    opt_state = optim.init(params)  # initialise optimiser stats.\n",
        "\n",
        "    learn = get_learner_fn(  # get batched iterated update.\n",
        "        env_step,\n",
        "        jax.jit(network.apply),\n",
        "        optim.update,\n",
        "        rollout_len=rollout_len,\n",
        "        agent_discount=discount_factor,\n",
        "        lambda_=0.95,\n",
        "        iterations=inner_iter_length,\n",
        "        epsilon=0.02,\n",
        "    )\n",
        "\n",
        "    learn = jax.pmap(learn, axis_name=\"i\")  # replicate over multiple cores\n",
        "\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (cores_count, batch_size) + x.shape)\n",
        "    params = jax.tree_map(broadcast, params)  # broadcast to cores and batch\n",
        "    opt_state = jax.tree_map(broadcast, opt_state)  # broadcast to cores and batch\n",
        "\n",
        "    episode_metrics = EpisodeMetrics(\n",
        "        episode_return=jnp.zeros(shape=(), dtype=jnp.float32),\n",
        "        episode_length=jnp.zeros(shape=(), dtype=jnp.int16),\n",
        "    )\n",
        "\n",
        "    episode_metrics = jax.tree_map(broadcast, episode_metrics)\n",
        "\n",
        "    rng, *env_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
        "    env_states, env_timesteps = jax.vmap(env.reset)(jnp.stack(env_rngs))  # init envs.\n",
        "    rng, *step_rngs = jax.random.split(rng, cores_count * batch_size + 1)\n",
        "\n",
        "    reshape = lambda x: jax.tree_util.tree_map(\n",
        "        lambda x: x.reshape((cores_count, batch_size) + x.shape[1:]), x\n",
        "    )\n",
        "\n",
        "    step_rngs = reshape(jnp.stack(step_rngs))  # add dimension to pmap over.\n",
        "    env_states = reshape(env_states)  # add dimension to pmap over.\n",
        "    env_timesteps = reshape(env_timesteps)\n",
        "\n",
        "    num_frames_compile = cores_count * inner_iter_length * rollout_len * batch_size\n",
        "    with TimeIt(tag=\"COMPILATION\", frames=num_frames_compile):\n",
        "        learn(\n",
        "            params, opt_state, step_rngs, env_states, env_timesteps, episode_metrics\n",
        "        )  # compiles\n",
        "\n",
        "    num_frames = cores_count * iterations * rollout_len * batch_size\n",
        "    n_outer_iter = int(iterations // inner_iter_length)\n",
        "    with TimeIt(tag=\"EXECUTION\", frames=num_frames):\n",
        "        for i in tqdm(range(n_outer_iter)):\n",
        "            (\n",
        "                params,\n",
        "                opt_state,\n",
        "                step_rngs,\n",
        "                env_states,\n",
        "                env_timesteps,\n",
        "                episode_metrics,\n",
        "            ), new_info = learn(\n",
        "                params, opt_state, step_rngs, env_states, env_timesteps, episode_metrics\n",
        "            )\n",
        "\n",
        "            max_return_info = {\n",
        "                \"max_episode_return\": jnp.nanmax(new_info[\"max_episode_return\"], axis=0)\n",
        "            }\n",
        "            new_info = jax.tree_util.tree_map(\n",
        "                lambda x: jnp.nanmean(x, axis=0), new_info\n",
        "            )\n",
        "            new_info.update(max_return_info)\n",
        "\n",
        "            # Evaluate and add to info\n",
        "            params_single_device = jax.tree_util.tree_map(lambda x: x[0, 0], params)\n",
        "\n",
        "            eval_return = evaluate(eval_env, params_single_device, rng_eval)\n",
        "\n",
        "            eval_return_info = {\"eval_return\": jnp.array([eval_return])}\n",
        "            new_info.update(eval_return_info)\n",
        "\n",
        "            if i == 0:\n",
        "                info = new_info\n",
        "            else:\n",
        "                info = {\n",
        "                    key: jnp.concatenate((info[key], new_info[key]))\n",
        "                    for key in info.keys()\n",
        "                }\n",
        "\n",
        "    params_single_device = jax.tree_util.tree_map(lambda x: x[0, 0], params)\n",
        "    return info, params_single_device\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Experiment and Visualise Results \n",
        "We first choose our set of hyper-parameters and call `run_experiment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf6r_IkAIKDO",
        "outputId": "0bc10958-ba18-4cc2-8277-61495ca658ae"
      },
      "outputs": [],
      "source": [
        "env = AutoResetWrapper(jumanji.make(\"Snake-v1\"))\n",
        "eval_env = jumanji.make(\"Snake-v1\")\n",
        "\n",
        "info, params = run_experiment(\n",
        "    env,\n",
        "    eval_env,\n",
        "    batch_size=256,\n",
        "    rollout_len=12,\n",
        "    step_size=2e-4,\n",
        "    iterations=60_000,\n",
        "    discount_factor=0.997,\n",
        "    epsilon=0.1,\n",
        "    seed=0,\n",
        "    time_limit=5000,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V30Cc_hdQqT2"
      },
      "outputs": [],
      "source": [
        "plt.plot(info[\"max_episode_return\"])\n",
        "plt.title(\"max episode return per batch\")\n",
        "plt.xlabel(\"training iteration\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(info[\"episode_return\"])\n",
        "plt.title(\"mean episode return per batch\")\n",
        "plt.xlabel(\"training iteration\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(info[\"eval_return\"])\n",
        "plt.title(\"evaluation return per batch\")\n",
        "plt.xlabel(\"outer training iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollout Agent\n",
        "We let the agent act greedily throughout an episode and visualise its behaviour.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "env = jumanji.make(\"Snake-v1\")\n",
        "state, timestep = env.reset(key)\n",
        "env_step = jax.jit(env.step)\n",
        "\n",
        "DQN = build_dqn(env.action_spec().num_values)\n",
        "initial_parameters = DQN.init(key, timestep.observation)\n",
        "policy = jax.jit(DQN.apply)\n",
        "\n",
        "done = False\n",
        "transitions = 0\n",
        "while not done and transitions < 10:\n",
        "\n",
        "    # select action greedily\n",
        "    action = jnp.argmax(policy(params, timestep.observation))\n",
        "\n",
        "    # take a step in the environment\n",
        "    state, timestep = env_step(state, action)\n",
        "\n",
        "    transitions += 1\n",
        "\n",
        "    done = not timestep.discount\n",
        "\n",
        "    env.render(state)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: add a little summary of what was done and optionally an invitation to look at the full documentation, and potentially a take home message."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "9afa828456cfb0658300d7120d39dda85b3a9b38362c4ce9e8ea6450f74bb614"
    },
    "kernelspec": {
      "display_name": "Python 3.10.11 ('jumanji_notebook')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
