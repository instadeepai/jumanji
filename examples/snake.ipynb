{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training DQN Agent on Snake using Jumanji API\n",
        "This notebook is an example of how to use the Jumanji API to train a Deep-RL agent. We will train a [DQN](https://arxiv.org/abs/1312.5602) agent on the `\"Snake-v1\"` Jumanji environment under the Anakin framework. The Anakin design was developed by Matteo Hessel, Manuel Kroiss, Fabio Viola and Hado van Hasselt in [Podracer architectures for scalable Reinforcement Learning](https://arxiv.org/abs/2104.06272).\n",
        "\n",
        "**Aims:**\n",
        "* Understand the basics of the Jumanji API.\n",
        "* Solve the `\"Snake-v1\"` environment with a DQN using the Anakin framework.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Familiarity with [JAX](https://jax.readthedocs.io/en/latest/beginner_guide.html#beginner-guide).\n",
        "* Familiarity with some Deep Reinforcement Learning algorithms. ([DQN Tutorial](https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc)) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAIUh72K5Ws1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip3 install -U pip\n",
        "\n",
        "# if using a GPU\n",
        "! pip install --upgrade \"jax[cuda11_local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# if using a TPU \n",
        "#! pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html # tpu\n",
        "\n",
        "! pip install jaxlib\n",
        "! pip3 install dm-haiku\n",
        "! pip3 install optax \n",
        "! pip3 install matplotlib\n",
        "! pip3 install jumanji\n",
        "! pip3 install tensorflow\n",
        "! pip3 install tensorrt\n",
        "! pip3 install chex\n",
        "! pip3 install rlax\n",
        "! pip3 install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8ML35o5thM",
        "outputId": "a0ff60f5-a882-4b16-dea4-14647bf4ba84"
      },
      "outputs": [],
      "source": [
        "# ensure GPU is not preallocating space\n",
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]='false'\n",
        "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n",
        "\n",
        "import jax\n",
        "import warnings\n",
        "\n",
        "accelerator_type = jax.devices()[0].platform\n",
        "\n",
        "if accelerator_type == 'tpu':\n",
        "    # setup TPU\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "    print(\"Running with TPU!\")\n",
        "\n",
        "elif accelerator_type == \"gpu\":\n",
        "    print(\"Running with GPU!\")\n",
        "\n",
        "else:\n",
        "    print(\"Running with CPU!\")\n",
        "    warnings.warn(\n",
        "        \"Running with CPU. We reccomend running this notebook in Colab with TPU enabled,\"\n",
        "        \"or GPU if TPU is not available.\"\n",
        "    )\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jaxlib\n",
        "from jax.lib import xla_bridge\n",
        "from jax import lax\n",
        "import tensorrt\n",
        "import tensorflow as tf\n",
        "\n",
        "import haiku as hk\n",
        "import optax\n",
        "import rlax\n",
        "import timeit\n",
        "import chex\n",
        "\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from typing import Sequence, Tuple, Callable, Any\n",
        "\n",
        "import jumanji\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "from jumanji.environments.routing.snake import State, Observation, Snake\n",
        "from jumanji.types import TimeStep\n",
        "\n",
        "# prevent TensorFlow from allocating GPU memory\n",
        "tf.config.set_visible_devices([], \"GPU\")\n",
        "\n",
        "# for VScode output\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usage of Jumanji\n",
        "We implement a 10 step episode to illustrate the basic usage of a Jumanji environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fcr6upF-6Cc",
        "outputId": "7998c730-1926-49dc-f926-8fd9aabc3b5e"
      },
      "outputs": [],
      "source": [
        "# initialise environment\n",
        "env = jumanji.make(\"Snake-v1\")\n",
        "num_actions = env.action_spec().num_values\n",
        "\n",
        "# generate PRNG key for randomness\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "# reset the environment and receive the initial state and timestep\n",
        "state, timestep = env.reset(key)\n",
        "\n",
        "# jit environment step function\n",
        "env_step = jax.jit(env.step)\n",
        "\n",
        "# run episode for 10 steps\n",
        "done = False\n",
        "transitions = 0\n",
        "while not done and transitions < 10:\n",
        "    # generate new key\n",
        "    key, _ = jax.random.split(key)\n",
        "\n",
        "    # select action randomly\n",
        "    action = jax.random.randint(key, (), 0, num_actions)\n",
        "\n",
        "    # transition to next state\n",
        "    state, timestep = env_step(state, action)\n",
        "    transitions += 1\n",
        "\n",
        "    # validate action\n",
        "    done = not timestep.discount\n",
        "\n",
        "    # render environment\n",
        "    env.render(state)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Q-Network\n",
        "Here, we define the DQN's architecture and get the `q_values` of some `dummy_observation`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpeCf7UQ96ar",
        "outputId": "68bec1fd-f238-41c0-f88e-57b7ceec91c4"
      },
      "outputs": [],
      "source": [
        "def build_dqn(\n",
        "    num_outputs: int,\n",
        "    mlp_units: Tuple[int] = (128, 64),\n",
        "    conv_n_channels: int = 32,\n",
        "    time_limit: int = 5000,\n",
        "):\n",
        "    \"\"\"Builds and returns Deep Q-Network\"\"\"\n",
        "\n",
        "    def network_fn(observation: Observation) -> chex.Array:\n",
        "        torso = hk.Sequential(\n",
        "            [\n",
        "                hk.Conv2D(conv_n_channels, (3, 3), 3),\n",
        "                jax.nn.relu,\n",
        "                hk.Conv2D(conv_n_channels, (2, 2), 2),\n",
        "                jax.nn.relu,\n",
        "                hk.Conv2D(conv_n_channels, (2, 2), 1),\n",
        "                jax.nn.relu,\n",
        "                hk.Flatten(),\n",
        "            ]\n",
        "        )\n",
        "        flat = hk.Flatten(preserve_dims=-2)\n",
        "\n",
        "        # convolve the grid and flatten the output\n",
        "        embedding = flat(torso(observation.grid))\n",
        "\n",
        "        # concatenate the embedding with the step count\n",
        "        norm_step_count = jnp.expand_dims(observation.step_count / time_limit, axis=-1)\n",
        "        embedding = jnp.concatenate((embedding, norm_step_count), axis=-1)\n",
        "\n",
        "        # pass the embedding through an MLP\n",
        "        head = hk.nets.MLP((*mlp_units, num_outputs), activate_final=False)\n",
        "\n",
        "        if num_outputs == 1:\n",
        "            value = jnp.squeeze(head(embedding), axis=-1)\n",
        "            return value\n",
        "        else:\n",
        "            logits = head(embedding)\n",
        "            logits = jnp.where(\n",
        "                observation.action_mask, logits, jnp.finfo(jnp.float32).min\n",
        "            )\n",
        "            return logits\n",
        "\n",
        "    return hk.without_apply_rng(hk.transform(network_fn))\n",
        "\n",
        "\n",
        "# build DQN and get dummy q-values\n",
        "DQN = build_dqn(num_outputs=num_actions)\n",
        "\n",
        "# initial DQN with a dummy observation\n",
        "initial_parameters = DQN.init(key, timestep.observation)\n",
        "\n",
        "# jit apply method\n",
        "model_apply = jax.jit(DQN.apply)\n",
        "\n",
        "# get q_values\n",
        "q_values = model_apply(initial_parameters, timestep.observation)\n",
        "\n",
        "print(q_values)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Anakin Framework\n",
        "The majority of the code below is copied verbatim from the [Podracer Architectures for Scalable RL](https://colab.research.google.com/drive/1974D-qP17fd5mLxy6QZv-ic4yxlPJp-G?usp=sharing#scrollTo=mGSmAiCHJsas).\n",
        "\n",
        "**Additions**\n",
        "* $\\epsilon$-greedy action selection as apposed to a greedy selection\n",
        "* Record metrics such as max and mean episode return per batch\n",
        "* An evaluation step per batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pSq6mIM_2Hg"
      },
      "outputs": [],
      "source": [
        "def get_evaluator(\n",
        "    forward_pass: Callable,\n",
        "    eval_env_reset: Callable,\n",
        "    eval_env_step: Callable,\n",
        "    key: chex.PRNGKey,\n",
        "    eval_batch_size: int = 5,\n",
        "):\n",
        "    \"\"\"Returns method to evaluate currently policy\"\"\"\n",
        "\n",
        "    def evaluate(params: hk.Params):\n",
        "        def evaluate_one_episode(key: chex.PRNGKey):\n",
        "            def one_step(\n",
        "                state: State,\n",
        "                timestep: TimeStep,\n",
        "                return_: chex.Numeric,\n",
        "            ):\n",
        "                action = jnp.argmax(forward_pass(params, timestep.observation))\n",
        "                state, timestep = eval_env_step(state, action)\n",
        "                return_ += timestep.reward\n",
        "                return state, timestep, return_\n",
        "\n",
        "            # reset environment and execute a single episode\n",
        "            key, _ = jax.random.split(key)\n",
        "            state, timestep = eval_env_reset(key)\n",
        "            *_, return_ = jax.lax.while_loop(\n",
        "                lambda carry: ~carry[1].last(),\n",
        "                lambda carry: one_step(*carry),\n",
        "                (state, timestep, jnp.array(0, float)),\n",
        "            )\n",
        "            return return_\n",
        "\n",
        "        # vmap to evaluate over batch of episodes\n",
        "        keys = jax.random.split(key, eval_batch_size)\n",
        "        returns = jax.vmap(evaluate_one_episode)(keys)\n",
        "        return jnp.mean(returns)\n",
        "\n",
        "    return evaluate\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define useful containers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeIt:\n",
        "    def __init__(self, tag, frames=None):\n",
        "        self.tag = tag\n",
        "        self.frames = frames\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = timeit.default_timer()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        self.elapsed_secs = timeit.default_timer() - self.start\n",
        "        msg = self.tag + (\": Elapsed time=%.2fs\" % self.elapsed_secs)\n",
        "        if self.frames:\n",
        "            msg += \", FPS=%.2e\" % (self.frames / self.elapsed_secs)\n",
        "        print(msg)\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Experience:\n",
        "    q_values: chex.Array\n",
        "    action: chex.Array\n",
        "    discount: chex.Array\n",
        "    reward: chex.Array\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class EpisodeMetrics:\n",
        "    \"\"\"Metrics that we use to keep track of the episode return and length\n",
        "    throughout interation with the environment.\"\"\"\n",
        "\n",
        "    episode_return: chex.Array\n",
        "    episode_length: chex.Array\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Learner Function"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method `get_learner_fn` returns a method `learner_fn` which:\n",
        "* Collects a batch of experiences from a trajectory while recording any useful information. \n",
        "* Computes the squared TD($\\lambda$) loss of the batch of experiences.\n",
        "* Computes the gradient of the loss with respect to the network's parameters.\n",
        "* Aggregates the gradient across a batch of trajectories, and across multiple devices (if using TPU).\n",
        "* Updates the network's parameters with the Adam optimizer.\n",
        "* Runs this update step multiple times without going back to python. The original Anakin implementation uses `jax.lax.fori` however we would like to obtain information from each step of the algoirthm. Therefore, we use `jax.lax.scan`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smY4U05D-V2u",
        "outputId": "89a621e5-5515-41fd-acad-4edd9c1df3eb"
      },
      "outputs": [],
      "source": [
        "def get_learner_fn(\n",
        "    env_step: Callable,\n",
        "    forward_pass: Callable,\n",
        "    opt_update: Callable,\n",
        "    rollout_len: int,\n",
        "    agent_discount: float,\n",
        "    lambda_: float,\n",
        "    iterations: int,\n",
        "    epsilon: float,\n",
        "):\n",
        "    \"\"\"Define the minimal unit of computation in Anakin.\"\"\"\n",
        "\n",
        "    def loss_fn(\n",
        "        params: hk.Params,\n",
        "        outer_key: chex.PRNGKey,\n",
        "        env_state: State,\n",
        "        env_timestep: TimeStep,\n",
        "        episode_metrics: EpisodeMetrics,\n",
        "    ):\n",
        "        \"\"\"Compute the loss on a single trajectory.\"\"\"\n",
        "\n",
        "        def step_fn(carry, key: chex.PRNGKey):\n",
        "            env_state, env_timestep, episode_metrics = carry\n",
        "\n",
        "            q_values = forward_pass(params, env_timestep.observation)\n",
        "            action = rlax.epsilon_greedy(epsilon).sample(key, q_values)\n",
        "            next_env_state, next_env_timestep = env_step(env_state, action)\n",
        "            reward = next_env_timestep.reward\n",
        "            discount = next_env_timestep.discount\n",
        "\n",
        "            info = {\n",
        "                \"episode_return\": jnp.where(\n",
        "                    discount == 0.0, reward + episode_metrics.episode_return, jnp.nan\n",
        "                ),\n",
        "                \"episode_length\": jnp.where(\n",
        "                    discount == 0.0, 1 + episode_metrics.episode_length, jnp.nan\n",
        "                ),\n",
        "            }\n",
        "\n",
        "            episode_metrics = EpisodeMetrics(\n",
        "                episode_return=jnp.where(\n",
        "                    discount == 0.0,\n",
        "                    jnp.array(0.0),\n",
        "                    reward + episode_metrics.episode_return,\n",
        "                ),\n",
        "                episode_length=jnp.where(\n",
        "                    discount == 0.0, jnp.array(0), 1 + episode_metrics.episode_length\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            carry = next_env_state, next_env_timestep, episode_metrics\n",
        "            return carry, (\n",
        "                Experience(\n",
        "                    q_values=q_values, action=action, discount=discount, reward=reward\n",
        "                ),\n",
        "                info,\n",
        "            )\n",
        "\n",
        "        # trajectory\n",
        "        step_keys = jax.random.split(outer_key, rollout_len)\n",
        "        (env_state, env_timestep, episode_metrics), (rollout, info) = lax.scan(\n",
        "            step_fn, (env_state, env_timestep, episode_metrics), step_keys\n",
        "        )\n",
        "\n",
        "        qa_tm1 = rlax.batched_index(rollout.q_values[:-1], rollout.action[:-1])\n",
        "\n",
        "        # compute multi-step temporal difference error\n",
        "        td_error = rlax.td_lambda(\n",
        "            v_tm1=qa_tm1,  # predictions\n",
        "            r_t=rollout.reward[1:],  # rewards\n",
        "            discount_t=agent_discount * rollout.discount[1:],  # discount\n",
        "            v_t=jnp.max(rollout.q_values[1:], axis=-1),  # bootstrap values\n",
        "            lambda_=lambda_,  # mixing hyper-parameter lambda\n",
        "        )\n",
        "\n",
        "        # log info\n",
        "        info = {\n",
        "            \"episode_return\": jnp.nanmean(info[\"episode_return\"], axis=0),\n",
        "            \"episode_length\": jnp.nanmean(info[\"episode_length\"], axis=0),\n",
        "            \"reward\": jnp.mean(rollout.reward),\n",
        "            \"q_values\": jnp.mean(qa_tm1),\n",
        "            \"max_episode_return\": jnp.nanmax(info[\"episode_return\"], axis=0),\n",
        "        }\n",
        "\n",
        "        return jnp.mean(td_error**2), (env_state, env_timestep, episode_metrics, info)\n",
        "\n",
        "    def update_fn(\n",
        "        params: hk.Params,\n",
        "        opt_state: Tuple,\n",
        "        key: chex.PRNGKey,\n",
        "        env_state: State,\n",
        "        env_timestep: TimeStep,\n",
        "        episode_metrics: EpisodeMetrics,\n",
        "    ):\n",
        "        \"\"\"Compute a gradient update from a single trajectory.\"\"\"\n",
        "\n",
        "        # compute grad for single trajectory\n",
        "        key, loss_key = jax.random.split(key)\n",
        "        grads, (new_env_state, new_env_timestep, episode_metrics, info) = jax.grad(\n",
        "            loss_fn, has_aux=True\n",
        "        )(params, loss_key, env_state, env_timestep, episode_metrics)\n",
        "        # reduce mean across cores and batch\n",
        "        grads = lax.pmean(grads, axis_name=\"j\")\n",
        "        grads = lax.pmean(grads, axis_name=\"i\")\n",
        "\n",
        "        # transform grads\n",
        "        updates, new_opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "        # update parameters\n",
        "        new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "        return (\n",
        "            new_params,\n",
        "            new_opt_state,\n",
        "            key,\n",
        "            new_env_state,\n",
        "            new_env_timestep,\n",
        "            episode_metrics,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def learner_fn(\n",
        "        params: hk.Params,\n",
        "        opt_state: Tuple,\n",
        "        keys: chex.PRNGKey,\n",
        "        env_states: State,\n",
        "        env_timestep: TimeStep,\n",
        "        episode_metrics: EpisodeMetrics,\n",
        "    ):\n",
        "        \"\"\"Vectorise and repeat the update.\"\"\"\n",
        "        # vectorize across batch\n",
        "        batched_update_fn = jax.vmap(update_fn, axis_name=\"j\")\n",
        "\n",
        "        def iterate_fn(carry, _):\n",
        "            params, opt_state, keys, env_states, env_timestep, episode_metrics = carry\n",
        "            (\n",
        "                new_params,\n",
        "                new_opt_state,\n",
        "                key,\n",
        "                new_env_state,\n",
        "                new_env_timestep,\n",
        "                episode_metrics,\n",
        "                info,\n",
        "            ) = batched_update_fn(\n",
        "                params, opt_state, keys, env_states, env_timestep, episode_metrics\n",
        "            )\n",
        "            carry = (\n",
        "                new_params,\n",
        "                new_opt_state,\n",
        "                key,\n",
        "                new_env_state,\n",
        "                new_env_timestep,\n",
        "                episode_metrics,\n",
        "            )\n",
        "\n",
        "            # update info\n",
        "            max_return_info = {\n",
        "                \"max_episode_return\": jnp.nanmax(info[\"max_episode_return\"], axis=0)\n",
        "            }\n",
        "            info = jax.tree_util.tree_map(lambda x: jnp.nanmean(x, axis=0), info)\n",
        "            info.update(max_return_info)\n",
        "\n",
        "            return carry, info\n",
        "\n",
        "        init = params, opt_state, keys, env_states, env_timestep, episode_metrics\n",
        "\n",
        "        return jax.lax.scan(iterate_fn, init, xs=None, length=iterations)\n",
        "\n",
        "    return learner_fn\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define experiment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a set of hyper-parameters, `run_experiment` initialises and executes the experiment. The experiement runs multiped pmapped iterations of `learner_fn` until the desired number of total training iterations is reached. `run_experiment` returns the recorded training information and the most recently updated network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myEDYuYAD9tp"
      },
      "outputs": [],
      "source": [
        "def run_experiment(\n",
        "    env,\n",
        "    eval_env,\n",
        "    batch_size: int,\n",
        "    rollout_len: int,\n",
        "    step_size: float,\n",
        "    iterations: int,\n",
        "    discount_factor: float,\n",
        "    epsilon: float,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"Runs experiment\"\"\"\n",
        "\n",
        "    # generate prng keys\n",
        "    key, env_key, param_key, eval_key = jax.random.split(jax.random.PRNGKey(seed), num=4)\n",
        "\n",
        "    # get available TPU cores\n",
        "    cores_count = len(jax.devices())\n",
        "\n",
        "    # define network\n",
        "    network = build_dqn(env.action_spec().num_values)\n",
        "\n",
        "    # get dummy observation to initialise network parameters\n",
        "    _, d_timestep = env.reset(env_key)\n",
        "    dummy_obs = d_timestep.observation\n",
        "    params = network.init(param_key, dummy_obs)\n",
        "\n",
        "    # define and initialise optimiser\n",
        "    optim = optax.adam(step_size)\n",
        "    opt_state = optim.init(params)\n",
        "\n",
        "    inner_iter_length = 100\n",
        "\n",
        "    # jitables\n",
        "    env_step = jax.jit(env.step)\n",
        "    model_apply = jax.jit(network.apply)\n",
        "    eval_env_reset = jax.jit(eval_env.reset)\n",
        "    eval_env_step = jax.jit(eval_env.step)\n",
        "\n",
        "    # get learner method\n",
        "    learn = get_learner_fn(\n",
        "        env_step=env_step,\n",
        "        forward_pass=model_apply,\n",
        "        opt_update=optim.update,\n",
        "        rollout_len=rollout_len,\n",
        "        agent_discount=discount_factor,\n",
        "        lambda_=0.95,\n",
        "        iterations=inner_iter_length,\n",
        "        epsilon=epsilon,\n",
        "    )\n",
        "\n",
        "    # get evaluation method\n",
        "    evaluate = jax.jit(\n",
        "        get_evaluator(\n",
        "            forward_pass=model_apply,\n",
        "            eval_env_reset=eval_env_reset,\n",
        "            eval_env_step=eval_env_step,\n",
        "            key=eval_key,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # replicate over multiple cores\n",
        "    learn = jax.pmap(learn, axis_name=\"i\")\n",
        "\n",
        "    # define episode metrics\n",
        "    episode_metrics = EpisodeMetrics(\n",
        "        episode_return=jnp.zeros(shape=(), dtype=jnp.float32),\n",
        "        episode_length=jnp.zeros(shape=(), dtype=jnp.int16),\n",
        "    )\n",
        "\n",
        "    # broadcast to cores and batch\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (cores_count, batch_size) + x.shape)\n",
        "    params = jax.tree_map(broadcast, params)\n",
        "    opt_state = jax.tree_map(broadcast, opt_state)\n",
        "    episode_metrics = jax.tree_map(broadcast, episode_metrics)\n",
        "\n",
        "    # get intial states\n",
        "    key, *env_keys = jax.random.split(key, cores_count * batch_size + 1)\n",
        "    env_states, env_timesteps = jax.vmap(env.reset)(jnp.stack(env_keys))\n",
        "    key, *step_keys = jax.random.split(key, cores_count * batch_size + 1)\n",
        "\n",
        "    reshape = lambda x: jax.tree_util.tree_map(\n",
        "        lambda x: x.reshape((cores_count, batch_size) + x.shape[1:]), x\n",
        "    )\n",
        "\n",
        "    # add dimension to pmap over\n",
        "    step_keys = reshape(jnp.stack(step_keys))\n",
        "    env_states = reshape(env_states)\n",
        "    env_timesteps = reshape(env_timesteps)\n",
        "\n",
        "    # compile\n",
        "    num_frames_compile = cores_count * inner_iter_length * rollout_len * batch_size\n",
        "    with TimeIt(tag=\"COMPILATION\", frames=num_frames_compile):\n",
        "        learn(params, opt_state, step_keys, env_states, env_timesteps, episode_metrics)\n",
        "\n",
        "    # execute mulitple training iterations\n",
        "    num_frames = cores_count * iterations * rollout_len * batch_size\n",
        "    n_outer_iter = int(iterations // inner_iter_length)\n",
        "    with TimeIt(tag=\"EXECUTION\", frames=num_frames):\n",
        "        for i in tqdm(range(n_outer_iter)):\n",
        "            (\n",
        "                params,\n",
        "                opt_state,\n",
        "                step_keys,\n",
        "                env_states,\n",
        "                env_timesteps,\n",
        "                episode_metrics,\n",
        "            ), new_info = learn(\n",
        "                params, opt_state, step_keys, env_states, env_timesteps, episode_metrics\n",
        "            )\n",
        "            \n",
        "            # update info\n",
        "            max_return_info = {\n",
        "                \"max_episode_return\": jnp.nanmax(new_info[\"max_episode_return\"], axis=0)\n",
        "            }\n",
        "            new_info = jax.tree_util.tree_map(\n",
        "                lambda x: jnp.nanmean(x, axis=0), new_info\n",
        "            )\n",
        "            new_info.update(max_return_info)\n",
        "\n",
        "            # evaluate and add to info\n",
        "            params_single_device = jax.tree_util.tree_map(lambda x: x[0, 0], params)\n",
        "\n",
        "            eval_return = evaluate(params_single_device)\n",
        "\n",
        "            eval_return_info = {\"eval_return\": jnp.array([eval_return])}\n",
        "            new_info.update(eval_return_info)\n",
        "\n",
        "            if i == 0:\n",
        "                info = new_info\n",
        "            else:\n",
        "                info = {\n",
        "                    dict_key: jnp.concatenate((info[dict_key], new_info[dict_key]))\n",
        "                    for dict_key in info.keys()\n",
        "                }\n",
        "\n",
        "    params_single_device = jax.tree_util.tree_map(lambda x: x[0, 0], params)\n",
        "    return info, params_single_device\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Experiment and Visualise Results \n",
        "We first choose our set of hyper-parameters and call `run_experiment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf6r_IkAIKDO",
        "outputId": "0bc10958-ba18-4cc2-8277-61495ca658ae"
      },
      "outputs": [],
      "source": [
        "env = AutoResetWrapper(jumanji.make(\"Snake-v1\"))\n",
        "eval_env = jumanji.make(\"Snake-v1\")\n",
        "\n",
        "info, params = run_experiment(\n",
        "    env,\n",
        "    eval_env,\n",
        "    batch_size=256,\n",
        "    rollout_len=12,\n",
        "    step_size=2e-4,\n",
        "    iterations=200_000,\n",
        "    discount_factor=0.997,\n",
        "    epsilon=0.02,\n",
        "    seed=0,\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V30Cc_hdQqT2"
      },
      "outputs": [],
      "source": [
        "plt.plot(info[\"max_episode_return\"])\n",
        "plt.title(\"max episode return per batch\")\n",
        "plt.xlabel(\"training iteration\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(info[\"episode_return\"])\n",
        "plt.title(\"mean episode return per batch\")\n",
        "plt.xlabel(\"training iteration\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(info[\"eval_return\"])\n",
        "plt.title(\"evaluation return per batch\")\n",
        "plt.xlabel(\"outer training iterations\")\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollout Agent\n",
        "We let the agent act greedily throughout an episode and visualise its behaviour.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "env = jumanji.make(\"Snake-v1\")\n",
        "state, timestep = env.reset(key)\n",
        "env_step = jax.jit(env.step)\n",
        "\n",
        "DQN = build_dqn(env.action_spec().num_values)\n",
        "initial_parameters = DQN.init(key, timestep.observation)\n",
        "policy = jax.jit(DQN.apply)\n",
        "\n",
        "done = False\n",
        "transitions = 0\n",
        "while not done and transitions < 10:\n",
        "    # select action greedily\n",
        "    action = jnp.argmax(policy(params, timestep.observation))\n",
        "\n",
        "    # take a step in the environment\n",
        "    state, timestep = env_step(state, action)\n",
        "    transitions += 1\n",
        "\n",
        "    done = not timestep.discount\n",
        "\n",
        "    env.render(state)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: add a little summary of what was done and optionally an invitation to look at the full documentation, and potentially a take home message."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "9afa828456cfb0658300d7120d39dda85b3a9b38362c4ce9e8ea6450f74bb614"
    },
    "kernelspec": {
      "display_name": "Python 3.10.11 ('jumanji_notebook')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
