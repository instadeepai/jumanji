{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Environments | Installation | Quickstart | Training | Citation | Docs</p>"},{"location":"#jumanji-iclr-2024","title":"Jumanji @ ICLR 2024","text":"<p>Jumanji has been accepted at ICLR 2024, check out our research paper.</p>"},{"location":"#welcome-to-the-jungle","title":"Welcome to the Jungle! \ud83c\udf34","text":"<p>Jumanji is a diverse suite of scalable reinforcement learning environments written in JAX. It now features 22 environments!</p> <p>Jumanji is helping pioneer a new wave of hardware-accelerated research and development in the field of RL. Jumanji's high-speed environments enable faster iteration and large-scale experimentation while simultaneously reducing complexity. Originating in the research team at InstaDeep, Jumanji is now developed jointly with the open-source community. To join us in these efforts, reach out, raise issues and read our contribution guidelines or just star \ud83c\udf1f to stay up to date with the latest developments!</p>"},{"location":"#goals","title":"Goals \ud83d\ude80","text":"<ol> <li>Provide a simple, well-tested API for JAX-based environments.</li> <li>Make research in RL more accessible.</li> <li>Facilitate the research on RL for problems in the industry and help close the gap between research and industrial applications.</li> <li>Provide environments whose difficulty can be scaled to be arbitrarily hard.</li> </ol>"},{"location":"#overview","title":"Overview \ud83e\udd9c","text":"<ul> <li>\ud83e\udd51 Environment API: core abstractions for JAX-based environments.</li> <li>\ud83d\udd79\ufe0f Environment Suite: a collection of RL environments ranging from simple games to NP-hard combinatorial problems.</li> <li>\ud83c\udf6c Wrappers: easily connect to your favourite RL frameworks and libraries such as Acme, Stable Baselines3, RLlib, Gymnasium and DeepMind-Env through our <code>dm_env</code> and <code>gym</code> wrappers.</li> <li>\ud83c\udf93 Examples: guides to facilitate Jumanji's adoption and highlight the added value of JAX-based environments.</li> <li>\ud83c\udfce\ufe0f Training: example agents that can be used as inspiration for the agents one may implement in their research.</li> </ul>"},{"location":"#environments","title":"Environments \ud83c\udf0d","text":"<p>Jumanji provides a diverse range of environments ranging from simple games to NP-hard combinatorial problems.</p> Environment Category Registered Version(s) Source Description \ud83d\udd22 Game2048 Logic <code>Game2048-v1</code> code doc \ud83c\udfa8 GraphColoring Logic <code>GraphColoring-v0</code> code doc \ud83d\udca3 Minesweeper Logic <code>Minesweeper-v0</code> code doc \ud83c\udfb2 RubiksCube Logic <code>RubiksCube-v0</code><code>RubiksCube-partly-scrambled-v0</code> code doc \ud83d\udd00 SlidingTilePuzzle Logic <code>SlidingTilePuzzle-v0</code> code doc \u270f\ufe0f Sudoku Logic <code>Sudoku-v0</code> <code>Sudoku-very-easy-v0</code> code doc \ud83d\udce6 BinPack (3D BinPacking Problem) Packing <code>BinPack-v1</code> code doc \ud83e\udde9 FlatPack (2D Grid Filling Problem) Packing <code>FlatPack-v0</code> code doc \ud83c\udfed JobShop (Job Shop Scheduling Problem) Packing <code>JobShop-v0</code> code doc \ud83c\udf92 Knapsack Packing <code>Knapsack-v1</code> code doc \u2592 Tetris Packing <code>Tetris-v0</code> code doc \ud83e\uddf9 Cleaner Routing <code>Cleaner-v0</code> code doc  Connector Routing <code>Connector-v2</code> code doc \ud83d\ude9a CVRP (Capacitated Vehicle Routing Problem) Routing <code>CVRP-v1</code> code doc \ud83d\ude9a MultiCVRP (Multi-Agent Capacitated Vehicle Routing Problem) Routing <code>MultiCVRP-v0</code> code doc  Maze Routing <code>Maze-v0</code> code doc  RobotWarehouse Routing <code>RobotWarehouse-v0</code> code doc \ud83d\udc0d Snake Routing <code>Snake-v1</code> code doc \ud83d\udcec TSP (Travelling Salesman Problem) Routing <code>TSP-v1</code> code doc Multi Minimum Spanning Tree Problem Routing <code>MMST-v0</code> code doc \u15e7\u2022\u2022\u2022\u15e3\u2022\u2022 PacMan Routing <code>PacMan-v1</code> code doc \ud83d\udc7e Sokoban Routing <code>Sokoban-v0</code> code doc \ud83c\udf4e Level-Based Foraging Routing <code>LevelBasedForaging-v0</code> code doc \ud83d\ude81 Search and Rescue Swarms <code>SearchAndRescue-v0</code> code doc"},{"location":"#install","title":"Installation \ud83c\udfac","text":"<p>You can install the latest release of Jumanji from PyPI:</p> <pre><code>pip install -U jumanji\n</code></pre> <p>Alternatively, you can install the latest development version directly from GitHub:</p> <pre><code>pip install git+https://github.com/instadeepai/jumanji.git\n</code></pre> <p>Jumanji has been tested on Python 3.10, 3.11 and 3.12. Note that because the installation of JAX differs depending on your hardware accelerator, we advise users to explicitly install the correct JAX version (see the official installation guide).</p> <p>Rendering: Matplotlib is used for rendering all the environments. To visualize the environments you will need a GUI backend. For example, on Linux, you can install Tk via: <code>apt-get install python3-tk</code>, or using conda: <code>conda install tk</code>. Check out Matplotlib backends for a list of backends you can use.</p>"},{"location":"#quickstart","title":"Quickstart \u26a1","text":"<p>RL practitioners will find Jumanji's interface familiar as it combines the widely adopted OpenAI Gym and DeepMind Environment interfaces. From OpenAI Gym, we adopted the idea of a <code>registry</code> and the <code>render</code> method, while our <code>TimeStep</code> structure is inspired by DeepMind Environment.</p>"},{"location":"#basic-usage","title":"Basic Usage \ud83e\uddd1\u200d\ud83d\udcbb","text":"<pre><code>import jax\nimport jumanji\n\n# Instantiate a Jumanji environment using the registry\nenv = jumanji.make('Snake-v1')\n\n# Reset your (jit-able) environment\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\n\n# (Optional) Render the env state\nenv.render(state)\n\n# Interact with the (jit-able) environment\naction = env.action_spec.generate_value()          # Action selection (dummy value here)\nstate, timestep = jax.jit(env.step)(state, action)   # Take a step and observe the next state and time step\n</code></pre> <ul> <li><code>state</code> represents the internal state of the environment: it contains all the information required to take a step when executing an action. This should not be confused with the <code>observation</code> contained in the <code>timestep</code>, which is the information perceived by the agent.</li> <li><code>timestep</code> is a dataclass containing <code>step_type</code>, <code>reward</code>, <code>discount</code>, <code>observation</code> and <code>extras</code>. This structure is similar to <code>dm_env.TimeStep</code> except for the <code>extras</code> field that was added to allow users to log environments metrics that are neither part of the agent's observation nor part of the environment's internal state.</li> </ul>"},{"location":"#advanced-usage","title":"Advanced Usage \ud83e\uddd1\u200d\ud83d\udd2c","text":"<p>Being written in JAX, Jumanji's environments benefit from many of its features including automatic vectorization/parallelization (<code>jax.vmap</code>, <code>jax.pmap</code>) and JIT-compilation (<code>jax.jit</code>), which can be composed arbitrarily. We provide an example of a more advanced usage in the advanced usage guide.</p>"},{"location":"#registry-and-versioning","title":"Registry and Versioning \ud83d\udcd6","text":"<p>Like OpenAI Gym, Jumanji keeps a strict versioning of its environments for reproducibility reasons. We maintain a registry of standard environments with their configuration. For each environment, a version suffix is appended, e.g. <code>Snake-v1</code>. When changes are made to environments that might impact learning results, the version number is incremented by one to prevent potential confusion. For a full list of registered versions of each environment, check out the documentation.</p>"},{"location":"#training","title":"Training \ud83c\udfce\ufe0f","text":"<p>To showcase how to train RL agents on Jumanji environments, we provide a random agent and a vanilla actor-critic (A2C) agent. These agents can be found in jumanji/training/.</p> <p>Because the environment framework in Jumanji is so flexible, it allows pretty much any problem to be implemented as a Jumanji environment, giving rise to very diverse observations. For this reason, environment-specific networks are required to capture the symmetries of each environment. Alongside the A2C agent implementation, we provide examples of such environment-specific actor-critic networks in jumanji/training/networks.</p> <p>\u26a0\ufe0f The example agents in <code>jumanji/training</code> are only meant to serve as inspiration for how one can implement an agent. Jumanji is first and foremost a library of environments - as such, the agents and networks will not be maintained to a production standard.</p> <p>For more information on how to use the example agents, see the training guide.</p>"},{"location":"#contributing","title":"Contributing \ud83e\udd1d","text":"<p>Contributions are welcome! See our issue tracker for good first issues. Please read our contributing guidelines for details on how to submit pull requests, our Contributor License Agreement, and community guidelines.</p>"},{"location":"#citing","title":"Citing Jumanji \u270f\ufe0f","text":"<p>If you use Jumanji in your work, please cite the library using:</p> <pre><code>@misc{bonnet2024jumanji,\n    title={Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX},\n    author={Cl\u00e9ment Bonnet and Daniel Luo and Donal Byrne and Shikha Surana and Sasha Abramowitz and Paul Duckworth and Vincent Coyette and Laurence I. Midgley and Elshadai Tegegn and Tristan Kalloniatis and Omayma Mahjoub and Matthew Macfarlane and Andries P. Smit and Nathan Grinsztajn and Raphael Boige and Cemlyn N. Waters and Mohamed A. Mimouni and Ulrich A. Mbou Sob and Ruan de Kock and Siddarth Singh and Daniel Furelos-Blanco and Victor Le and Arnu Pretorius and Alexandre Laterre},\n    year={2024},\n    eprint={2306.09884},\n    url={https://arxiv.org/abs/2306.09884},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"#see-also","title":"See Also \ud83d\udd0e","text":"<p>Other works have embraced the approach of writing RL environments in JAX. In particular, we suggest users check out the following sister repositories:</p> <ul> <li>\ud83e\udd16 Qdax is a library to accelerate Quality-Diversity and neuro-evolution algorithms through hardware accelerators and parallelization.</li> <li>\ud83c\udf33 Evojax provides tools to enable neuroevolution algorithms to work with neural networks running across multiple TPU/GPUs.</li> <li>\ud83e\uddbe Brax is a differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators.</li> <li>\ud83c\udfcb\ufe0f\u200d Gymnax implements classic environments including classic control, bsuite, MinAtar and a collection of meta RL tasks.</li> <li>\ud83c\udfb2 Pgx provides classic board game environments like Backgammon, Shogi, and Go.</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements \ud83d\ude4f","text":"<p>The development of this library was supported with Cloud TPUs from Google's TPU Research Cloud (TRC) \ud83c\udf24.</p>"},{"location":"api/env/","title":"Base","text":"<p>               Bases: <code>ABC</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>Environment written in Jax that differs from the gym API to make the step and reset functions jittable. The state contains all the dynamics and data needed to step the environment, no computation stored in attributes of self. The API is inspired by brax.</p> <p>Initialize environment.</p> Source code in <code>jumanji/env.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize environment.\"\"\"\n    self.observation_spec  # noqa: B018\n    self.action_spec  # noqa: B018\n    self.reward_spec  # noqa: B018\n    self.discount_spec  # noqa: B018\n</code></pre>"},{"location":"api/env/#jumanji.env.Environment.action_spec","title":"<code>action_spec</code>  <code>abstractmethod</code> <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>ActionSpec</code> <p>a potentially nested <code>Spec</code> structure representing the action.</p>"},{"location":"api/env/#jumanji.env.Environment.discount_spec","title":"<code>discount_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the discount spec. By default, this is assumed to be a float between 0 and 1.</p> <p>Returns:</p> Name Type Description <code>discount_spec</code> <code>BoundedArray</code> <p>a <code>specs.BoundedArray</code> spec.</p>"},{"location":"api/env/#jumanji.env.Environment.observation_spec","title":"<code>observation_spec</code>  <code>abstractmethod</code> <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Name Type Description <code>observation_spec</code> <code>Spec[Observation]</code> <p>a potentially nested <code>Spec</code> structure representing the observation.</p>"},{"location":"api/env/#jumanji.env.Environment.reward_spec","title":"<code>reward_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the reward spec. By default, this is assumed to be a single float.</p> <p>Returns:</p> Name Type Description <code>reward_spec</code> <code>Array</code> <p>a <code>specs.Array</code> spec.</p>"},{"location":"api/env/#jumanji.env.Environment.__exit__","title":"<code>__exit__(*args)</code>","text":"<p>Calls :meth:<code>close()</code>.</p> Source code in <code>jumanji/env.py</code> <pre><code>def __exit__(self, *args: Any) -&gt; None:\n    \"\"\"Calls :meth:`close()`.\"\"\"\n    self.close()\n</code></pre>"},{"location":"api/env/#jumanji.env.Environment.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> Source code in <code>jumanji/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\"\"\"\n</code></pre>"},{"location":"api/env/#jumanji.env.Environment.render","title":"<code>render(state)</code>","text":"<p>Render frames of the environment for a given state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the current dynamics of the environment.</p> required Source code in <code>jumanji/env.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Render frames of the environment for a given state.\n\n    Args:\n        state: State object containing the current dynamics of the environment.\n    \"\"\"\n    raise NotImplementedError(\"Render method not implemented for this environment.\")\n</code></pre>"},{"location":"api/env/#jumanji.env.Environment.reset","title":"<code>reset(key)</code>  <code>abstractmethod</code>","text":"<p>Resets the environment to an initial state.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the first timestep returned by the environment,</p> Source code in <code>jumanji/env.py</code> <pre><code>@abc.abstractmethod\ndef reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: State object corresponding to the new state of the environment,\n        timestep: TimeStep object corresponding the first timestep returned by the environment,\n    \"\"\"\n</code></pre>"},{"location":"api/env/#jumanji.env.Environment.step","title":"<code>step(state, action)</code>  <code>abstractmethod</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the action to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the next state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the timestep returned by the environment,</p> Source code in <code>jumanji/env.py</code> <pre><code>@abc.abstractmethod\ndef step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the action to take.\n\n    Returns:\n        state: State object corresponding to the next state of the environment,\n        timestep: TimeStep object corresponding the timestep returned by the environment,\n    \"\"\"\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#jumanji.types.StepType","title":"<code>StepType</code>","text":"<p>               Bases: <code>int8</code></p> <p>Defines the status of a <code>TimeStep</code> within a sequence.</p> <p>First: 0 Mid: 1 Last: 2</p>"},{"location":"api/types/#jumanji.types.TimeStep","title":"<code>TimeStep</code>","text":"<p>               Bases: <code>Generic[Observation]</code></p> <p>Copied from <code>dm_env.TimeStep</code> with the goal of making it a Jax Type. The original <code>dm_env.TimeStep</code> is not a Jax type because inheriting a namedtuple is not treated as a valid Jax type (https://github.com/google/jax/issues/806).</p> <p>A <code>TimeStep</code> contains the data emitted by an environment at each step of interaction. A <code>TimeStep</code> holds a <code>step_type</code>, an <code>observation</code> (typically a NumPy array or a dict or list of arrays), and an associated <code>reward</code> and <code>discount</code>.</p> <p>The first <code>TimeStep</code> in a sequence will have <code>StepType.FIRST</code>. The final <code>TimeStep</code> will have <code>StepType.LAST</code>. All other <code>TimeStep</code>s in a sequence will have `StepType.MID.</p> <p>Attributes:</p> Name Type Description <code>step_type</code> <code>StepType</code> <p>A <code>StepType</code> enum value.</p> <code>reward</code> <code>Array</code> <p>A scalar, NumPy array, nested dict, list or tuple of rewards; or <code>None</code> if <code>step_type</code> is <code>StepType.FIRST</code>, i.e. at the start of a sequence.</p> <code>discount</code> <code>Array</code> <p>A scalar, NumPy array, nested dict, list or tuple of discount values in the range <code>[0, 1]</code>, or <code>None</code> if <code>step_type</code> is <code>StepType.FIRST</code>, i.e. at the start of a sequence.</p> <code>observation</code> <code>Observation</code> <p>A NumPy array, or a nested dict, list or tuple of arrays. Scalar values that can be cast to NumPy arrays (e.g. Python floats) are also valid in place of a scalar array.</p> <code>extras</code> <code>Dict</code> <p>environment metric(s) or information returned by the environment but not observed by the agent (hence not in the observation). For example, it could be whether an invalid action was taken. In most environments, extras is an empty dictionary.</p>"},{"location":"api/types/#jumanji.types.get_valid_dtype","title":"<code>get_valid_dtype(dtype)</code>","text":"<p>Cast a dtype taking into account the user type precision. E.g., if 64 bit is not enabled, jnp.dtype(jnp.float_) is still float64. By passing the given dtype through <code>jnp.empty</code> we get the supported dtype of float32.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>Union[dtype, type]</code> <p>jax numpy dtype or string specifying the array dtype.</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>dtype converted to the correct type precision.</p> Source code in <code>jumanji/types.py</code> <pre><code>def get_valid_dtype(dtype: Union[jnp.dtype, type]) -&gt; jnp.dtype:\n    \"\"\"Cast a dtype taking into account the user type precision. E.g., if 64 bit is not enabled,\n    jnp.dtype(jnp.float_) is still float64. By passing the given dtype through `jnp.empty` we get\n    the supported dtype of float32.\n\n    Args:\n        dtype: jax numpy dtype or string specifying the array dtype.\n\n    Returns:\n        dtype converted to the correct type precision.\n    \"\"\"\n    return jnp.empty((), dtype).dtype  # type: ignore\n</code></pre>"},{"location":"api/types/#jumanji.types.restart","title":"<code>restart(observation, extras=None, shape=(), dtype=float)</code>","text":"<p>Returns a <code>TimeStep</code> with <code>step_type</code> set to <code>StepType.FIRST</code>.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>Observation</code> <p>array or tree of arrays.</p> required <code>extras</code> <code>Optional[Dict]</code> <p>environment metric(s) or information returned by the environment but not observed by the agent (hence not in the observation). For example, it could be whether an invalid action was taken. In most environments, extras is None.</p> <code>None</code> <code>shape</code> <code>Union[int, Sequence[int]]</code> <p>optional parameter to specify the shape of the rewards and discounts. Allows multi-agent environment compatibility. Defaults to () for scalar reward and discount.</p> <code>()</code> <code>dtype</code> <code>Union[dtype, type]</code> <p>Optional parameter to specify the data type of the rewards and discounts. Defaults to <code>float</code>.</p> <code>float</code> <p>Returns:</p> Type Description <code>TimeStep</code> <p>TimeStep identified as a reset.</p> Source code in <code>jumanji/types.py</code> <pre><code>def restart(\n    observation: Observation,\n    extras: Optional[Dict] = None,\n    shape: Union[int, Sequence[int]] = (),\n    dtype: Union[jnp.dtype, type] = float,\n) -&gt; TimeStep:\n    \"\"\"Returns a `TimeStep` with `step_type` set to `StepType.FIRST`.\n\n    Args:\n        observation: array or tree of arrays.\n        extras: environment metric(s) or information returned by the environment but\n            not observed by the agent (hence not in the observation). For example, it\n            could be whether an invalid action was taken. In most environments, extras\n            is None.\n        shape: optional parameter to specify the shape of the rewards and discounts.\n            Allows multi-agent environment compatibility. Defaults to () for\n            scalar reward and discount.\n        dtype: Optional parameter to specify the data type of the rewards and discounts.\n            Defaults to `float`.\n\n    Returns:\n        TimeStep identified as a reset.\n    \"\"\"\n    extras = extras or {}\n    return TimeStep(\n        step_type=StepType.FIRST,\n        reward=jnp.zeros(shape, dtype=dtype),\n        discount=jnp.ones(shape, dtype=dtype),\n        observation=observation,\n        extras=extras,\n    )\n</code></pre>"},{"location":"api/types/#jumanji.types.termination","title":"<code>termination(reward, observation, extras=None, shape=(), dtype=float)</code>","text":"<p>Returns a <code>TimeStep</code> with <code>step_type</code> set to <code>StepType.LAST</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reward</code> <code>Array</code> <p>array.</p> required <code>observation</code> <code>Observation</code> <p>array or tree of arrays.</p> required <code>extras</code> <code>Optional[Dict]</code> <p>environment metric(s) or information returned by the environment but not observed by the agent (hence not in the observation). For example, it could be whether an invalid action was taken. In most environments, extras is None.</p> <code>None</code> <code>shape</code> <code>Union[int, Sequence[int]]</code> <p>optional parameter to specify the shape of the rewards and discounts. Allows multi-agent environment compatibility. Defaults to () for scalar reward and discount.</p> <code>()</code> <code>dtype</code> <code>Union[dtype, type]</code> <p>Optional parameter to specify the data type of the discounts. Defaults to <code>float</code>.</p> <code>float</code> <p>Returns:</p> Type Description <code>TimeStep</code> <p>TimeStep identified as the termination of an episode.</p> Source code in <code>jumanji/types.py</code> <pre><code>def termination(\n    reward: Array,\n    observation: Observation,\n    extras: Optional[Dict] = None,\n    shape: Union[int, Sequence[int]] = (),\n    dtype: Union[jnp.dtype, type] = float,\n) -&gt; TimeStep:\n    \"\"\"Returns a `TimeStep` with `step_type` set to `StepType.LAST`.\n\n    Args:\n        reward: array.\n        observation: array or tree of arrays.\n        extras: environment metric(s) or information returned by the environment but\n            not observed by the agent (hence not in the observation). For example, it\n            could be whether an invalid action was taken. In most environments, extras\n            is None.\n        shape: optional parameter to specify the shape of the rewards and discounts.\n            Allows multi-agent environment compatibility. Defaults to () for\n            scalar reward and discount.\n        dtype: Optional parameter to specify the data type of the discounts. Defaults\n            to `float`.\n\n    Returns:\n        TimeStep identified as the termination of an episode.\n    \"\"\"\n    extras = extras or {}\n    return TimeStep(\n        step_type=StepType.LAST,\n        reward=reward,\n        discount=jnp.zeros(shape, dtype=dtype),\n        observation=observation,\n        extras=extras,\n    )\n</code></pre>"},{"location":"api/types/#jumanji.types.transition","title":"<code>transition(reward, observation, discount=None, extras=None, shape=(), dtype=float)</code>","text":"<p>Returns a <code>TimeStep</code> with <code>step_type</code> set to <code>StepType.MID</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reward</code> <code>Array</code> <p>array.</p> required <code>observation</code> <code>Observation</code> <p>array or tree of arrays.</p> required <code>discount</code> <code>Optional[Array]</code> <p>array.</p> <code>None</code> <code>extras</code> <code>Optional[Dict]</code> <p>environment metric(s) or information returned by the environment but not observed by the agent (hence not in the observation). For example, it could be whether an invalid action was taken. In most environments, extras is None.</p> <code>None</code> <code>shape</code> <code>Union[int, Sequence[int]]</code> <p>optional parameter to specify the shape of the rewards and discounts. Allows multi-agent environment compatibility. Defaults to () for scalar reward and discount.</p> <code>()</code> <code>dtype</code> <code>Union[dtype, type]</code> <p>Optional parameter to specify the data type of the discounts. Defaults to <code>float</code>.</p> <code>float</code> <p>Returns:</p> Type Description <code>TimeStep</code> <p>TimeStep identified as a transition.</p> Source code in <code>jumanji/types.py</code> <pre><code>def transition(\n    reward: Array,\n    observation: Observation,\n    discount: Optional[Array] = None,\n    extras: Optional[Dict] = None,\n    shape: Union[int, Sequence[int]] = (),\n    dtype: Union[jnp.dtype, type] = float,\n) -&gt; TimeStep:\n    \"\"\"Returns a `TimeStep` with `step_type` set to `StepType.MID`.\n\n    Args:\n        reward: array.\n        observation: array or tree of arrays.\n        discount: array.\n        extras: environment metric(s) or information returned by the environment but\n            not observed by the agent (hence not in the observation). For example, it\n            could be whether an invalid action was taken. In most environments, extras\n            is None.\n        shape: optional parameter to specify the shape of the rewards and discounts.\n            Allows multi-agent environment compatibility. Defaults to () for\n            scalar reward and discount.\n        dtype: Optional parameter to specify the data type of the discounts. Defaults\n            to `float`.\n\n    Returns:\n        TimeStep identified as a transition.\n    \"\"\"\n    discount = discount if discount is not None else jnp.ones(shape, dtype=dtype)\n    extras = extras or {}\n    return TimeStep(\n        step_type=StepType.MID,\n        reward=reward,\n        discount=discount,\n        observation=observation,\n        extras=extras,\n    )\n</code></pre>"},{"location":"api/types/#jumanji.types.truncation","title":"<code>truncation(reward, observation, discount=None, extras=None, shape=(), dtype=float)</code>","text":"<p>Returns a <code>TimeStep</code> with <code>step_type</code> set to <code>StepType.LAST</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reward</code> <code>Array</code> <p>array.</p> required <code>observation</code> <code>Observation</code> <p>array or tree of arrays.</p> required <code>discount</code> <code>Optional[Array]</code> <p>array.</p> <code>None</code> <code>extras</code> <code>Optional[Dict]</code> <p>environment metric(s) or information returned by the environment but not observed by the agent (hence not in the observation). For example, it could be whether an invalid action was taken. In most environments, extras is None.</p> <code>None</code> <code>shape</code> <code>Union[int, Sequence[int]]</code> <p>optional parameter to specify the shape of the rewards and discounts. Allows multi-agent environment compatibility. Defaults to () for scalar reward and discount.</p> <code>()</code> <code>dtype</code> <code>Union[dtype, type]</code> <p>Optional parameter to specify the data type of the discounts. Defaults to <code>float</code>.</p> <code>float</code> <p>Returns:</p> Type Description <code>TimeStep</code> <p>TimeStep identified as the truncation of an episode.</p> Source code in <code>jumanji/types.py</code> <pre><code>def truncation(\n    reward: Array,\n    observation: Observation,\n    discount: Optional[Array] = None,\n    extras: Optional[Dict] = None,\n    shape: Union[int, Sequence[int]] = (),\n    dtype: Union[jnp.dtype, type] = float,\n) -&gt; TimeStep:\n    \"\"\"Returns a `TimeStep` with `step_type` set to `StepType.LAST`.\n\n    Args:\n        reward: array.\n        observation: array or tree of arrays.\n        discount: array.\n        extras: environment metric(s) or information returned by the environment but\n            not observed by the agent (hence not in the observation). For example, it\n            could be whether an invalid action was taken. In most environments, extras\n            is None.\n        shape: optional parameter to specify the shape of the rewards and discounts.\n            Allows multi-agent environment compatibility. Defaults to () for\n            scalar reward and discount.\n        dtype: Optional parameter to specify the data type of the discounts. Defaults\n            to `float`.\n\n    Returns:\n        TimeStep identified as the truncation of an episode.\n    \"\"\"\n    discount = discount if discount is not None else jnp.ones(shape, dtype=dtype)\n    extras = extras or {}\n    return TimeStep(\n        step_type=StepType.LAST,\n        reward=reward,\n        discount=discount,\n        observation=observation,\n        extras=extras,\n    )\n</code></pre>"},{"location":"api/wrappers/","title":"Wrappers","text":""},{"location":"api/wrappers/#jumanji.wrappers.AutoResetWrapper","title":"<code>AutoResetWrapper(env, next_obs_in_extras=False)</code>","text":"<p>               Bases: <code>Wrapper[State, ActionSpec, Observation]</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>Automatically resets environments that are done. Once the terminal state is reached, the state, observation, and step_type are reset. The observation and step_type of the terminal TimeStep is reset to the reset observation and StepType.LAST, respectively. The reward, discount, and extras retrieved from the transition to the terminal state. NOTE: The observation from the terminal TimeStep is stored in timestep.extras[\"next_obs\"]. WARNING: do not <code>jax.vmap</code> the wrapped environment (e.g. do not use with the <code>VmapWrapper</code>), which would lead to inefficient computation due to both the <code>step</code> and <code>reset</code> functions being processed each time <code>step</code> is called. Please use the <code>VmapAutoResetWrapper</code> instead.</p> <p>Wrap an environment to automatically reset it when the episode terminates.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Environment[State, ActionSpec, Observation]</code> <p>the environment to wrap.</p> required <code>next_obs_in_extras</code> <code>bool</code> <p>whether to store the next observation in the extras of the terminal timestep. This is useful for e.g. truncation.</p> <code>False</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(\n    self,\n    env: Environment[State, ActionSpec, Observation],\n    next_obs_in_extras: bool = False,\n):\n    \"\"\"Wrap an environment to automatically reset it when the episode terminates.\n\n    Args:\n        env: the environment to wrap.\n        next_obs_in_extras: whether to store the next observation in the extras of the\n            terminal timestep. This is useful for e.g. truncation.\n    \"\"\"\n    super().__init__(env)\n    self.next_obs_in_extras = next_obs_in_extras\n    if next_obs_in_extras:\n        self._maybe_add_obs_to_extras = add_obs_to_extras\n    else:\n        self._maybe_add_obs_to_extras = lambda timestep: timestep  # no-op\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.AutoResetWrapper.step","title":"<code>step(state, action)</code>","text":"<p>Step the environment, with automatic resetting if the episode terminates.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Step the environment, with automatic resetting if the episode terminates.\"\"\"\n    state, timestep = self._env.step(state, action)\n\n    # Overwrite the state and timestep appropriately if the episode terminates.\n    state, timestep = jax.lax.cond(\n        timestep.last(),\n        self._auto_reset,\n        lambda s, t: (s, self._maybe_add_obs_to_extras(t)),\n        state,\n        timestep,\n    )\n\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToDMEnvWrapper","title":"<code>JumanjiToDMEnvWrapper(env, key=None)</code>","text":"<p>               Bases: <code>Environment</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>A wrapper that converts Environment to dm_env.Environment.</p> <p>Create the wrapped environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Environment[State, ActionSpec, Observation]</code> <p><code>Environment</code>to wrap to a <code>dm_env.Environment</code>.</p> required <code>key</code> <code>Optional[PRNGKey]</code> <p>optional key to initialize the <code>Environment</code> with.</p> <code>None</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(\n    self,\n    env: Environment[State, ActionSpec, Observation],\n    key: Optional[chex.PRNGKey] = None,\n):\n    \"\"\"Create the wrapped environment.\n\n    Args:\n        env: `Environment`to wrap to a `dm_env.Environment`.\n        key: optional key to initialize the `Environment` with.\n    \"\"\"\n    self._env = env\n    if key is None:\n        self._key = jax.random.PRNGKey(0)\n    else:\n        self._key = key\n    self._state: Any\n    self._jitted_reset: Callable[[chex.PRNGKey], Tuple[State, TimeStep]] = jax.jit(\n        self._env.reset\n    )\n    self._jitted_step: Callable[[State, chex.Array], Tuple[State, TimeStep]] = jax.jit(\n        self._env.step\n    )\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToDMEnvWrapper.action_spec","title":"<code>action_spec()</code>","text":"<p>Returns the dm_env action spec.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def action_spec(self) -&gt; dm_env.specs.Array:\n    \"\"\"Returns the dm_env action spec.\"\"\"\n    return specs.jumanji_specs_to_dm_env_specs(self._env.action_spec)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToDMEnvWrapper.observation_spec","title":"<code>observation_spec()</code>","text":"<p>Returns the dm_env observation spec.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def observation_spec(self) -&gt; dm_env.specs.Array:\n    \"\"\"Returns the dm_env observation spec.\"\"\"\n    return specs.jumanji_specs_to_dm_env_specs(self._env.observation_spec)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToDMEnvWrapper.reset","title":"<code>reset()</code>","text":"<p>Starts a new sequence and returns the first <code>TimeStep</code> of this sequence.</p> <p>Returns:</p> Type Description <code>TimeStep</code> <p>A <code>TimeStep</code> namedtuple containing: - step_type: A <code>StepType</code> of <code>FIRST</code>. - reward: <code>None</code>, indicating the reward is undefined. - discount: <code>None</code>, indicating the discount is undefined. - observation: A NumPy array, or a nested dict, list or tuple of arrays.     Scalar values that can be cast to NumPy arrays (e.g. Python floats)     are also valid in place of a scalar array. Must conform to the     specification returned by <code>observation_spec</code>.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(self) -&gt; dm_env.TimeStep:\n    \"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\n\n    Returns:\n        A `TimeStep` namedtuple containing:\n            - step_type: A `StepType` of `FIRST`.\n            - reward: `None`, indicating the reward is undefined.\n            - discount: `None`, indicating the discount is undefined.\n            - observation: A NumPy array, or a nested dict, list or tuple of arrays.\n                Scalar values that can be cast to NumPy arrays (e.g. Python floats)\n                are also valid in place of a scalar array. Must conform to the\n                specification returned by `observation_spec`.\n    \"\"\"\n    reset_key, self._key = jax.random.split(self._key)\n    self._state, timestep = self._jitted_reset(reset_key)\n    return dm_env.restart(observation=timestep.observation)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToDMEnvWrapper.step","title":"<code>step(action)</code>","text":"<p>Updates the environment according to the action and returns a <code>TimeStep</code>.</p> <p>If the environment returned a <code>TimeStep</code> with <code>StepType.LAST</code> at the previous step, this call to <code>step</code> will start a new sequence and <code>action</code> will be ignored.</p> <p>This method will also start a new sequence if called after the environment has been constructed and <code>reset</code> has not been called. Again, in this case <code>action</code> will be ignored.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>ArrayNumpy</code> <p>A NumPy array, or a nested dict, list or tuple of arrays corresponding to <code>action_spec</code>.</p> required <p>Returns:</p> Type Description <code>TimeStep</code> <p>A <code>TimeStep</code> namedtuple containing: - step_type: A <code>StepType</code> value. - reward: Reward at this timestep, or None if step_type is     <code>StepType.FIRST</code>. Must conform to the specification returned by     <code>reward_spec</code>. - discount: A discount in the range [0, 1], or None if step_type is     <code>StepType.FIRST</code>. Must conform to the specification returned by     <code>discount_spec</code>. - observation: A NumPy array, or a nested dict, list or tuple of arrays.     Scalar values that can be cast to NumPy arrays (e.g. Python floats)     are also valid in place of a scalar array. Must conform to the     specification returned by <code>observation_spec</code>.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, action: chex.ArrayNumpy) -&gt; dm_env.TimeStep:\n    \"\"\"Updates the environment according to the action and returns a `TimeStep`.\n\n    If the environment returned a `TimeStep` with `StepType.LAST` at the\n    previous step, this call to `step` will start a new sequence and `action`\n    will be ignored.\n\n    This method will also start a new sequence if called after the environment\n    has been constructed and `reset` has not been called. Again, in this case\n    `action` will be ignored.\n\n    Args:\n        action: A NumPy array, or a nested dict, list or tuple of arrays\n            corresponding to `action_spec`.\n\n    Returns:\n        A `TimeStep` namedtuple containing:\n            - step_type: A `StepType` value.\n            - reward: Reward at this timestep, or None if step_type is\n                `StepType.FIRST`. Must conform to the specification returned by\n                `reward_spec`.\n            - discount: A discount in the range [0, 1], or None if step_type is\n                `StepType.FIRST`. Must conform to the specification returned by\n                `discount_spec`.\n            - observation: A NumPy array, or a nested dict, list or tuple of arrays.\n                Scalar values that can be cast to NumPy arrays (e.g. Python floats)\n                are also valid in place of a scalar array. Must conform to the\n                specification returned by `observation_spec`.\n    \"\"\"\n    self._state, timestep = self._jitted_step(self._state, action)\n    return dm_env.TimeStep(\n        step_type=timestep.step_type,\n        reward=timestep.reward,\n        discount=timestep.discount,\n        observation=timestep.observation,\n    )\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper","title":"<code>JumanjiToGymWrapper(env, seed=0, backend=None)</code>","text":"<p>               Bases: <code>Env</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>A wrapper that converts a Jumanji <code>Environment</code> to one that follows the <code>gym.Env</code> API.</p> <p>Create the Gym environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Environment[State, ActionSpec, Observation]</code> <p><code>Environment</code> to wrap to a <code>gym.Env</code>.</p> required <code>seed</code> <code>int</code> <p>the seed that is used to initialize the environment's PRNG.</p> <code>0</code> <code>backend</code> <code>Optional[str]</code> <p>the XLA backend.</p> <code>None</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(\n    self,\n    env: Environment[State, ActionSpec, Observation],\n    seed: int = 0,\n    backend: Optional[str] = None,\n):\n    \"\"\"Create the Gym environment.\n\n    Args:\n        env: `Environment` to wrap to a `gym.Env`.\n        seed: the seed that is used to initialize the environment's PRNG.\n        backend: the XLA backend.\n    \"\"\"\n    self._env = env\n    self.metadata: Dict[str, str] = {}\n    self._key = jax.random.PRNGKey(seed)\n    self.backend = backend\n    self._state = None\n    self.observation_space = specs.jumanji_specs_to_gym_spaces(self._env.observation_spec)\n    self.action_space = specs.jumanji_specs_to_gym_spaces(self._env.action_spec)\n\n    def reset(key: chex.PRNGKey) -&gt; Tuple[State, Observation, Optional[Dict]]:\n        \"\"\"Reset function of a Jumanji environment to be jitted.\"\"\"\n        state, timestep = self._env.reset(key)\n        return state, timestep.observation, timestep.extras\n\n    self._reset = jax.jit(reset, backend=self.backend)\n\n    def step(\n        state: State, action: chex.Array\n    ) -&gt; Tuple[State, Observation, chex.Array, chex.Array, chex.Array, Optional[Any]]:\n        \"\"\"Step function of a Jumanji environment to be jitted.\"\"\"\n        state, timestep = self._env.step(state, action)\n        term = ~timestep.discount.astype(bool)\n        trunc = timestep.last().astype(bool)\n        return state, timestep.observation, timestep.reward, term, trunc, timestep.extras\n\n    self._step = jax.jit(step, backend=self.backend)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper.close","title":"<code>close()</code>","text":"<p>Closes the environment, important for rendering where pygame is imported.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the environment, important for rendering where pygame is imported.\"\"\"\n    self._env.close()\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper.render","title":"<code>render(mode='human')</code>","text":"<p>Renders the environment.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>currently not used since Jumanji does not currently support modes.</p> <code>'human'</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def render(self, mode: str = \"human\") -&gt; Any:\n    \"\"\"Renders the environment.\n\n    Args:\n        mode: currently not used since Jumanji does not currently support modes.\n    \"\"\"\n    del mode\n    if self._state is None:\n        raise ValueError(\"Cannot render when _state is None.\")\n    return self._env.render(self._state)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper.reset","title":"<code>reset(*, seed=None, options=None)</code>","text":"<p>Resets the environment to an initial state by starting a new sequence and returns the first <code>Observation</code> of this sequence.</p> <p>Returns:</p> Name Type Description <code>obs</code> <code>GymObservation</code> <p>an element of the environment's observation_space.</p> <code>info</code> <code>optional</code> <p>contains supplementary information such as metrics.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(\n    self,\n    *,\n    seed: Optional[int] = None,\n    options: Optional[dict] = None,\n) -&gt; Tuple[GymObservation, Dict[str, Any]]:\n    \"\"\"Resets the environment to an initial state by starting a new sequence\n    and returns the first `Observation` of this sequence.\n\n    Returns:\n        obs: an element of the environment's observation_space.\n        info (optional): contains supplementary information such as metrics.\n    \"\"\"\n    if seed is not None:\n        self.seed(seed)\n    key, self._key = jax.random.split(self._key)\n    self._state, obs, extras = self._reset(key)\n\n    # Convert the observation to a numpy array or a nested dict thereof\n    obs = jumanji_to_gym_obs(obs)\n\n    return obs, jax.device_get(extras)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper.seed","title":"<code>seed(seed=0)</code>","text":"<p>Function which sets the seed for the environment's random number generator(s).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the seed value for the random number generator(s).</p> <code>0</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def seed(self, seed: int = 0) -&gt; None:\n    \"\"\"Function which sets the seed for the environment's random number generator(s).\n\n    Args:\n        seed: the seed value for the random number generator(s).\n    \"\"\"\n    self._key = jax.random.PRNGKey(seed)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.JumanjiToGymWrapper.step","title":"<code>step(action)</code>","text":"<p>Updates the environment according to the action and returns an <code>Observation</code>.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>ArrayNumpy</code> <p>A NumPy array representing the action provided by the agent.</p> required <p>Returns:</p> Name Type Description <code>observation</code> <code>GymObservation</code> <p>an element of the environment's observation_space.</p> <code>reward</code> <code>float</code> <p>the amount of reward returned as a result of taking the action.</p> <code>terminated</code> <code>bool</code> <p>whether a terminal state is reached.</p> <code>info</code> <code>bool</code> <p>contains supplementary information such as metrics.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(\n    self, action: chex.ArrayNumpy\n) -&gt; Tuple[GymObservation, float, bool, bool, Dict[str, Any]]:\n    \"\"\"Updates the environment according to the action and returns an `Observation`.\n\n    Args:\n        action: A NumPy array representing the action provided by the agent.\n\n    Returns:\n        observation: an element of the environment's observation_space.\n        reward: the amount of reward returned as a result of taking the action.\n        terminated: whether a terminal state is reached.\n        info: contains supplementary information such as metrics.\n    \"\"\"\n\n    action_jax = jnp.asarray(action)  # Convert input numpy array to JAX array\n    self._state, obs, reward, term, trunc, extras = self._step(self._state, action_jax)\n\n    # Convert to get the correct signature\n    obs = jumanji_to_gym_obs(obs)\n    reward = float(reward)\n    terminated = bool(term)\n    truncated = bool(trunc)\n    info = jax.device_get(extras)\n\n    return obs, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.MultiToSingleWrapper","title":"<code>MultiToSingleWrapper(env, reward_aggregator=jnp.sum, discount_aggregator=jnp.max)</code>","text":"<p>               Bases: <code>Wrapper[State, ActionSpec, Observation]</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>A wrapper that converts a multi-agent Environment to a single-agent Environment.</p> <p>Create the wrapped environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Environment[State, ActionSpec, Observation]</code> <p><code>Environment</code> to wrap to a <code>dm_env.Environment</code>.</p> required <code>reward_aggregator</code> <code>Callable</code> <p>a function to aggregate all agents rewards into a single scalar value, e.g. sum.</p> <code>sum</code> <code>discount_aggregator</code> <code>Callable</code> <p>a function to aggregate all agents discounts into a single scalar value, e.g. max.</p> <code>max</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(\n    self,\n    env: Environment[State, ActionSpec, Observation],\n    reward_aggregator: Callable = jnp.sum,\n    discount_aggregator: Callable = jnp.max,\n):\n    \"\"\"Create the wrapped environment.\n\n    Args:\n        env: `Environment` to wrap to a `dm_env.Environment`.\n        reward_aggregator: a function to aggregate all agents rewards into a single scalar\n            value, e.g. sum.\n        discount_aggregator: a function to aggregate all agents discounts into a single\n            scalar value, e.g. max.\n    \"\"\"\n    super().__init__(env)\n    self._reward_aggregator = reward_aggregator\n    self._discount_aggregator = discount_aggregator\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.MultiToSingleWrapper.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment to an initial state.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the first timestep returned by the environment,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: State object corresponding to the new state of the environment,\n        timestep: TimeStep object corresponding the first timestep returned by the environment,\n    \"\"\"\n    state, timestep = self._env.reset(key)\n    timestep = self._aggregate_timestep(timestep)\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.MultiToSingleWrapper.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>The rewards are aggregated into a single value based on the given reward aggregator. The discount value is set to the largest discount of all the agents. This essentially means that if any single agent is alive, the discount value won't be zero.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the action to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the next state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the timestep returned by the environment,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    The rewards are aggregated into a single value based on the given reward aggregator.\n    The discount value is set to the largest discount of all the agents. This\n    essentially means that if any single agent is alive, the discount value won't be zero.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the action to take.\n\n    Returns:\n        state: State object corresponding to the next state of the environment,\n        timestep: TimeStep object corresponding the timestep returned by the environment,\n    \"\"\"\n    state, timestep = self._env.step(state, action)\n    timestep = self._aggregate_timestep(timestep)\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapAutoResetWrapper","title":"<code>VmapAutoResetWrapper(env, next_obs_in_extras=False)</code>","text":"<p>               Bases: <code>Wrapper[State, ActionSpec, Observation]</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>Efficient combination of VmapWrapper and AutoResetWrapper, to be used as a replacement of the combination of both wrappers. <code>env = VmapAutoResetWrapper(env)</code> is equivalent to <code>env = VmapWrapper(AutoResetWrapper(env))</code> but is more efficient as it parallelizes homogeneous computation and does not run branches of the computational graph that are not needed (heterogeneous computation). - Homogeneous computation: call step function on all environments in the batch. - Heterogeneous computation: conditional auto-reset (call reset function for some environments     within the batch because they have terminated). NOTE: The observation from the terminal TimeStep is stored in timestep.extras[\"next_obs\"].</p> <p>Wrap an environment to vmap it and automatically reset it when the episode terminates.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>Environment[State, ActionSpec, Observation]</code> <p>the environment to wrap.</p> required <code>next_obs_in_extras</code> <code>bool</code> <p>whether to store the next observation in the extras of the terminal timestep. This is useful for e.g. truncation.</p> <code>False</code> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(\n    self,\n    env: Environment[State, ActionSpec, Observation],\n    next_obs_in_extras: bool = False,\n):\n    \"\"\"Wrap an environment to vmap it and automatically reset it when the episode terminates.\n\n    Args:\n        env: the environment to wrap.\n        next_obs_in_extras: whether to store the next observation in the extras of the\n            terminal timestep. This is useful for e.g. truncation.\n    \"\"\"\n    super().__init__(env)\n    self.next_obs_in_extras = next_obs_in_extras\n    if next_obs_in_extras:\n        self._maybe_add_obs_to_extras = add_obs_to_extras\n    else:\n        self._maybe_add_obs_to_extras = lambda timestep: timestep  # no-op\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapAutoResetWrapper.render","title":"<code>render(state)</code>","text":"<p>Render the first environment state of the given batch. The remaining elements of the batched state are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the current dynamics of the environment.</p> required Source code in <code>jumanji/wrappers.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Render the first environment state of the given batch.\n    The remaining elements of the batched state are ignored.\n\n    Args:\n        state: State object containing the current dynamics of the environment.\n    \"\"\"\n    state_0 = tree_utils.tree_slice(state, 0)\n    return super().render(state_0)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapAutoResetWrapper.reset","title":"<code>reset(key)</code>","text":"<p>Resets a batch of environments to initial states.</p> <p>The first dimension of the key will dictate the number of concurrent environments.</p> <p>To obtain a key with the right first dimension, you may call <code>jax.random.split</code> on key with the parameter <code>num</code> representing the number of concurrent environments.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random keys used to reset the environments where the first dimension is the number of desired environments.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environments,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the first timesteps returned by the environments,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets a batch of environments to initial states.\n\n    The first dimension of the key will dictate the number of concurrent environments.\n\n    To obtain a key with the right first dimension, you may call `jax.random.split` on key\n    with the parameter `num` representing the number of concurrent environments.\n\n    Args:\n        key: random keys used to reset the environments where the first dimension is the number\n            of desired environments.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environments,\n        timestep: `TimeStep` object corresponding the first timesteps returned by the\n            environments,\n    \"\"\"\n    state, timestep = jax.vmap(self._env.reset)(key)\n    timestep = self._maybe_add_obs_to_extras(timestep)\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapAutoResetWrapper.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of all environments' dynamics. It automatically resets environment(s) in which episodes have terminated.</p> <p>The first dimension of the state will dictate the number of concurrent environments.</p> <p>See <code>VmapAutoResetWrapper.reset</code> for more details on how to get a state of concurrent environments.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environments.</p> required <code>action</code> <code>Array</code> <p><code>Array</code> containing the actions to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the next states of the environments.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the timesteps returned by the environments.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of all environments' dynamics. It automatically resets environment(s)\n    in which episodes have terminated.\n\n    The first dimension of the state will dictate the number of concurrent environments.\n\n    See `VmapAutoResetWrapper.reset` for more details on how to get a state of concurrent\n    environments.\n\n    Args:\n        state: `State` object containing the dynamics of the environments.\n        action: `Array` containing the actions to take.\n\n    Returns:\n        state: `State` object corresponding to the next states of the environments.\n        timestep: `TimeStep` object corresponding the timesteps returned by the environments.\n    \"\"\"\n    # Vmap homogeneous computation (parallelizable).\n    state, timestep = jax.vmap(self._env.step)(state, action)\n    # Map heterogeneous computation (non-parallelizable).\n    state, timestep = jax.lax.map(lambda args: self._maybe_reset(*args), (state, timestep))\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapWrapper","title":"<code>VmapWrapper(env)</code>","text":"<p>               Bases: <code>Wrapper[State, ActionSpec, Observation]</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>Vectorized Jax env. Please note that all methods that return arrays do not return a batch dimension because the batch size is not known to the VmapWrapper. Methods that omit the batch dimension include: - observation_spec - action_spec - reward_spec - discount_spec</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(self, env: Environment[State, ActionSpec, Observation]):\n    self._env = env\n    super().__init__()\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapWrapper.render","title":"<code>render(state)</code>","text":"<p>Render the first environment state of the given batch. The remaining elements of the batched state are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the current dynamics of the environment.</p> required Source code in <code>jumanji/wrappers.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Render the first environment state of the given batch.\n    The remaining elements of the batched state are ignored.\n\n    Args:\n        state: State object containing the current dynamics of the environment.\n    \"\"\"\n    state_0 = tree_utils.tree_slice(state, 0)\n    return super().render(state_0)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapWrapper.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment to an initial state.</p> <p>The first dimension of the key will dictate the number of concurrent environments.</p> <p>To obtain a key with the right first dimension, you may call <code>jax.random.split</code> on key with the parameter <code>num</code> representing the number of concurrent environments.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random keys used to reset the environments where the first dimension is the number of desired environments.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environments,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the first timesteps returned by the environments,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\n\n    The first dimension of the key will dictate the number of concurrent environments.\n\n    To obtain a key with the right first dimension, you may call `jax.random.split` on key\n    with the parameter `num` representing the number of concurrent environments.\n\n    Args:\n        key: random keys used to reset the environments where the first dimension is the number\n            of desired environments.\n\n    Returns:\n        state: State object corresponding to the new state of the environments,\n        timestep: TimeStep object corresponding the first timesteps returned by the\n            environments,\n    \"\"\"\n    state, timestep = jax.vmap(self._env.reset)(key)\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.VmapWrapper.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>The first dimension of the state will dictate the number of concurrent environments.</p> <p>See <code>VmapWrapper.reset</code> for more details on how to get a state of concurrent environments.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environments.</p> required <code>action</code> <code>Array</code> <p>Array containing the actions to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the next states of the environments,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the timesteps returned by the environments,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    The first dimension of the state will dictate the number of concurrent environments.\n\n    See `VmapWrapper.reset` for more details on how to get a state of concurrent\n    environments.\n\n    Args:\n        state: State object containing the dynamics of the environments.\n        action: Array containing the actions to take.\n\n    Returns:\n        state: State object corresponding to the next states of the environments,\n        timestep: TimeStep object corresponding the timesteps returned by the environments,\n    \"\"\"\n    state, timestep = jax.vmap(self._env.step)(state, action)\n    return state, timestep\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper","title":"<code>Wrapper(env)</code>","text":"<p>               Bases: <code>Environment[State, ActionSpec, Observation]</code>, <code>Generic[State, ActionSpec, Observation]</code></p> <p>Wraps the environment to allow modular transformations. Source: https://github.com/google/brax/blob/main/brax/envs/env.py#L72</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def __init__(self, env: Environment[State, ActionSpec, Observation]):\n    self._env = env\n    super().__init__()\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.discount_spec","title":"<code>discount_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the discount spec.</p>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.reward_spec","title":"<code>reward_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the reward spec.</p>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.unwrapped","title":"<code>unwrapped</code>  <code>property</code>","text":"<p>Returns the wrapped env.</p>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    return self._env.close()\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.render","title":"<code>render(state)</code>","text":"<p>Compute render frames during initialisation of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required Source code in <code>jumanji/wrappers.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Compute render frames during initialisation of the environment.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n    \"\"\"\n    return self._env.render(state)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment to an initial state.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the first timestep returned by the environment,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: State object corresponding to the new state of the environment,\n        timestep: TimeStep object corresponding the first timestep returned by the environment,\n    \"\"\"\n    return self._env.reset(key)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.Wrapper.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the action to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the next state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the timestep returned by the environment,</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the action to take.\n\n    Returns:\n        state: State object corresponding to the next state of the environment,\n        timestep: TimeStep object corresponding the timestep returned by the environment,\n    \"\"\"\n    return self._env.step(state, action)\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.add_obs_to_extras","title":"<code>add_obs_to_extras(timestep)</code>","text":"<p>Place the observation in timestep.extras[NEXT_OBS_KEY_IN_EXTRAS]. Used when auto-resetting to store the observation from the terminal TimeStep (useful for e.g. truncation).</p> <p>Parameters:</p> Name Type Description Default <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object containing the timestep returned by the environment.</p> required <p>Returns:</p> Type Description <code>TimeStep[Observation]</code> <p>timestep where the observation is placed in timestep.extras[\"next_obs\"].</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def add_obs_to_extras(timestep: TimeStep[Observation]) -&gt; TimeStep[Observation]:\n    \"\"\"Place the observation in timestep.extras[NEXT_OBS_KEY_IN_EXTRAS].\n    Used when auto-resetting to store the observation from the terminal TimeStep (useful for\n    e.g. truncation).\n\n    Args:\n        timestep: TimeStep object containing the timestep returned by the environment.\n\n    Returns:\n        timestep where the observation is placed in timestep.extras[\"next_obs\"].\n    \"\"\"\n    extras = timestep.extras\n    extras[NEXT_OBS_KEY_IN_EXTRAS] = timestep.observation\n    return timestep.replace(extras=extras)  # type: ignore\n</code></pre>"},{"location":"api/wrappers/#jumanji.wrappers.jumanji_to_gym_obs","title":"<code>jumanji_to_gym_obs(observation)</code>","text":"<p>Convert a Jumanji observation into a gym observation.</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>Observation</code> <p>JAX pytree with (possibly nested) containers that either have the <code>__dict__</code> or <code>_asdict</code> methods implemented.</p> required <p>Returns:</p> Type Description <code>GymObservation</code> <p>Numpy array or nested dictionary of numpy arrays.</p> Source code in <code>jumanji/wrappers.py</code> <pre><code>def jumanji_to_gym_obs(observation: Observation) -&gt; GymObservation:\n    \"\"\"Convert a Jumanji observation into a gym observation.\n\n    Args:\n        observation: JAX pytree with (possibly nested) containers that\n            either have the `__dict__` or `_asdict` methods implemented.\n\n    Returns:\n        Numpy array or nested dictionary of numpy arrays.\n    \"\"\"\n    if isinstance(observation, jnp.ndarray):\n        return np.asarray(observation)\n    elif hasattr(observation, \"__dict__\"):\n        # Applies to various containers including `chex.dataclass`\n        return {key: jumanji_to_gym_obs(value) for key, value in vars(observation).items()}\n    elif hasattr(observation, \"_asdict\"):\n        # Applies to `NamedTuple` container.\n        return {\n            key: jumanji_to_gym_obs(value)\n            for key, value in observation._asdict().items()  # type: ignore\n        }\n    else:\n        raise NotImplementedError(\n            \"Conversion only implemented for JAX pytrees with (possibly nested) containers \"\n            \"that either have the `__dict__` or `_asdict` methods implemented.\"\n        )\n</code></pre>"},{"location":"api/environments/bin_pack/","title":"BinPack","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>Problem of 3D bin packing, where a set of items have to be placed in a 3D container with the goal of maximizing its volume utilization. This environment only supports 1 bin, meaning it is equivalent to the 3D-knapsack problem. We use the Empty Maximal Space (EMS) formulation of this problem. An EMS is a 3D-rectangular space that lives inside the container and has the following properties:     - It does not intersect any items, and it is not fully included into any other EMSs.     - It is defined by 2 3D-points, hence 6 coordinates (x1, x2, y1, y2, z1, z2),     the first point corresponding to its bottom-left location while the second defining its     top-right corner.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>ems: <code>EMS</code> tree of jax arrays (float if <code>normalize_dimensions</code> else int32) each of     shape (obs_num_ems,),     coordinates of all EMSs at the current timestep.</li> <li>ems_mask: jax array (bool) of shape (obs_num_ems,)     indicates the EMSs that are valid.</li> <li>items: <code>Item</code> tree of jax arrays (float if <code>normalize_dimensions</code> else int32) each of     shape (max_num_items,),     characteristics of all items for this instance.</li> <li>items_mask: jax array (bool) of shape (max_num_items,)     indicates the items that are valid.</li> <li>items_placed: jax array (bool) of shape (max_num_items,)     indicates the items that have been placed so far.</li> <li>action_mask: jax array (bool) of shape (obs_num_ems, max_num_items)     mask of the joint action space: <code>True</code> if the action (ems_id, item_id) is valid.</li> </ul> </li> <li> <p>action: <code>MultiDiscreteArray</code> (int32) of shape (obs_num_ems, max_num_items).</p> <ul> <li>ems_id: int between 0 and obs_num_ems - 1 (included).</li> <li>item_id: int between 0 and max_num_items - 1 (included).</li> </ul> </li> <li> <p>reward: jax array (float) of shape (), could be either:</p> <ul> <li>dense: increase in volume utilization of the container due to packing the chosen item.</li> <li>sparse: volume utilization of the container at the end of the episode.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>if no action can be performed, i.e. no items fit in any EMSs, or all items have been     packed.</li> <li>if an invalid action is taken, i.e. an item that does not fit in an EMS or one that is     already packed.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>container: space defined by 2 points, i.e. 6 coordinates.</li> <li>ems: empty maximal spaces (EMSs) in the container, each defined by 2 points     (6 coordinates).</li> <li>ems_mask: array of booleans that indicate the EMSs that are valid.</li> <li>items: defined by 3 attributes (x, y, z).</li> <li>items_mask: array of booleans that indicate the items that can be packed.</li> <li>items_placed: array of booleans that indicate the items that have been placed so far.</li> <li>items_location: locations of items in the container, defined by 3 coordinates (x, y, x).</li> <li>action_mask: array of booleans that indicate the valid actions,     i.e. EMSs and items that can be chosen.</li> <li>sorted_ems_indexes: EMS indexes that are sorted by decreasing volume order.</li> <li>key: random key used for auto-reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import BinPack\nenv = BinPack()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>BinPack</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>RandomGenerator</code>, <code>ToyGenerator</code>, <code>CSVGenerator</code>]. Defaults to <code>RandomGenerator</code> that generates up to 20 items maximum and that can handle 40 EMSs.</p> <code>None</code> <code>obs_num_ems</code> <code>int</code> <p>number of EMSs (possible spaces in which to place an item) to show to the agent. If <code>obs_num_ems</code> is smaller than <code>generator.max_num_ems</code>, the first <code>obs_num_ems</code> largest EMSs (in terms of volume) will be returned in the observation. The good number heavily depends on the number of items (given by the instance generator). Default to 40 EMSs observable.</p> <code>40</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>compute the reward based on the current state, the chosen action, the next state, whether the transition is valid and if it is terminal. Implemented options are [<code>DenseReward</code>, <code>SparseReward</code>]. In each case, the total return at the end of an episode is the volume utilization of the container. Defaults to <code>DenseReward</code>.</p> <code>None</code> <code>normalize_dimensions</code> <code>bool</code> <p>if True, the observation is normalized (float) along each dimension into a unit cubic container. If False, the observation is returned in millimeters, i.e. integers (for both items and EMSs). Default to True.</p> <code>True</code> <code>debug</code> <code>bool</code> <p>if True, will add to timestep.extras an <code>invalid_ems_from_env</code> field that checks if an invalid EMS was created by the environment, which should not happen. Computing this metric slows down the environment. Default to False.</p> <code>False</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>BinPackViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    obs_num_ems: int = 40,\n    reward_fn: Optional[RewardFn] = None,\n    normalize_dimensions: bool = True,\n    debug: bool = False,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates a `BinPack` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment\n            instance. Implemented options are [`RandomGenerator`, `ToyGenerator`,\n            `CSVGenerator`]. Defaults to `RandomGenerator` that generates up to 20 items maximum\n            and that can handle 40 EMSs.\n        obs_num_ems: number of EMSs (possible spaces in which to place an item) to show to the\n            agent. If `obs_num_ems` is smaller than `generator.max_num_ems`, the first\n            `obs_num_ems` largest EMSs (in terms of volume) will be returned in the observation.\n            The good number heavily depends on the number of items (given by the instance\n            generator). Default to 40 EMSs observable.\n        reward_fn: compute the reward based on the current state, the chosen action, the next\n            state, whether the transition is valid and if it is terminal. Implemented options\n            are [`DenseReward`, `SparseReward`]. In each case, the total return at the end of\n            an episode is the volume utilization of the container. Defaults to `DenseReward`.\n        normalize_dimensions: if True, the observation is normalized (float) along each\n            dimension into a unit cubic container. If False, the observation is returned in\n            millimeters, i.e. integers (for both items and EMSs). Default to True.\n        debug: if True, will add to timestep.extras an `invalid_ems_from_env` field that checks\n            if an invalid EMS was created by the environment, which should not happen. Computing\n            this metric slows down the environment. Default to False.\n        viewer: `Viewer` used for rendering. Defaults to `BinPackViewer` with \"human\" render\n            mode.\n    \"\"\"\n    self.generator = generator or RandomGenerator(\n        max_num_items=20,\n        max_num_ems=40,\n        split_num_same_items=2,\n    )\n    self.obs_num_ems = obs_num_ems\n    self.reward_fn = reward_fn or DenseReward()\n    self.normalize_dimensions = normalize_dimensions\n    super().__init__()\n    self._viewer = viewer or BinPackViewer(\"BinPack\", render_mode=\"human\")\n    self.debug = debug\n</code></pre>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the action expected by the <code>BinPack</code> environment.</p> <p>Returns:</p> Type Description <code>MultiDiscreteArray</code> <p>MultiDiscreteArray (int32) of shape (obs_num_ems, max_num_items).</p> <code>MultiDiscreteArray</code> <ul> <li>ems_id: int between 0 and obs_num_ems - 1 (included).</li> </ul> <code>MultiDiscreteArray</code> <ul> <li>item_id: int between 0 and max_num_items - 1 (included).</li> </ul>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>BinPack</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>ems:</li> <li>if normalize_dimensions:     tree of BoundedArray (float) of shape (obs_num_ems,).</li> <li>else:     tree of BoundedArray (int32) of shape (obs_num_ems,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>ems_mask: BoundedArray (bool) of shape (obs_num_ems,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>items:</li> <li>if normalize_dimensions:     tree of BoundedArray (float) of shape (max_num_items,).</li> <li>else:     tree of BoundedArray (int32) of shape (max_num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>items_mask: BoundedArray (bool) of shape (max_num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>items_placed: BoundedArray (bool) of shape (max_num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (obs_num_ems, max_num_items).</li> </ul>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>BinPack</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `BinPack` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the current dynamics of the environment.</p> required Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment.\n\n    Args:\n        state: State object containing the current dynamics of the environment.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment by calling the instance generator for a new instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment after a reset.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the first timestep returned by the environment after a reset. Also contains the following metrics in the <code>extras</code> field: - volume_utilization: utilization (in [0, 1]) of the container. - packed_items: number of items that are packed in the container. - ratio_packed_items: ratio (in [0, 1]) of items that are packed in the container. - active_ems: number of active EMSs in the current instance. - invalid_action: True if the action that was just taken was invalid. - invalid_ems_from_env (optional): True if the environment produced an EMS that was     invalid. Only available in debug mode.</p> Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment by calling the instance generator for a new instance.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environment after a reset.\n        timestep: `TimeStep` object corresponding the first timestep returned by the environment\n            after a reset. Also contains the following metrics in the `extras` field:\n            - volume_utilization: utilization (in [0, 1]) of the container.\n            - packed_items: number of items that are packed in the container.\n            - ratio_packed_items: ratio (in [0, 1]) of items that are packed in the container.\n            - active_ems: number of active EMSs in the current instance.\n            - invalid_action: True if the action that was just taken was invalid.\n            - invalid_ems_from_env (optional): True if the environment produced an EMS that was\n                invalid. Only available in debug mode.\n    \"\"\"\n    # Generate a new instance.\n    state = self.generator(key)\n\n    # Make the observation.\n    state, observation, extras = self._make_observation_and_extras(state)\n\n    extras.update(invalid_action=jnp.array(False))\n    if self.debug:\n        extras.update(invalid_ems_from_env=jnp.array(False))\n    timestep = restart(observation, extras)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/bin_pack/#jumanji.environments.packing.bin_pack.env.BinPack.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics. If the action is invalid, the state is not updated, i.e. the action is not taken, and the episode terminates.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the data of the current instance.</p> required <code>action</code> <code>Array</code> <p>jax array (int32) of shape (2,): (ems_id, item_id). This means placing the given item at the location of the given EMS. If the action is not valid, the flag <code>invalid_action</code> will be set to True in <code>timestep.extras</code> and the episode terminates.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the timestep returned by the environment. Also contains metrics in the <code>extras</code> field: - volume_utilization: utilization (in [0, 1]) of the container. - packed_items: number of items that are packed in the container. - ratio_packed_items: ratio (in [0, 1]) of items that are packed in the container. - active_ems: number of EMSs in the current instance. - invalid_action: True if the action that was just taken was invalid. - invalid_ems_from_env (optional): True if the environment produced an EMS that was     invalid. Only available in debug mode.</p> Source code in <code>jumanji/environments/packing/bin_pack/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics. If the action is invalid, the state\n    is not updated, i.e. the action is not taken, and the episode terminates.\n\n    Args:\n        state: `State` object containing the data of the current instance.\n        action: jax array (int32) of shape (2,): (ems_id, item_id). This means placing the given\n            item at the location of the given EMS. If the action is not valid, the flag\n            `invalid_action` will be set to True in `timestep.extras` and the episode\n            terminates.\n\n    Returns:\n        state: `State` object corresponding to the next state of the environment.\n        timestep: `TimeStep` object corresponding to the timestep returned by the environment.\n            Also contains metrics in the `extras` field:\n            - volume_utilization: utilization (in [0, 1]) of the container.\n            - packed_items: number of items that are packed in the container.\n            - ratio_packed_items: ratio (in [0, 1]) of items that are packed in the container.\n            - active_ems: number of EMSs in the current instance.\n            - invalid_action: True if the action that was just taken was invalid.\n            - invalid_ems_from_env (optional): True if the environment produced an EMS that was\n                invalid. Only available in debug mode.\n    \"\"\"\n    action_is_valid = state.action_mask[tuple(action)]  # type: ignore\n\n    obs_ems_id, item_id = action\n    ems_id = state.sorted_ems_indexes[obs_ems_id]\n\n    # Pack the item if the provided action is valid.\n    next_state = jax.lax.cond(\n        action_is_valid,\n        lambda s: self._pack_item(s, ems_id, item_id),\n        lambda s: s,\n        state,\n    )\n\n    # Make the observation.\n    next_state, observation, extras = self._make_observation_and_extras(next_state)\n\n    done = ~jnp.any(next_state.action_mask) | ~action_is_valid\n    reward = self.reward_fn(state, action, next_state, action_is_valid, done)\n\n    extras.update(invalid_action=~action_is_valid)\n    if self.debug:\n        ems_are_all_valid = self._ems_are_all_valid(next_state)\n        extras.update(invalid_ems_from_env=~ems_are_all_valid)\n\n    timestep = jax.lax.cond(\n        done,\n        lambda: termination(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n        lambda: transition(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/cleaner/","title":"Cleaner","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>A JAX implementation of the 'Cleaner' game where multiple agents have to clean all tiles of a maze.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>grid: jax array (int8) of shape (num_rows, num_cols)     contains the state of the board: 0 for dirty tile, 1 for clean tile, 2 for wall.</li> <li>agents_locations: jax array (int32) of shape (num_agents, 2)     contains the location of each agent on the board.</li> <li>action_mask: jax array (bool) of shape (num_agents, 4)     indicates for each agent if each of the four actions (up, right, down, left) is allowed.</li> <li>step_count: (int32)     the number of step since the beginning of the episode.</li> </ul> </li> <li> <p>action: jax array (int32) of shape (num_agents,)     the action for each agent: (0: up, 1: right, 2: down, 3: left)</p> </li> <li> <p>reward: jax array (float) of shape ()     +1 every time a tile is cleaned and a configurable penalty (-0.5 by default) for     each timestep.</p> </li> <li> <p>episode termination:</p> <ul> <li>All tiles are clean.</li> <li>The number of steps is greater than the limit.</li> <li>An invalid action is selected for any of the agents.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>grid: jax array (int8) of shape (num_rows, num_cols)     contains the current state of the board: 0 for dirty tile, 1 for clean tile, 2 for wall.</li> <li>agents_locations: jax array (int32) of shape (num_agents, 2)     contains the location of each agent on the board.</li> <li>action_mask: jax array (bool) of shape (num_agents, 4)     indicates for each agent if each of the four actions (up, right, down, left) is allowed.</li> <li>step_count: jax array (int32) of shape ()     the number of steps since the beginning of the episode.</li> <li>key: jax array (uint) of shape (2,)     jax random generation key. Ignored since the environment is deterministic.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Cleaner\nenv = Cleaner()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>Cleaner</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>RandomGenerator</code>]. Defaults to <code>RandomGenerator</code> with <code>num_rows=10</code>, <code>num_cols=10</code> and <code>num_agents=3</code>.</p> <code>None</code> <code>time_limit</code> <code>Optional[int]</code> <p>max number of steps in an episode. Defaults to <code>num_rows * num_cols</code>.</p> <code>None</code> <code>penalty_per_timestep</code> <code>float</code> <p>the penalty returned at each timestep in the reward.</p> <code>0.5</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>CleanerViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    time_limit: Optional[int] = None,\n    penalty_per_timestep: float = 0.5,\n    viewer: Optional[Viewer[State]] = None,\n) -&gt; None:\n    \"\"\"Instantiates a `Cleaner` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`RandomGenerator`]. Defaults to `RandomGenerator` with\n            `num_rows=10`, `num_cols=10` and `num_agents=3`.\n        time_limit: max number of steps in an episode. Defaults to `num_rows * num_cols`.\n        penalty_per_timestep: the penalty returned at each timestep in the reward.\n        viewer: `Viewer` used for rendering. Defaults to `CleanerViewer` with \"human\" render\n            mode.\n    \"\"\"\n    self.generator = generator or RandomGenerator(num_rows=10, num_cols=10, num_agents=3)\n    self.num_agents = self.generator.num_agents\n    self.num_rows = self.generator.num_rows\n    self.num_cols = self.generator.num_cols\n    self.grid_shape = (self.num_rows, self.num_cols)\n    self.time_limit = time_limit or (self.num_rows * self.num_cols)\n    super().__init__()\n    self.penalty_per_timestep = penalty_per_timestep\n\n    # Create viewer used for rendering\n    self._viewer = viewer or CleanerViewer(\"Cleaner\", render_mode=\"human\")\n</code></pre>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specification of the action for the <code>Cleaner</code> environment.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p>a <code>specs.MultiDiscreteArray</code> spec.</p>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specification of the observation of the <code>Cleaner</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code>, consisting of the fields: - grid: BoundedArray (int8) of shape (num_rows, num_cols). Values     are between 0 and 2 (inclusive). - agent_locations_spec: BoundedArray (int32) of shape (num_agents, 2).     Maximum value for the first column is num_rows, and maximum value     for the second is num_cols. - action_mask: BoundedArray (bool) of shape (num_agent, 4). - step_count: BoundedArray (int32) of shape ().</p>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>Cleaner</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `Cleaner` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment.\n\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.reset","title":"<code>reset(key)</code>","text":"<p>Reset the environment to its initial state.</p> <p>All the tiles except upper left are dirty, and the agents start in the upper left corner of the grid.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment after a reset.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the first timestep returned by the environment after a reset.</p> Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Reset the environment to its initial state.\n\n    All the tiles except upper left are dirty, and the agents start in the upper left\n    corner of the grid.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environment after a reset.\n        timestep: `TimeStep` object corresponding to the first timestep returned by the\n            environment after a reset.\n    \"\"\"\n    # Agents start in upper left corner\n    agents_locations = jnp.zeros((self.num_agents, 2), int)\n\n    state = self.generator(key)\n\n    # Create the action mask and update the state\n    state.action_mask = self._compute_action_mask(state.grid, agents_locations)\n\n    observation = self._observation_from_state(state)\n\n    extras = self._compute_extras(state)\n    timestep = restart(observation, extras)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/cleaner/#jumanji.environments.routing.cleaner.env.Cleaner.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>If an action is invalid, the corresponding agent does not move and the episode terminates.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>current environment state.</p> required <code>action</code> <code>Array</code> <p>Jax array of shape (num_agents,). Each agent moves one step in the specified direction (0: up, 1: right, 2: down, 3: left).</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/cleaner/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    If an action is invalid, the corresponding agent does not move and\n    the episode terminates.\n\n    Args:\n        state: current environment state.\n        action: Jax array of shape (num_agents,). Each agent moves one step in\n            the specified direction (0: up, 1: right, 2: down, 3: left).\n\n    Returns:\n        state: `State` object corresponding to the next state of the environment.\n        timestep: `TimeStep` object corresponding to the timestep returned by the environment.\n    \"\"\"\n    is_action_valid = self._is_action_valid(action, state.action_mask)\n\n    agents_locations = self._update_agents_locations(\n        state.agents_locations, action, is_action_valid\n    )\n\n    grid = self._clean_tiles_containing_agents(state.grid, agents_locations)\n\n    prev_state = state\n\n    state = State(\n        agents_locations=agents_locations,\n        grid=grid,\n        action_mask=self._compute_action_mask(grid, agents_locations),\n        step_count=state.step_count + 1,\n        key=state.key,\n    )\n\n    reward = self._compute_reward(prev_state, state)\n\n    observation = self._observation_from_state(state)\n\n    done = self._should_terminate(state, is_action_valid)\n\n    extras = self._compute_extras(state)\n    # Return either a MID or a LAST timestep depending on done.\n    timestep = jax.lax.cond(\n        done,\n        lambda reward, observation, extras: termination(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n        lambda reward, observation, extras: transition(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n        reward,\n        observation,\n        extras,\n    )\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/connector/","title":"Connector","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>The <code>Connector</code> environment is a gridworld problem where multiple pairs of points (sets) must be connected without overlapping the paths taken by any other set. This is achieved by allowing certain points to move to an adjacent cell at each step. However, each time a point moves it leaves an impassable trail behind it. The goal is to connect all sets.</p> <ul> <li> <p>observation - <code>Observation</code></p> <ul> <li>action mask: jax array (bool) of shape (num_agents, 5).</li> <li>step_count: jax array (int32) of shape ()     the current episode step.</li> <li>grid: jax array (int32) of shape (grid_size, grid_size)<ul> <li>with 2 agents you might have a grid like this:   4 0 1   5 0 1   6 3 2   which means agent 1 has moved from the top right of the grid down and is currently in   the bottom right corner and is aiming to get to the middle bottom cell. Agent 2   started in the top left and moved down once towards its target in the bottom left.</li> </ul> </li> </ul> </li> <li> <p>action: jax array (int32) of shape (num_agents,):</p> <ul> <li>can take the values [0,1,2,3,4] which correspond to [No Op, Up, Right, Down, Left].</li> <li>each value in the array corresponds to an agent's action.</li> </ul> </li> <li> <p>reward: jax array (float) of shape (num_agents,):</p> <ul> <li>dense: for each agent the reward is 1 for each successful connection on that step.         Additionally, each pair of points that have not connected receives a         penalty reward of -0.03.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>all agents either can't move (no available actions) or have connected to their target.</li> <li>the time limit is reached.</li> </ul> </li> <li> <p>state: State:</p> <ul> <li>key: jax PRNG key used to randomly spawn agents and targets.</li> <li>grid: jax array (int32) of shape (grid_size, grid_size) giving the observation.</li> <li>step_count: jax array (int32) of shape () number of steps elapsed in the current episode.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Connector\nenv = Connector()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Create the <code>Connector</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>UniformRandomGenerator</code>, <code>RandomWalkGenerator</code>]. Defaults to <code>RandomWalkGenerator</code> with <code>grid_size=10</code> and <code>num_agents=10</code>.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>class of type <code>RewardFn</code>, whose <code>__call__</code> is used as a reward function. Implemented options are [<code>DenseRewardFn</code>]. Defaults to <code>DenseRewardFn</code>.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>the number of steps allowed before an episode terminates. Defaults to 50.</p> <code>50</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>ConnectorViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    time_limit: int = 50,\n    viewer: Optional[Viewer[State]] = None,\n) -&gt; None:\n    \"\"\"Create the `Connector` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`UniformRandomGenerator`, `RandomWalkGenerator`].\n            Defaults to `RandomWalkGenerator` with `grid_size=10` and `num_agents=10`.\n        reward_fn: class of type `RewardFn`, whose `__call__` is used as a reward function.\n            Implemented options are [`DenseRewardFn`]. Defaults to `DenseRewardFn`.\n        time_limit: the number of steps allowed before an episode terminates. Defaults to 50.\n        viewer: `Viewer` used for rendering. Defaults to `ConnectorViewer` with \"human\" render\n            mode.\n    \"\"\"\n    self._generator = generator or RandomWalkGenerator(grid_size=10, num_agents=10)\n    self._reward_fn = reward_fn or DenseRewardFn()\n    self.time_limit = time_limit\n    self.num_agents = self._generator.num_agents\n    self.grid_size = self._generator.grid_size\n    super().__init__()\n    self._agent_ids = jnp.arange(self.num_agents)\n    self._viewer = viewer or ConnectorViewer(\"Connector\", self.num_agents, render_mode=\"human\")\n</code></pre>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec for the Connector environment.</p> <p>5 actions: [0,1,2,3,4] -&gt; [No Op, Up, Right, Down, Left]. Since this is an environment with a multi-dimensional action space, it expects an array of actions of shape (num_agents,).</p> <p>Returns:</p> Name Type Description <code>observation_spec</code> <code>MultiDiscreteArray</code> <p><code>MultiDiscreteArray</code> of shape (num_agents,).</p>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.discount_spec","title":"<code>discount_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns: discount per agent.</p>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>Connector</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>grid: BoundedArray (int32) of shape (grid_size, grid_size).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_agents, 5).</li> </ul> <code>Spec[Observation]</code> <ul> <li>step_count: BoundedArray (int32) of shape ().</li> </ul>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.reward_spec","title":"<code>reward_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns: a reward per agent.</p>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Create an animation from a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation that can export to gif, mp4, or render with HTML.</p> Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Create an animation from a sequence of states.\n\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation that can export to gif, mp4, or render with HTML.\n    \"\"\"\n    grids = [state.grid for state in states]\n    return self._viewer.animate(grids, interval, save_path)\n</code></pre>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment.\n\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state.grid)\n</code></pre>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the connector grid.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the initial environment timestep.</p> Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: used to randomly generate the connector grid.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environment.\n        timestep: `TimeStep` object corresponding to the initial environment timestep.\n    \"\"\"\n    state = self._generator(key)\n\n    action_mask = jax.vmap(self._get_action_mask, (0, None))(state.agents, state.grid)\n    observation = Observation(\n        grid=state.grid,\n        action_mask=action_mask,\n        step_count=state.step_count,\n    )\n    extras = self._get_extras(state)\n    timestep = restart(observation=observation, extras=extras, shape=(self.num_agents,))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/connector/#jumanji.environments.routing.connector.env.Connector.step","title":"<code>step(state, action)</code>","text":"<p>Perform an environment step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the actions to take for each agent. - 0 no op - 1 move up - 2 move right - 3 move down - 4 move left</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/connector/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Perform an environment step.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the actions to take for each agent.\n            - 0 no op\n            - 1 move up\n            - 2 move right\n            - 3 move down\n            - 4 move left\n\n    Returns:\n        state: `State` object corresponding to the next state of the environment.\n        timestep: `TimeStep` object corresponding the timestep returned by the environment.\n    \"\"\"\n    agents, grid = self._step_agents(state, action)\n    new_state = State(grid=grid, step_count=state.step_count + 1, agents=agents, key=state.key)\n\n    # Construct timestep: get reward, legal actions and done\n    reward = self._reward_fn(state, action, new_state)\n    action_mask = jax.vmap(self._get_action_mask, (0, None))(agents, grid)\n    observation = Observation(\n        grid=grid, action_mask=action_mask, step_count=new_state.step_count\n    )\n\n    done = jax.vmap(connected_or_blocked)(agents, action_mask)\n    discount = (1 - done).astype(float)\n    extras = self._get_extras(new_state)\n    timestep = jax.lax.cond(\n        jnp.all(done) | (new_state.step_count &gt;= self.time_limit),\n        lambda: termination(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n            shape=(self.num_agents,),\n        ),\n        lambda: transition(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n            discount=discount,\n            shape=(self.num_agents,),\n        ),\n    )\n\n    return new_state, timestep\n</code></pre>"},{"location":"api/environments/cvrp/","title":"CVRP","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Capacitated Vehicle Routing Problem (CVRP) environment as described in [1].</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>coordinates: jax array (float) of shape (num_nodes + 1, 2)     the coordinates of each node and the depot.</li> <li>demands: jax array (float) of shape (num_nodes + 1,)     the associated cost of each node and the depot (0.0 for the depot).</li> <li>unvisited_nodes: jax array (bool) of shape (num_nodes + 1,)     indicates nodes that remain to be visited.</li> <li>position: jax array (int32) of shape ()     the index of the last visited node.</li> <li>trajectory: jax array (int32) of shape (2 * num_nodes,)     array of node indices defining the route (set to DEPOT_IDX if not filled yet).</li> <li>capacity: jax array (float) of shape ()     the current capacity of the vehicle.</li> <li>action_mask: jax array (bool) of shape (num_nodes + 1,)     binary mask (False/True &lt;--&gt; invalid/valid action).</li> </ul> </li> <li> <p>action: jax array (int32) of shape ()     [0, ..., num_nodes] -&gt; node to visit. 0 corresponds to visiting the depot.</p> </li> <li> <p>reward: jax array (float) of shape (), could be either:</p> <ul> <li>dense: the negative distance between the current node and the chosen next node to go to.     For the last node, it also includes the distance to the depot to complete the tour.</li> <li>sparse: the negative tour length at the end of the episode. The tour length is defined     as the sum of the distances between consecutive nodes. In both cases, the reward is a large negative penalty of <code>-2 * num_nodes * sqrt(2)</code> if the action is invalid, e.g. a previously selected node other than the depot is selected again.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>if no action can be performed, i.e. all nodes have been visited.</li> <li>if an invalid action is taken, i.e. a previously visited city other than the depot is     chosen.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>coordinates: jax array (float) of shape (num_nodes + 1, 2)     the coordinates of each node and the depot.</li> <li>demands: jax array (int32) of shape (num_nodes + 1,)     the associated cost of each node and the depot (0.0 for the depot).</li> <li>position: jax array (int32)     the index of the last visited node.</li> <li>capacity: jax array (int32)     the current capacity of the vehicle.</li> <li>visited_mask: jax array (bool) of shape (num_nodes + 1,)     binary mask (False/True &lt;--&gt; not visited/visited).</li> <li>trajectory: jax array (int32) of shape (2 * num_nodes,)     identifiers of the nodes that have been visited (set to DEPOT_IDX if not filled yet).</li> <li>num_visits: int32     number of actions that have been taken (i.e., unique visits).</li> </ul> </li> </ul> <p>[1] Toth P., Vigo D. (2014). \"Vehicle routing: problems, methods, and applications\".</p> <pre><code>from jumanji.environments import CVRP\nenv = CVRP()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>CVRP</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. The default option is 'UniformGenerator' which randomly generates CVRP instances with 20 cities sampled from a uniform distribution, a maximum vehicle capacity of 30, and a maximum city demand of 10.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p><code>RewardFn</code> whose <code>__call__</code> method computes the reward of an environment transition. The function must compute the reward based on the current state, the chosen action, the next state and whether the action is valid. Implemented options are [<code>DenseReward</code>, <code>SparseReward</code>]. Defaults to <code>DenseReward</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>CVRPViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates a `CVRP` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            The default option is 'UniformGenerator' which randomly generates\n            CVRP instances with 20 cities sampled from a uniform distribution,\n            a maximum vehicle capacity of 30, and a maximum city demand of 10.\n        reward_fn: `RewardFn` whose `__call__` method computes the reward of an environment\n            transition. The function must compute the reward based on the current state,\n            the chosen action, the next state and whether the action is valid.\n            Implemented options are [`DenseReward`, `SparseReward`]. Defaults to `DenseReward`.\n        viewer: `Viewer` used for rendering. Defaults to `CVRPViewer` with \"human\" render mode.\n    \"\"\"\n\n    self.generator = generator or UniformGenerator(\n        num_nodes=20,\n        max_capacity=30,\n        max_demand=10,\n    )\n    self.num_nodes = self.generator.num_nodes\n    super().__init__()\n    self.max_capacity = self.generator.max_capacity\n    self.max_demand = self.generator.max_demand\n    if self.max_capacity &lt; self.max_demand:\n        raise ValueError(\n            f\"The demand associated with each node must be lower than the maximum capacity, \"\n            f\"hence the maximum capacity must be &gt;= {self.max_demand}.\"\n        )\n    self.reward_fn = reward_fn or DenseReward()\n    self._viewer = viewer or CVRPViewer(\n        name=\"CVRP\",\n        num_cities=self.num_nodes,\n        render_mode=\"human\",\n    )\n</code></pre>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>a <code>specs.DiscreteArray</code> spec.</p>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>coordinates: BoundedArray (float) of shape (num_nodes + 1, 2).</li> </ul> <code>Spec[Observation]</code> <ul> <li>demands: BoundedArray (float) of shape (num_nodes + 1,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>unvisited_nodes: BoundedArray (bool) of shape (num_nodes + 1,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>position: DiscreteArray (num_values = num_nodes + 1) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>trajectory: BoundedArray (int32) of shape (2 * num_nodes,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>capacity: BoundedArray (float) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_nodes + 1,).</li> </ul>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the CVRP environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the CVRP environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment. This rendering shows the layout of the tour so  far with the cities as circles, and the depot as a square.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>environment state to render.</p> required <p>Returns:</p> Name Type Description <code>rgb_array</code> <code>Optional[ArrayNumpy]</code> <p>the RGB image of the state as an array.</p> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[chex.ArrayNumpy]:\n    \"\"\"Render the given state of the environment. This rendering shows the layout of the tour so\n     far with the cities as circles, and the depot as a square.\n\n    Args:\n        state: environment state to render.\n\n    Returns:\n        rgb_array: the RGB image of the state as an array.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the coordinates.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: used to randomly generate the coordinates.\n\n    Returns:\n         state: `State` object corresponding to the new state of the environment.\n         timestep: `TimeStep` object corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n    state = self.generator(key)\n    timestep = restart(observation=self._state_to_observation(state))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/cvrp/#jumanji.environments.routing.cvrp.env.CVRP.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Numeric</code> <p>jax array (int32) of shape () containing the index of the next node to visit.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>state, timestep: next state of the environment and timestep to be observed.</p> Source code in <code>jumanji/environments/routing/cvrp/env.py</code> <pre><code>def step(self, state: State, action: chex.Numeric) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: jax array (int32) of shape () containing the index of the next node to visit.\n\n    Returns:\n        state, timestep: next state of the environment and timestep to be observed.\n    \"\"\"\n    node_demand = state.demands[action]\n    node_is_visited = state.visited_mask[action]\n    is_valid = ~node_is_visited &amp; (state.capacity &gt;= node_demand)\n\n    next_state = jax.lax.cond(\n        is_valid,\n        self._update_state,\n        lambda *_: state,\n        state,\n        action,\n    )\n\n    reward = self.reward_fn(state, action, next_state, is_valid)\n    observation = self._state_to_observation(next_state)\n\n    # Terminate if all nodes have been visited or the action is invalid.\n    is_done = next_state.visited_mask.all() | ~is_valid\n\n    timestep = jax.lax.cond(\n        is_done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/flat_pack/","title":"FlatPack","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>The FlatPack environment with a configurable number of row and column blocks. Here the goal of an agent is to completely fill an empty grid by placing all available blocks. It can be thought of as a discrete 2D version of the <code>BinPack</code> environment.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>grid: jax array (int) of shape (num_rows, num_cols) with the     current state of the grid.</li> <li>blocks: jax array (int) of shape (num_blocks, 3, 3) with the blocks to     be placed on the grid. Here each block is a 2D array with shape (3, 3).</li> <li>action_mask: jax array (bool) showing where which blocks can be placed on the grid.     this mask includes all possible rotations and possible placement locations     for each block on the grid.</li> </ul> </li> <li> <p>action: jax array (int32) of shape (4,)     multi discrete array containing the move to perform     (block to place, number of rotations, row coordinate, column coordinate).</p> </li> <li> <p>reward: jax array (float) of shape (), could be either:</p> <ul> <li>cell dense: the number of non-zero cells in a placed block normalised by the     total number of cells in a grid. this will be a value in the range [0, 1].     that is to say that the agent will optimise for the maximum area to fill on     the grid.</li> <li>block dense: each placed block will receive a reward of 1./num_blocks. this will     be a value in the range [0, 1]. that is to say that the agent will optimise     for the maximum number of blocks placed on the grid.</li> <li>sparse: 1 if the grid is completely filled, otherwise 0 at each timestep.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>if all blocks have been placed on the board.</li> <li>if the agent has taken <code>num_blocks</code> steps in the environment.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>num_blocks: jax array (int32) of shape () with the     number of blocks in the environment.</li> <li>blocks: jax array (int32) of shape (num_blocks, 3, 3) with the blocks to     be placed on the grid. Here each block is a 2D array with shape (3, 3).</li> <li>action_mask: jax array (bool) showing where which blocks can be placed on the grid.     this mask includes all possible rotations and possible placement locations     for each block on the grid.</li> <li>placed_blocks: jax array (bool) of shape (num_blocks,) showing which blocks     have been placed on the grid.</li> <li>grid: jax array (int32) of shape (num_rows, num_cols) with the     current state of the grid.</li> <li>step_count: jax array (int32) of shape () with the number of steps taken     in the environment.</li> <li>key: jax array of shape (2,) with the random key used for board     generation.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import FlatPack\nenv = FlatPack()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Initializes the FlatPack environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[InstanceGenerator]</code> <p>Instance generator for the environment, default to <code>RandomFlatPackGenerator</code> with a grid of 5 blocks per row and column.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>Reward function for the environment, default to <code>CellDenseReward</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p>Viewer for rendering the environment.</p> <code>None</code> Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[InstanceGenerator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Initializes the FlatPack environment.\n\n    Args:\n        generator: Instance generator for the environment, default to `RandomFlatPackGenerator`\n            with a grid of 5 blocks per row and column.\n        reward_fn: Reward function for the environment, default to `CellDenseReward`.\n        viewer: Viewer for rendering the environment.\n    \"\"\"\n\n    default_generator = RandomFlatPackGenerator(\n        num_row_blocks=5,\n        num_col_blocks=5,\n    )\n\n    self.generator = generator or default_generator\n    self.num_row_blocks = self.generator.num_row_blocks\n    self.num_col_blocks = self.generator.num_col_blocks\n    self.num_blocks = self.num_row_blocks * self.num_col_blocks\n    self.num_rows, self.num_cols = (\n        compute_grid_dim(self.num_row_blocks),\n        compute_grid_dim(self.num_col_blocks),\n    )\n    self.reward_fn = reward_fn or CellDenseReward()\n    self.viewer = viewer or FlatPackViewer(\"FlatPack\", self.num_blocks, render_mode=\"human\")\n    super().__init__()\n</code></pre>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the action expected by the <code>FlatPack</code> environment.</p> <p>Returns:</p> Type Description <code>MultiDiscreteArray</code> <p>MultiDiscreteArray (int32) of shape (num_blocks, num_rotations,</p> <code>MultiDiscreteArray</code> <p>num_rows-2, num_cols-2).</p> <code>MultiDiscreteArray</code> <ul> <li>num_blocks: int between 0 and num_blocks - 1 (inclusive).</li> </ul> <code>MultiDiscreteArray</code> <ul> <li>num_rotations: int between 0 and 3 (inclusive).</li> </ul> <code>MultiDiscreteArray</code> <ul> <li>max_row_position: int between 0 and num_rows - 3 (inclusive).</li> </ul> <code>MultiDiscreteArray</code> <ul> <li>max_col_position: int between 0 and num_cols - 3 (inclusive).</li> </ul>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec of the environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for each filed in the observation:</p> <code>Spec[Observation]</code> <ul> <li>grid: BoundedArray (int) of shape (num_rows, num_cols).</li> </ul> <code>Spec[Observation]</code> <ul> <li>blocks: BoundedArray (int) of shape (num_blocks, 3, 3).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_blocks, 4, num_rows-2, num_cols-2).</li> </ul>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Create an animation from a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation that can export to gif, mp4, or render with HTML.</p> Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Create an animation from a sequence of states.\n\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation that can export to gif, mp4, or render with HTML.\n    \"\"\"\n\n    return self.viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically <code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically `close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n\n    self.viewer.close()\n</code></pre>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.render","title":"<code>render(state)</code>","text":"<p>Render a given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render a given state of the environment.\n\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n\n    return self.viewer.render(state)\n</code></pre>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>PRNG key for generating a new instance.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>a tuple of the initial environment state and a time step.</p> Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def reset(\n    self,\n    key: chex.PRNGKey,\n) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: PRNG key for generating a new instance.\n\n    Returns:\n        a tuple of the initial environment state and a time step.\n    \"\"\"\n\n    grid_state = self.generator(key)\n\n    obs = self._observation_from_state(grid_state)\n    timestep = restart(observation=obs)\n\n    return grid_state, timestep\n</code></pre>"},{"location":"api/environments/flat_pack/#jumanji.environments.packing.flat_pack.env.FlatPack.step","title":"<code>step(state, action)</code>","text":"<p>Steps the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>current state of the environment.</p> required <code>action</code> <code>Array</code> <p>action to take.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>a tuple of the next environment state and a time step.</p> Source code in <code>jumanji/environments/packing/flat_pack/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Steps the environment.\n\n    Args:\n        state: current state of the environment.\n        action: action to take.\n\n    Returns:\n        a tuple of the next environment state and a time step.\n    \"\"\"\n\n    # Unpack and use actions\n    block_idx, rotation, row_idx, col_idx = action\n\n    chosen_block = state.blocks[block_idx]\n\n    # Rotate chosen block\n    chosen_block = rotate_block(chosen_block, rotation)\n\n    grid_block = self._expand_block_to_grid(chosen_block, row_idx, col_idx)\n\n    action_is_legal = state.action_mask[block_idx, rotation, row_idx, col_idx]\n\n    # If the action is legal create a new grid and update the placed blocks array\n    new_grid = jax.lax.cond(\n        action_is_legal,\n        lambda: state.grid + grid_block,\n        lambda: state.grid,\n    )\n    placed_blocks = jax.lax.cond(\n        action_is_legal,\n        lambda: state.placed_blocks.at[block_idx].set(True),\n        lambda: state.placed_blocks,\n    )\n\n    new_action_mask = self._make_action_mask(new_grid, state.blocks, placed_blocks)\n\n    next_state = State(\n        grid=new_grid,\n        blocks=state.blocks,\n        action_mask=new_action_mask,\n        num_blocks=state.num_blocks,\n        key=state.key,\n        step_count=state.step_count + 1,\n        placed_blocks=placed_blocks,\n    )\n\n    done = self._is_done(next_state)\n    next_obs = self._observation_from_state(next_state)\n    reward = self.reward_fn(state, grid_block, next_state, action_is_legal, done)\n\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_obs,\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/game_2048/","title":"Game2048","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Environment for the game 2048. The game consists of a board of size board_size x board_size (4x4 by default) in which the player can take actions to move the tiles on the board up, down, left, or right. The goal of the game is to combine tiles with the same number to create a tile with twice the value, until the player at least creates a tile with the value 2048 to consider it a win.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>board: jax array (int32) of shape (board_size, board_size)     the current state of the board. An empty tile is represented by zero whereas a non-empty     tile is an exponent of 2, e.g. 1, 2, 3, 4, ... (corresponding to 2, 4, 8, 16, ...).</li> <li>action_mask: jax array (bool) of shape (4,)     indicates which actions are valid in the current state of the environment.</li> </ul> </li> <li> <p>action: jax array (int32) of shape (). Is in [0, 1, 2, 3] representing the actions up, right,     down, and left, respectively.</p> </li> <li> <p>reward: jax array (float) of shape (). The reward is 0 except when the player combines tiles     to create a new tile with twice the value. In this case, the reward is the value of the new     tile.</p> </li> <li> <p>episode termination:</p> <ul> <li>if no more valid moves exist (this can happen when the board is full).</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>board: same as observation.</li> <li>step_count: jax array (int32) of shape (),     the number of time steps in the episode so far.</li> <li>action_mask: same as observation.</li> <li>score: jax array (int32) of shape (),     the sum of all tile values on the board.</li> <li>key: jax array (uint32) of shape (2,)     random key used to generate random numbers at each step and for auto-reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Game2048\nenv = Game2048()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Initialize the 2048 game.</p> <p>Parameters:</p> Name Type Description Default <code>board_size</code> <code>int</code> <p>size of the board. Defaults to 4.</p> <code>4</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>Game2048Viewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def __init__(self, board_size: int = 4, viewer: Optional[Viewer[State]] = None) -&gt; None:\n    \"\"\"Initialize the 2048 game.\n\n    Args:\n        board_size: size of the board. Defaults to 4.\n        viewer: `Viewer` used for rendering. Defaults to `Game2048Viewer`.\n    \"\"\"\n    self.board_size = board_size\n    super().__init__()\n\n    # Create viewer used for rendering\n    self._viewer = viewer or Game2048Viewer(\"2048\", board_size)\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>4 actions: [0, 1, 2, 3] -&gt; [Up, Right, Down, Left].</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p><code>DiscreteArray</code> spec object.</p>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>Game2048</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing all the specifications for all the <code>Observation</code> fields: - board: Array (jnp.int32) of shape (board_size, board_size). - action_mask: BoundedArray (bool) of shape (4,).</p>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.__repr__","title":"<code>__repr__()</code>","text":"<p>String representation of the environment.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the string representation of the environment.</p> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the environment.\n\n    Returns:\n        str: the string representation of the environment.\n    \"\"\"\n    return f\"2048 Game(board_size={self.board_size})\"\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the 2048 game board based on the sequence of game states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>is a list of <code>State</code> objects representing the sequence of game states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the 2048 game board based on the sequence of game states.\n\n    Args:\n        states: is a list of `State` objects representing the sequence of game states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n        will not be stored.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the game board.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>is the current game state to be rendered.</p> required Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the game board.\n\n    Args:\n        state: is the current game state to be rendered.\n    \"\"\"\n    return self._viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random number generator key.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: random number generator key.\n\n    Returns:\n        state: the new state of the environment.\n        timestep: the first timestep returned by the environment.\n    \"\"\"\n\n    key, board_key = jax.random.split(key)\n    board = self._generate_board(board_key)\n    action_mask = self._get_action_mask(board)\n\n    obs = Observation(board=board, action_mask=action_mask)\n\n    state = State(\n        board=board,\n        step_count=jnp.array(0, jnp.int32),\n        action_mask=action_mask,\n        key=key,\n        score=jnp.array(0, float),\n    )\n\n    highest_tile = 2 ** jnp.max(board)\n    timestep = restart(observation=obs, extras={\"highest_tile\": highest_tile})\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/game_2048/#jumanji.environments.logic.game_2048.env.Game2048.step","title":"<code>step(state, action)</code>","text":"<p>Updates the environment state after the agent takes an action.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the current state of the environment.</p> required <code>action</code> <code>Array</code> <p>the action taken by the agent.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the next timestep.</p> Source code in <code>jumanji/environments/logic/game_2048/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Updates the environment state after the agent takes an action.\n\n    Args:\n        state: the current state of the environment.\n        action: the action taken by the agent.\n\n    Returns:\n        state: the new state of the environment.\n        timestep: the next timestep.\n    \"\"\"\n    # Take the action in the environment: Up, Right, Down, Left.\n    updated_board, reward = move(state.board, action)\n\n    # Generate new key.\n    random_cell_key, new_state_key = jax.random.split(state.key)\n\n    # Update the state of the board by adding a new random cell.\n    updated_board = jax.lax.cond(\n        state.action_mask[action],\n        self._add_random_cell,\n        lambda board, key: board,\n        updated_board,\n        random_cell_key,\n    )\n\n    # Generate action mask to keep in the state for the next step and\n    # to provide to the agent in the observation.\n    action_mask = self._get_action_mask(board=updated_board)\n\n    # Build the state.\n    state = State(\n        board=updated_board,\n        action_mask=action_mask,\n        step_count=state.step_count + 1,\n        key=new_state_key,\n        score=state.score + reward,\n    )\n\n    # Generate the observation from the environment state.\n    observation = Observation(\n        board=updated_board,\n        action_mask=action_mask,\n    )\n\n    # Check if the episode terminates (i.e. there are no legal actions).\n    done = ~jnp.any(action_mask)\n\n    # Return either a MID or a LAST timestep depending on done.\n    highest_tile = 2 ** jnp.max(updated_board)\n    extras = {\"highest_tile\": highest_tile}\n    timestep = jax.lax.cond(\n        done,\n        lambda: termination(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n        lambda: transition(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n    )\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/graph_coloring/","title":"GraphColoring","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Environment for the GraphColoring problem. The problem is a combinatorial optimization task where the goal is   to assign a color to each vertex of a graph   in such a way that no two adjacent vertices share the same color. The problem is usually formulated as minimizing the number of colors used.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>adj_matrix: jax array (bool) of shape (num_nodes, num_nodes),     representing the adjacency matrix of the graph.</li> <li>colors: jax array (int32) of shape (num_nodes,),     representing the current color assignments for the vertices.</li> <li>action_mask: jax array (bool) of shape (num_colors,),     indicating which actions are valid in the current state of the environment.</li> <li>current_node_index: integer representing the current node being colored.</li> </ul> </li> <li> <p>action: int, the color to be assigned to the current node (0 to num_nodes - 1)</p> </li> <li> <p>reward: float, a sparse reward is provided at the end of the episode.     Equals the negative of the number of unique colors used to color all vertices in the graph.     If an invalid action is taken, the reward is the negative of the total number of colors.</p> </li> <li> <p>episode termination:</p> <ul> <li>if all nodes have been assigned a color or if an invalid action is taken.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>adj_matrix: jax array (bool) of shape (num_nodes, num_nodes),     representing the adjacency matrix of the graph.</li> <li>colors: jax array (int32) of shape (num_nodes,),     color assigned to each node, -1 if not assigned.</li> <li>current_node_index: jax array (int) with shape (),     index of the current node.</li> <li>action_mask: jax array (bool) of shape (num_colors,),     indicating which actions are valid in the current state of the environment.</li> <li>key: jax array (uint32) of shape (2,),     random key used to generate random numbers at each step and for auto-reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import GraphColoring\nenv = GraphColoring()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiate a <code>GraphColoring</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p>callable to instantiate environment instances. Defaults to <code>RandomGenerator</code> which generates graphs with 20 <code>num_nodes</code> and <code>edge_probability</code> equal to 0.8.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p>environment viewer for rendering. Defaults to <code>GraphColoringViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiate a `GraphColoring` environment.\n\n    Args:\n        generator: callable to instantiate environment instances.\n            Defaults to `RandomGenerator` which generates graphs with\n            20 `num_nodes` and `edge_probability` equal to 0.8.\n        viewer: environment viewer for rendering.\n            Defaults to `GraphColoringViewer`.\n    \"\"\"\n    self.generator = generator or RandomGenerator(num_nodes=20, edge_probability=0.8)\n    self.num_nodes = self.generator.num_nodes\n    super().__init__()\n\n    # Create viewer used for rendering\n    self._env_viewer = viewer or GraphColoringViewer(name=\"GraphColoring\")\n</code></pre>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specification of the action for the <code>GraphColoring</code> environment.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>specs.DiscreteArray object</p>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>adj_matrix: BoundedArray (bool) of shape (num_nodes, num_nodes). Represents the adjacency matrix of the graph.</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_nodes,). Represents the valid actions in the current state.</li> </ul> <code>Spec[Observation]</code> <ul> <li>colors: BoundedArray (int32) of shape (num_nodes,). Represents the colors assigned to each node.</li> </ul> <code>Spec[Observation]</code> <ul> <li>current_node_index: BoundedArray (int32) of shape (). Represents the index of the current node.</li> </ul>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>GraphColoring</code> environment based on a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>is a list of <code>State</code> objects representing the sequence of game states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be stored.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `GraphColoring` environment based on a sequence of states.\n\n    Args:\n        states: is a list of `State` objects representing the sequence of game states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be stored.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._env_viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._env_viewer.close()\n</code></pre>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the <code>GraphColoring</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>is the current game state to be rendered.</p> required Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the `GraphColoring` environment.\n\n    Args:\n        state: is the current game state to be rendered.\n    \"\"\"\n    return self._env_viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment to an initial state.</p> <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>The initial state and timestep.</p> Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\n\n    Returns:\n        The initial state and timestep.\n    \"\"\"\n    colors = jnp.full(self.num_nodes, -1, dtype=jnp.int32)\n    key, subkey = jax.random.split(key)\n    adj_matrix = self.generator(subkey)\n\n    action_mask = jnp.ones(self.num_nodes, dtype=bool)\n    current_node_index = jnp.array(0, jnp.int32)\n    state = State(\n        adj_matrix=adj_matrix,\n        colors=colors,\n        current_node_index=current_node_index,\n        action_mask=action_mask,\n        key=key,\n    )\n    obs = Observation(\n        adj_matrix=adj_matrix,\n        colors=colors,\n        action_mask=action_mask,\n        current_node_index=current_node_index,\n    )\n    timestep = restart(observation=obs)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/graph_coloring/#jumanji.environments.logic.graph_coloring.env.GraphColoring.step","title":"<code>step(state, action)</code>","text":"<p>Updates the environment state after the agent takes an action.</p> <p>Specifically, this function allows the agent to choose a color for the current node (based on the action taken) in a graph coloring problem. It then updates the state of the environment based on the color chosen and calculates the reward based on the validity of the action and the completion of the coloring task.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the current state of the environment.</p> required <code>action</code> <code>Array</code> <p>the action taken by the agent.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the next timestep.</p> Source code in <code>jumanji/environments/logic/graph_coloring/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Updates the environment state after the agent takes an action.\n\n    Specifically, this function allows the agent to choose\n    a color for the current node (based on the action taken)\n    in a graph coloring problem.\n    It then updates the state of the environment based on\n    the color chosen and calculates the reward based on\n    the validity of the action and the completion of the coloring task.\n\n    Args:\n        state: the current state of the environment.\n        action: the action taken by the agent.\n\n    Returns:\n        state: the new state of the environment.\n        timestep: the next timestep.\n    \"\"\"\n    # Get the valid actions for the current state.\n    valid_actions = state.action_mask\n\n    # Check if the chosen action is invalid (not in valid_actions).\n    invalid_action_taken = jnp.logical_not(valid_actions[action])\n\n    # Update the colors array with the chosen action.\n    colors = state.colors.at[state.current_node_index].set(action)\n\n    # Determine if all nodes have been assigned a color\n    all_nodes_colored = jnp.all(colors &gt;= 0)\n\n    # Calculate the reward\n    unique_colors_used = jnp.unique(colors, size=self.num_nodes, fill_value=-1)\n    num_unique_colors = jnp.count_nonzero(unique_colors_used &gt;= 0)\n    reward = jnp.where(all_nodes_colored, -num_unique_colors, 0.0)\n\n    # Apply the maximum penalty when an invalid action is taken and terminate the episode\n    reward = jnp.where(invalid_action_taken, -self.num_nodes, reward)\n    done = jnp.logical_or(all_nodes_colored, invalid_action_taken)\n\n    # Update the current node index\n    next_node_index = (state.current_node_index + 1) % self.num_nodes\n\n    next_action_mask = self._get_valid_actions(next_node_index, state.adj_matrix, state.colors)\n\n    next_state = State(\n        adj_matrix=state.adj_matrix,\n        colors=colors,\n        current_node_index=next_node_index,\n        action_mask=next_action_mask,\n        key=state.key,\n    )\n    obs = Observation(\n        adj_matrix=state.adj_matrix,\n        colors=colors,\n        action_mask=next_state.action_mask,\n        current_node_index=next_node_index,\n    )\n    timestep = lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        obs,\n    )\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/job_shop/","title":"JobShop","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>The Job Shop Scheduling Problem, as described in [1], is one of the best known combinatorial optimization problems. We are given <code>num_jobs</code> jobs, each consisting of at most <code>max_num_ops</code> ops, which need to be processed on <code>num_machines</code> machines. Each operation (op) has a specific machine that it needs to be processed on and a duration (which must be less than or equal to <code>max_duration_op</code>). The goal is to minimise the total length of the schedule, also known as the makespan.</p> <p>[1] https://developers.google.com/optimization/scheduling/job_shop.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>ops_machine_ids: jax array (int32) of (num_jobs, max_num_ops)     id of the machine each operation must be processed on.</li> <li>ops_durations: jax array (int32) of (num_jobs, max_num_ops)     processing time of each operation.</li> <li>ops_mask: jax array (bool) of (num_jobs, max_num_ops)     indicating which operations have yet to be scheduled.</li> <li>machines_job_ids: jax array (int32) of shape (num_machines,)     id of the job (or no-op) that each machine is processing.</li> <li>machines_remaining_times: jax array (int32) of shape (num_machines,)     specifying, for each machine, the number of time steps until available.</li> <li>action_mask: jax array (bool) of shape (num_machines, num_jobs + 1)     indicates which job(s) (or no-op) can legally be scheduled on each machine.</li> </ul> </li> <li> <p>action: jax array (int32) of shape (num_machines,).</p> </li> <li> <p>reward: jax array (float) of shape (). A reward of <code>-1</code> is given each time step.     If all machines are simultaneously idle or the agent selects an invalid action,     the agent is given a large penalty of <code>-num_jobs * max_num_ops * max_op_duration</code>     which is an upper bound on the makespan.</p> </li> <li> <p>episode termination:</p> <ul> <li>Finished schedule: all operations (and thus all jobs) every job have been processed.</li> <li>Illegal action: the agent ignores the action mask and takes an illegal action.</li> <li>Simultaneously idle: all machines are inactive at the same time.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>ops_machine_ids: same as observation.</li> <li>ops_durations: same as observation.</li> <li>ops_mask: same as observation.</li> <li>machines_job_ids: same as observation.</li> <li>machines_remaining_times: same as observation.</li> <li>action_mask: same as observation.</li> <li>step_count: jax array (int32) of shape (),     the number of time steps in the episode so far.</li> <li>scheduled_times: jax array (int32) of shape (num_jobs, max_num_ops),     specifying the timestep at which every op (scheduled so far) was scheduled.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import JobShop\nenv = JobShop()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiate a <code>JobShop</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are ['ToyGenerator', 'RandomGenerator']. Defaults to <code>RandomGenerator</code> with 20 jobs, 10 machines, up to 8 ops for any given job, and a max operation duration of 6.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>JobShopViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiate a `JobShop` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are ['ToyGenerator', 'RandomGenerator'].\n            Defaults to `RandomGenerator` with 20 jobs, 10 machines, up to 8 ops\n            for any given job, and a max operation duration of 6.\n        viewer: `Viewer` used for rendering. Defaults to `JobShopViewer`.\n    \"\"\"\n    self.generator = generator or RandomGenerator(\n        num_jobs=20,\n        num_machines=10,\n        max_num_ops=8,\n        max_op_duration=6,\n    )\n    self.num_jobs = self.generator.num_jobs\n    self.num_machines = self.generator.num_machines\n    self.max_num_ops = self.generator.max_num_ops\n    self.max_op_duration = self.generator.max_op_duration\n    super().__init__()\n\n    # Define the \"job id\" of a no-op action as the number of jobs\n    self.no_op_idx = self.num_jobs\n\n    # Create viewer used for rendering\n    self._viewer = viewer or JobShopViewer(\n        \"JobShop\",\n        self.num_jobs,\n        self.num_machines,\n        self.max_num_ops,\n        self.max_op_duration,\n    )\n</code></pre>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the action in the <code>JobShop</code> environment. The action gives each machine a job id ranging from 0, 1, ..., num_jobs where the last value corresponds to a no-op.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p>a <code>specs.MultiDiscreteArray</code> spec.</p>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>JobShop</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing the specifications for all the <code>Observation</code> fields:</p> <code>Spec[Observation]</code> <ul> <li>ops_machine_ids: BoundedArray (int32) of shape (num_jobs, max_num_ops).</li> </ul> <code>Spec[Observation]</code> <ul> <li>ops_durations: BoundedArray (int32) of shape (num_jobs, max_num_ops).</li> </ul> <code>Spec[Observation]</code> <ul> <li>ops_mask: BoundedArray (bool) of shape (num_jobs, max_num_ops).</li> </ul> <code>Spec[Observation]</code> <ul> <li>machines_job_ids: BoundedArray (int32) of shape (num_machines,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>machines_remaining_times: BoundedArray (int32) of shape (num_machines,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_machines, num_jobs + 1).</li> </ul>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the Jobshop environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the Jobshop environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment. This rendering shows which job (or no-op) is running on each machine for the current time step and previous time steps.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment. This rendering shows which job (or no-op)\n    is running on each machine for the current time step and previous time steps.\n\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment by creating a new problem instance and initialising the state and timestep.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the environment state after the reset.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the first timestep returned by the environment after the reset.</p> Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment by creating a new problem instance and initialising the state\n    and timestep.\n\n    Args:\n        key: random key used to reset the environment.\n\n    Returns:\n        state: the environment state after the reset.\n        timestep: the first timestep returned by the environment after the reset.\n    \"\"\"\n    # Generate a new problem instance\n    state = self.generator(key)\n\n    # Create the action mask and update the state\n    state.action_mask = self._create_action_mask(\n        state.machines_job_ids,\n        state.machines_remaining_times,\n        state.ops_machine_ids,\n        state.ops_mask,\n    )\n\n    # Get the observation and the timestep\n    obs = self._observation_from_state(state)\n    timestep = restart(observation=obs)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/job_shop/#jumanji.environments.packing.job_shop.env.JobShop.step","title":"<code>step(state, action)</code>","text":"<p>Updates the status of all machines, the status of the operations, and increments the time step. It updates the environment state and the timestep (which contains the new observation). It calculates the reward based on the three terminal conditions:     - The action provided by the agent is invalid.     - The schedule has finished.     - All machines do a no-op that leads to all machines being simultaneously idle.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the environment state.</p> required <code>action</code> <code>Array</code> <p>the action to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the updated environment state.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the updated timestep.</p> Source code in <code>jumanji/environments/packing/job_shop/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Updates the status of all machines, the status of the operations, and increments the\n    time step. It updates the environment state and the timestep (which contains the new\n    observation). It calculates the reward based on the three terminal conditions:\n        - The action provided by the agent is invalid.\n        - The schedule has finished.\n        - All machines do a no-op that leads to all machines being simultaneously idle.\n\n    Args:\n        state: the environment state.\n        action: the action to take.\n\n    Returns:\n        state: the updated environment state.\n        timestep: the updated timestep.\n    \"\"\"\n    # Check the action is legal\n    invalid = ~jnp.all(state.action_mask[jnp.arange(self.num_machines), action])  # type: ignore\n\n    # Obtain the id for every job's next operation\n    op_ids = jnp.argmax(state.ops_mask, axis=-1)\n\n    # Update the status of all machines\n    (\n        updated_machines_job_ids,\n        updated_machines_remaining_times,\n    ) = self._update_machines(\n        action,\n        op_ids,\n        state.machines_job_ids,\n        state.machines_remaining_times,\n        state.ops_durations,\n    )\n\n    # Update the status of operations that have been scheduled\n    updated_ops_mask, updated_scheduled_times = self._update_operations(\n        action,\n        op_ids,\n        state.step_count,\n        state.scheduled_times,\n        state.ops_mask,\n    )\n\n    # Update the action_mask\n    updated_action_mask = self._create_action_mask(\n        updated_machines_job_ids,\n        updated_machines_remaining_times,\n        state.ops_machine_ids,\n        updated_ops_mask,\n    )\n\n    # Increment the time step\n    updated_step_count = jnp.array(state.step_count + 1, jnp.int32)\n\n    # Check if all machines are idle simultaneously\n    all_machines_idle = jnp.all(\n        (updated_machines_job_ids == self.no_op_idx) &amp; (updated_machines_remaining_times == 0)\n    )\n\n    # Check if the schedule has finished\n    all_operations_scheduled = ~jnp.any(updated_ops_mask)\n    schedule_finished = all_operations_scheduled &amp; jnp.all(\n        updated_machines_remaining_times == 0\n    )\n\n    # Update the state and extract the next observation\n    next_state = State(\n        ops_machine_ids=state.ops_machine_ids,\n        ops_durations=state.ops_durations,\n        ops_mask=updated_ops_mask,\n        machines_job_ids=updated_machines_job_ids,\n        machines_remaining_times=updated_machines_remaining_times,\n        action_mask=updated_action_mask,\n        step_count=updated_step_count,\n        scheduled_times=updated_scheduled_times,\n        key=state.key,\n    )\n    next_obs = self._observation_from_state(next_state)\n\n    # Compute terminal condition\n    done = invalid | all_machines_idle | schedule_finished\n\n    # Compute reward\n    reward = jnp.where(\n        invalid | all_machines_idle,\n        jnp.array(-self.num_jobs * self.max_num_ops * self.max_op_duration, float),\n        jnp.array(-1, float),\n    )\n\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_obs,\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/knapsack/","title":"Knapsack","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Knapsack environment as described in [1].</p> <ul> <li> <p>observation: Observation</p> <ul> <li>weights: jax array (float) of shape (num_items,)     the weights of the items.</li> <li>values: jax array (float) of shape (num_items,)     the values of the items.</li> <li>packed_items: jax array (bool) of shape (num_items,)     binary mask denoting which items are already packed into the knapsack.</li> <li>action_mask: jax array (bool) of shape (num_items,)     binary mask denoting which items can be packed into the knapsack.</li> </ul> </li> <li> <p>action: jax array (int32) of shape ()     [0, ..., num_items - 1] -&gt; item to pack.</p> </li> <li> <p>reward: jax array (float) of shape (), could be either:</p> <ul> <li>dense: the value of the item to pack at the current timestep.</li> <li>sparse: the sum of the values of the items packed in the bag at the end of the episode. In both cases, the reward is 0 if the action is invalid, i.e. an item that was previously selected is selected again or has a weight larger than the bag capacity.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>if no action can be performed, i.e. all items are packed or each remaining item's weight     is larger than the bag capacity.</li> <li>if an invalid action is taken, i.e. the chosen item is already packed or has a weight     larger than the bag capacity.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>weights: jax array (float) of shape (num_items,)     the weights of the items.</li> <li>values: jax array (float) of shape (num_items,)     the values of the items.</li> <li>packed_items: jax array (bool) of shape (num_items,)     binary mask denoting which items are already packed into the knapsack.</li> <li>remaining_budget: jax array (float)     the budget currently remaining.</li> </ul> </li> </ul> <p>[1] https://arxiv.org/abs/2010.16011</p> <pre><code>from jumanji.environments import Knapsack\nenv = Knapsack()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>Knapsack</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. The default option is 'RandomGenerator' which samples Knapsack instances with 50 items and a total budget of 12.5.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p><code>RewardFn</code> whose <code>__call__</code> method computes the reward of an environment transition. The function must compute the reward based on the current state, the chosen action, the next state and whether the action is valid. Implemented options are [<code>DenseReward</code>, <code>SparseReward</code>]. Defaults to <code>DenseReward</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>KnapsackViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates a `Knapsack` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            The default option is 'RandomGenerator' which samples Knapsack instances\n            with 50 items and a total budget of 12.5.\n        reward_fn: `RewardFn` whose `__call__` method computes the reward of an environment\n            transition. The function must compute the reward based on the current state,\n            the chosen action, the next state and whether the action is valid.\n            Implemented options are [`DenseReward`, `SparseReward`]. Defaults to `DenseReward`.\n        viewer: `Viewer` used for rendering. Defaults to `KnapsackViewer` with \"human\" render\n            mode.\n    \"\"\"\n\n    self.generator = generator or RandomGenerator(\n        num_items=50,\n        total_budget=12.5,\n    )\n    self.num_items = self.generator.num_items\n    super().__init__()\n    self.total_budget = self.generator.total_budget\n    self.reward_fn = reward_fn or DenseReward()\n    self._viewer = viewer or KnapsackViewer(\n        name=\"Knapsack\",\n        render_mode=\"human\",\n        total_budget=self.total_budget,\n    )\n</code></pre>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>a <code>specs.DiscreteArray</code> spec.</p>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for each field in the Observation:</p> <code>Spec[Observation]</code> <ul> <li>weights: BoundedArray (float) of shape (num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>values: BoundedArray (float) of shape (num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>packed_items: BoundedArray (bool) of shape (num_items,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_items,).</li> </ul>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>Knapsack</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `Knapsack` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.render","title":"<code>render(state)</code>","text":"<p>Render the environment state, displaying which items have been picked so far, their value, and the remaining budget.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the environment state to be rendered.</p> required Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the environment state, displaying which items have been picked so far,\n    their value, and the remaining budget.\n\n    Args:\n        state: the environment state to be rendered.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the weights and values of the items.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: used to randomly generate the weights and values of the items.\n\n    Returns:\n        state: the new state of the environment.\n        timestep: the first timestep returned by the environment.\n    \"\"\"\n    state = self.generator(key)\n    timestep = restart(observation=self._state_to_observation(state))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/knapsack/#jumanji.environments.packing.knapsack.env.Knapsack.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Numeric</code> <p>index of next item to take.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the timestep to be observed.</p> Source code in <code>jumanji/environments/packing/knapsack/env.py</code> <pre><code>def step(self, state: State, action: chex.Numeric) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: index of next item to take.\n\n    Returns:\n        state: next state of the environment.\n        timestep: the timestep to be observed.\n    \"\"\"\n    item_fits = state.remaining_budget &gt;= state.weights[action]\n    item_not_packed = ~state.packed_items[action]\n    is_valid = item_fits &amp; item_not_packed\n    next_state = jax.lax.cond(\n        is_valid,\n        self._update_state,\n        lambda *_: state,\n        state,\n        action,\n    )\n\n    observation = self._state_to_observation(next_state)\n\n    no_items_available = ~jnp.any(observation.action_mask)\n    is_done = no_items_available | ~is_valid\n\n    reward = self.reward_fn(state, action, next_state, is_valid, is_done)\n\n    timestep = jax.lax.cond(\n        is_done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/lbf/","title":"Level-Based Foraging","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>An implementation of the Level-Based Foraging environment where agents need to cooperate to collect food and split the reward.</p> <p>Original implementation: https://github.com/semitable/lb-foraging</p> <ul> <li> <p><code>observation</code>: <code>Observation</code></p> <ul> <li><code>agent_views</code>: Depending on the <code>observer</code> passed to <code>__init__</code>, it can be a   <code>GridObserver</code> or a <code>VectorObserver</code>.<ul> <li><code>GridObserver</code>: Returns an agent's view with a shape of   (num_agents, 3, 2 * fov + 1, 2 * fov +1).</li> <li><code>VectorObserver</code>: Returns an agent's view with a shape of   (num_agents, 3 * (num_food + num_agents).</li> </ul> </li> <li><code>action_mask</code>: JAX array (bool) of shape (num_agents, 6)   indicating for each agent which size actions   (no-op, up, down, left, right, load) are allowed.</li> <li><code>step_count</code>: int32, the number of steps since the beginning of the episode.</li> </ul> </li> <li> <p><code>action</code>: JAX array (int32) of shape (num_agents,). The valid actions for each     agent are (0: noop, 1: up, 2: down, 3: left, 4: right, 5: load).</p> </li> <li> <p><code>reward</code>: JAX array (float) of shape (num_agents,)     When one or more agents load food, the food level is rewarded to the agents, weighted     by the level of each agent. The reward is then normalized so that, at the end,     the sum of the rewards (if all food items have been picked up) is one.</p> </li> <li> <p>Episode Termination:</p> <ul> <li>All food items have been eaten.</li> <li>The number of steps is greater than the limit.</li> </ul> </li> <li> <p><code>state</code>: <code>State</code></p> <ul> <li><code>agents</code>: Stacked Pytree of <code>Agent</code> objects of length <code>num_agents</code>.<ul> <li><code>Agent</code>:<ul> <li><code>id</code>: JAX array (int32) of shape ().</li> <li><code>position</code>: JAX array (int32) of shape (2,).</li> <li><code>level</code>: JAX array (int32) of shape ().</li> <li><code>loading</code>: JAX array (bool) of shape ().</li> </ul> </li> </ul> </li> <li><code>food_items</code>: Stacked Pytree of <code>Food</code> objects of length <code>num_food</code>.<ul> <li><code>Food</code>:<ul> <li><code>id</code>: JAX array (int32) of shape ().</li> <li><code>position</code>: JAX array (int32) of shape (2,).</li> <li><code>level</code>: JAX array (int32) of shape ().</li> <li><code>eaten</code>: JAX array (bool) of shape ().</li> </ul> </li> </ul> </li> <li><code>step_count</code>: JAX array (int32) of shape (), the number of steps since the beginning   of the episode.</li> <li><code>key</code>: JAX array (uint) of shape (2,)     JAX random generation key. Ignored since the environment is deterministic.</li> </ul> </li> </ul> <p>Example: <pre><code>from jumanji.environments import LevelBasedForaging\nenv = LevelBasedForaging()\nkey = jax.random.key(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre></p> <p>Initialization Args: - <code>generator</code>: A <code>Generator</code> object that generates the initial state of the environment.     Defaults to a <code>RandomGenerator</code> with the following parameters:         - <code>grid_size</code>: 8         - <code>fov</code>: 8 (full observation of the grid)         - <code>num_agents</code>: 2         - <code>num_food</code>: 2         - <code>max_agent_level</code>: 2         - <code>force_coop</code>: True - <code>time_limit</code>: The maximum number of steps in an episode. Defaults to 200. - <code>grid_observation</code>: If <code>True</code>, the observer generates a grid observation (default is <code>False</code>). - <code>normalize_reward</code>: If <code>True</code>, normalizes the reward (default is <code>True</code>). - <code>penalty</code>: The penalty value (default is 0.0). - <code>viewer</code>: Viewer to render the environment. Defaults to <code>LevelBasedForagingViewer</code>.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[RandomGenerator] = None,\n    viewer: Optional[Viewer[State]] = None,\n    time_limit: int = 100,\n    grid_observation: bool = False,\n    normalize_reward: bool = True,\n    penalty: float = 0.0,\n) -&gt; None:\n    self._generator = generator or RandomGenerator(\n        grid_size=8,\n        fov=8,\n        num_agents=2,\n        num_food=2,\n        force_coop=True,\n    )\n    self.time_limit = time_limit\n    self.grid_size: int = self._generator.grid_size\n    self.num_agents: int = self._generator.num_agents\n    self.num_food: int = self._generator.num_food\n    self.fov = self._generator.fov\n    self.normalize_reward = normalize_reward\n    self.penalty = penalty\n\n    self._observer: Union[VectorObserver, GridObserver]\n    if not grid_observation:\n        self._observer = VectorObserver(\n            fov=self.fov,\n            grid_size=self.grid_size,\n            num_agents=self.num_agents,\n            num_food=self.num_food,\n        )\n    else:\n        self._observer = GridObserver(\n            fov=self.fov,\n            grid_size=self.grid_size,\n            num_agents=self.num_agents,\n            num_food=self.num_food,\n        )\n\n    super().__init__()\n\n    # create viewer for rendering environment\n    self._viewer = viewer or LevelBasedForagingViewer(self.grid_size, \"LevelBasedForaging\")\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec for the Level Based Foraging environment.</p> <p>Returns:</p> Type Description <code>MultiDiscreteArray</code> <p>specs.MultiDiscreteArray: Action spec for the environment with shape (num_agents,).</p>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.discount_spec","title":"<code>discount_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Describes the discount returned by the environment.</p> <p>Returns:</p> Name Type Description <code>discount_spec</code> <code>BoundedArray</code> <p>a <code>specs.BoundedArray</code> spec.</p>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the environment.</p> <p>The spec's shape depends on the <code>observer</code> passed to <code>__init__</code>.</p> <p>The GridObserver returns an agent's view with a shape of     (num_agents, 3, 2 * fov + 1, 2 * fov +1). The VectorObserver returns an agent's view with a shape of (num_agents, 3 * num_food + 3 * num_agents). See a more detailed description of the observations in the docs of <code>GridObserver</code> and <code>VectorObserver</code>.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>specs.Spec[Observation]: Spec for the <code>Observation</code> with fields grid,</p> <code>Spec[Observation]</code> <p>action_mask, and step_count.</p>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.reward_spec","title":"<code>reward_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the reward specification for the <code>LevelBasedForaging</code> environment.</p> <p>Since this is a multi-agent environment each agent gets its own reward.</p> <p>Returns:</p> Type Description <code>Array</code> <p>specs.Array: Reward specification, of shape (num_agents,) for the  environment.</p>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animation from a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>Sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>Delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>The path where the animation file should be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>matplotlib.animation.FuncAnimation: Animation object that can be saved as a GIF, MP4,</p> <code>FuncAnimation</code> <p>or rendered with HTML.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animation from a sequence of states.\n\n    Args:\n        states (Sequence[State]): Sequence of `State` corresponding to subsequent timesteps.\n        interval (int): Delay between frames in milliseconds, default to 200.\n        save_path (Optional[str]): The path where the animation file should be saved.\n\n    Returns:\n        matplotlib.animation.FuncAnimation: Animation object that can be saved as a GIF, MP4,\n        or rendered with HTML.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.get_reward","title":"<code>get_reward(food_items, adj_loading_agents_levels, eaten_this_step)</code>","text":"<p>Returns a reward for all agents given all food items.</p> <p>Parameters:</p> Name Type Description Default <code>food_items</code> <code>Food</code> <p>All the food items in the environment.</p> required <code>adj_loading_agents_levels</code> <code>Array</code> <p>The level of all agents adjacent to all foods.</p> required <code>eaten_this_step</code> <code>Array</code> <p>Whether the food was eaten or not (this step).</p> required Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def get_reward(\n    self,\n    food_items: Food,\n    adj_loading_agents_levels: chex.Array,\n    eaten_this_step: chex.Array,\n) -&gt; chex.Array:\n    \"\"\"Returns a reward for all agents given all food items.\n\n    Args:\n        food_items (Food): All the food items in the environment.\n        adj_loading_agents_levels (chex.Array): The level of all agents adjacent to all foods.\n        eaten_this_step (chex.Array): Whether the food was eaten or not (this step).\n    \"\"\"\n\n    def get_reward_per_food(\n        food: Food,\n        adj_loading_agents_levels: chex.Array,\n        eaten_this_step: chex.Array,\n    ) -&gt; chex.Array:\n        \"\"\"Returns the reward for all agents given a single food.\"\"\"\n\n        # If the food has already been eaten or is not loaded, the sum will be equal to 0\n        sum_agents_levels = jnp.sum(adj_loading_agents_levels)\n\n        # Penalize agents for not being able to cooperate and eat food\n        penalty = jnp.where(\n            (sum_agents_levels != 0) &amp; (sum_agents_levels &lt; food.level),\n            self.penalty,\n            0,\n        )\n\n        # Zero out all agents if food was not eaten and add penalty\n        reward = (adj_loading_agents_levels * eaten_this_step * food.level) - penalty\n\n        # jnp.nan_to_num: Used in the case where no agents are adjacent to the food\n        normalizer = sum_agents_levels * total_food_level\n        reward = jnp.where(self.normalize_reward, jnp.nan_to_num(reward / normalizer), reward)\n\n        return reward\n\n    # Get reward per food for all food items,\n    # then sum it on the agent dimension to get reward per agent.\n    total_food_level = jnp.sum(food_items.level)\n    reward_per_food = jax.vmap(get_reward_per_food, in_axes=(0, 0, 0))(\n        food_items, adj_loading_agents_levels, eaten_this_step\n    )\n    return jnp.sum(reward_per_food, axis=0)\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the <code>LevelBasedForaging</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>The current environment state to be rendered.</p> required <p>Returns:</p> Type Description <code>Optional[NDArray]</code> <p>Optional[NDArray]: Rendered environment state.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the `LevelBasedForaging` environment.\n\n    Args:\n        state (State): The current environment state to be rendered.\n\n    Returns:\n        Optional[NDArray]: Rendered environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Used to randomly generate the new <code>State</code>.</p> required <p>Returns:</p> Type Description <code>State</code> <p>Tuple[State, TimeStep]: <code>State</code> object corresponding to the new initial state</p> <code>TimeStep</code> <p>of the environment and <code>TimeStep</code> object corresponding to the initial timestep.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key (chex.PRNGKey): Used to randomly generate the new `State`.\n\n    Returns:\n        Tuple[State, TimeStep]: `State` object corresponding to the new initial state\n        of the environment and `TimeStep` object corresponding to the initial timestep.\n    \"\"\"\n    state = self._generator(key)\n    observation = self._observer.state_to_observation(state)\n    timestep = restart(observation, shape=self.num_agents)\n    timestep.extras = self._get_extra_info(state, timestep)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/lbf/#jumanji.environments.routing.lbf.env.LevelBasedForaging.step","title":"<code>step(state, actions)</code>","text":"<p>Simulate one step of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State  containing the dynamics of the environment.</p> required <code>actions</code> <code>Array</code> <p>Array containing the actions to take for each agent.</p> required <p>Returns:</p> Type Description <code>State</code> <p>Tuple[State, TimeStep]: <code>State</code> object corresponding to the next state and</p> <code>TimeStep</code> <p><code>TimeStep</code> object corresponding the timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/lbf/env.py</code> <pre><code>def step(self, state: State, actions: chex.Array) -&gt; Tuple[State, TimeStep]:\n    \"\"\"Simulate one step of the environment.\n\n    Args:\n        state (State): State  containing the dynamics of the environment.\n        actions (chex.Array): Array containing the actions to take for each agent.\n\n    Returns:\n        Tuple[State, TimeStep]: `State` object corresponding to the next state and\n        `TimeStep` object corresponding the timestep returned by the environment.\n    \"\"\"\n    # Move agents, fix collisions that may happen and set loading status.\n    moved_agents = utils.update_agent_positions(\n        state.agents, actions, state.food_items, self.grid_size\n    )\n\n    # Eat the food\n    food_items, eaten_this_step, adj_loading_agents_levels = jax.vmap(\n        utils.eat_food, (None, 0)\n    )(moved_agents, state.food_items)\n\n    reward = self.get_reward(food_items, adj_loading_agents_levels, eaten_this_step)\n\n    state = State(\n        agents=moved_agents,\n        food_items=food_items,\n        step_count=state.step_count + 1,\n        key=state.key,\n    )\n    observation = self._observer.state_to_observation(state)\n\n    # First condition is truncation, second is termination.\n    terminate = jnp.all(state.food_items.eaten)\n    truncate = state.step_count &gt;= self.time_limit\n\n    timestep = jax.lax.switch(\n        terminate + 2 * truncate,\n        [\n            # !terminate !trunc\n            lambda rew, obs: transition(reward=rew, observation=obs, shape=self.num_agents),\n            # terminate !truncate\n            lambda rew, obs: termination(reward=rew, observation=obs, shape=self.num_agents),\n            # !terminate truncate\n            lambda rew, obs: truncation(reward=rew, observation=obs, shape=self.num_agents),\n            # terminate truncate\n            lambda rew, obs: termination(reward=rew, observation=obs, shape=self.num_agents),\n        ],\n        reward,\n        observation,\n    )\n    timestep.extras = self._get_extra_info(state, timestep)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/macvrp/","title":"Macvrp","text":"<p>               Bases: <code>Environment[State, BoundedArray, Observation]</code></p> <p>Multi-Vehicle Routing Problems with Soft Time Windows (MVRPSTW) environment as described in [1]. We simplfy the naming to multi-agent capacitated vehicle routing problem (MultiCVRP).</p> <ul> <li> <p>reward: jax array (float32)     this global reward is provided to each agent. The reward is equal to the negative sum of the     distances between consecutive nodes at the end of the episode over all agents. All time     penalties are also added to the reward.</p> </li> <li> <p>observation and state:     the observation and state variable types are defined in:     jumanji/environments/routing/multi_cvrp/types.py</p> </li> </ul> <p>[1] Zhang et al. (2020). \"Multi-Vehicle Routing Problems with Soft Time Windows: A Multi-Agent Reinforcement Learning Approach\".</p> <pre><code>from jumanji.environments import MultiCVRP\nenv = MultiCVRP()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>MultiCVRP</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>UniformRandomGenerator</code>]. Defaults to <code>UniformRandomGenerator</code> with <code>num_customers=20</code> and <code>num_vehicles=2</code>.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p><code>RewardFn</code> whose <code>__call__</code> method computes the reward of an environment transition. The function must compute the reward based on the current state and whether the environment is done. Implemented options are [<code>DenseReward</code>, <code>SparseReward</code>]. Defaults to <code>DenseReward</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>MultiCVRPViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/multi_cvrp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer] = None,\n):\n    \"\"\"\n    Instantiates a `MultiCVRP` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`UniformRandomGenerator`].\n            Defaults to `UniformRandomGenerator` with `num_customers=20` and `num_vehicles=2`.\n        reward_fn: `RewardFn` whose `__call__` method computes the reward of an environment\n            transition. The function must compute the reward based on the current state\n            and whether the environment is done.\n            Implemented options are [`DenseReward`, `SparseReward`]. Defaults to `DenseReward`.\n        viewer: `Viewer` used for rendering. Defaults to `MultiCVRPViewer` with \"human\" render\n            mode.\n    \"\"\"\n\n    # Create generator used for generating new environments\n    self._generator = generator or UniformRandomGenerator(\n        num_customers=20,\n        num_vehicles=2,\n    )\n\n    self._max_capacity = self._generator._max_capacity\n    self._map_max = self._generator._map_max\n    self._customer_demand_max = self._generator._customer_demand_max\n    self._max_start_window = self._generator._max_start_window\n    self._max_end_window = self._generator._max_end_window\n    self._time_window_length = self._generator._time_window_length\n    self._early_coef_rand = self._generator._early_coef_rand\n    self._late_coef_rand = self._generator._late_coef_rand\n    self._num_customers = self._generator._num_customers\n    self._num_vehicles = self._generator._num_vehicles\n\n    # Create reward function used for computing rewards\n    self._reward_fn = reward_fn or DenseReward(\n        self._num_vehicles, self._num_customers, self._map_max\n    )\n\n    # Create viewer used for rendering\n    self._viewer = viewer or MultiCVRPViewer(\n        name=\"MultiCVRP\",\n        num_vehicles=self._num_vehicles,\n        num_customers=self._num_customers,\n        map_max=self._map_max,\n        render_mode=\"human\",\n    )\n\n    # From the paper: All distances are represented by Euclidean distances in the plane,\n    # and the speeds of all  vehicles are assumed to be identical (i.e., it takes one\n    # unit of time to travel one unit of distance)\n    self._speed: int = 1\n\n    self._max_local_time = (\n        max_single_vehicle_distance(self._map_max, self._num_customers) / self._speed\n    )\n    super().__init__()\n</code></pre>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>BoundedArray</code> <p>a <code>specs.BoundedArray</code> spec.</p>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Name Type Description <code>observation_spec</code> <code>Spec[Observation]</code> <p>a Tuple containing the spec for each of the constituent fields of an observation.</p>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Create an animation from a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation that can export to gif, mp4, or render with HTML.</p> Source code in <code>jumanji/environments/routing/multi_cvrp/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Create an animation from a sequence of states.\n\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation that can export to gif, mp4, or render with HTML.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment. This rendering shows the layout of the tour so     far with the cities as circles, and the depot as a square.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>environment state to render.</p> required <code>save_path</code> <p>the optional path where the image should be saved.</p> required <p>Returns:</p> Name Type Description <code>rgb_array</code> <code>Optional[NDArray]</code> <p>the RGB image of the state as an array.</p> Source code in <code>jumanji/environments/routing/multi_cvrp/env.py</code> <pre><code>def render(\n    self,\n    state: State,\n) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment. This rendering shows the layout of the tour so\n        far with the cities as circles, and the depot as a square.\n\n    Args:\n        state: environment state to render.\n        save_path: the optional path where the image should be saved.\n\n    Returns:\n        rgb_array: the RGB image of the state as an array.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the problem and the start node.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/multi_cvrp/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"\n    Resets the environment.\n\n    Args:\n        key: used to randomly generate the problem and the start node.\n\n    Returns:\n         state: State object corresponding to the new state of the environment.\n         timestep: TimeStep object corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n\n    state = self._generator(key)\n\n    timestep = restart(observation=self._state_to_observation(state))\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/macvrp/#jumanji.environments.routing.multi_cvrp.env.MultiCVRP.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the index of the next nodes to visit.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>state, timestep: Tuple[State, TimeStep] containing the next state of the environment, as well as the timestep to be observed.</p> Source code in <code>jumanji/environments/routing/multi_cvrp/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"\n    Run one timestep of the environment's dynamics.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the index of the next nodes to visit.\n\n    Returns:\n        state, timestep: Tuple[State, TimeStep] containing the next state of the environment,\n            as well as the timestep to be observed.\n    \"\"\"\n    new_state = self._update_state(state, action)\n\n    is_done = (\n        (new_state.nodes.demands.sum() == 0) &amp; (new_state.vehicles.positions == DEPOT_IDX).all()\n    ) | jnp.any(new_state.step_count &gt; self._num_customers * 2)\n\n    reward = self._reward_fn(state, new_state, is_done)\n\n    timestep = self._state_to_timestep(new_state, reward, is_done)\n\n    return new_state, timestep\n</code></pre>"},{"location":"api/environments/maze/","title":"Maze","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>A JAX implementation of a 2D Maze. The goal is to navigate the maze to find the target position.</p> <ul> <li> <p>observation:</p> <ul> <li>agent_position: current 2D Position of agent.</li> <li>target_position: 2D Position of target cell.</li> <li>walls: jax array (bool) of shape (num_rows, num_cols)     whose values are <code>True</code> where walls are and <code>False</code> for empty cells.</li> <li>action_mask: array (bool) of shape (4,)     defining the available actions in the current position.</li> <li>step_count: jax array (int32) of shape ()     step number of the episode.</li> </ul> </li> <li> <p>action: jax array (int32) of shape () specifying which action to take: [0,1,2,3] correspond to     [Up, Right, Down, Left]. If an invalid action is taken, i.e. there is a wall blocking the     action, then no action (no-op) is taken.</p> </li> <li> <p>reward: jax array (float32) of shape (): 1 if the target is reached, 0 otherwise.</p> </li> <li> <p>episode termination (if any):</p> <ul> <li>agent reaches the target position.</li> <li>the time_limit is reached.</li> </ul> </li> <li> <p>state: State:</p> <ul> <li>agent_position: current 2D Position of agent.</li> <li>target_position: 2D Position of target cell.</li> <li>walls: jax array (bool) of shape (num_rows, num_cols)     whose values are <code>True</code> where walls are and <code>False</code> for empty cells.</li> <li>action_mask: array (bool) of shape (4,)     defining the available actions in the current position.</li> <li>step_count: jax array (int32) of shape ()     step number of the episode.</li> <li>key: random key (uint) of shape (2,).</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Maze\nenv = Maze()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>Maze</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>ToyGenerator</code>, <code>RandomGenerator</code>]. Defaults to <code>RandomGenerator</code> with <code>num_rows=10</code> and <code>num_cols=10</code>.</p> <code>None</code> <code>time_limit</code> <code>Optional[int]</code> <p>the time_limit of an episode, i.e. the maximum number of environment steps before the episode terminates. By default, <code>time_limit = num_rows * num_cols</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>MazeEnvViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    time_limit: Optional[int] = None,\n    viewer: Optional[Viewer[State]] = None,\n) -&gt; None:\n    \"\"\"Instantiates a `Maze` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`ToyGenerator`, `RandomGenerator`].\n            Defaults to `RandomGenerator` with `num_rows=10` and `num_cols=10`.\n        time_limit: the time_limit of an episode, i.e. the maximum number of environment steps\n            before the episode terminates. By default, `time_limit = num_rows * num_cols`.\n        viewer: `Viewer` used for rendering. Defaults to `MazeEnvViewer` with \"human\" render\n            mode.\n    \"\"\"\n    self.generator = generator or RandomGenerator(num_rows=10, num_cols=10)\n    self.num_rows = self.generator.num_rows\n    self.num_cols = self.generator.num_cols\n    super().__init__()\n    self.shape = (self.num_rows, self.num_cols)\n    self.time_limit = time_limit or self.num_rows * self.num_cols\n\n    # Create viewer used for rendering\n    self._viewer = viewer or MazeEnvViewer(\"Maze\", render_mode=\"human\")\n</code></pre>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. 4 actions: [0,1,2,3] -&gt; [Up, Right, Down, Left].</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>discrete action space with 4 values.</p>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>Maze</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>agent_position: tree of BoundedArray (int32) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>target_position: tree of BoundedArray (int32) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>walls: BoundedArray (bool) of shape (num_rows, num_cols).</li> </ul> <code>Spec[Observation]</code> <ul> <li>step_count: Array (int32) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (4,).</li> </ul>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>Maze</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `Maze` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment.\n\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment by calling the instance generator for a new instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment since it is stochastic.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment after a reset.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the first timestep returned by the environment after a reset.</p> Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment by calling the instance generator for a new instance.\n\n    Args:\n        key: random key used to reset the environment since it is stochastic.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environment after a reset.\n        timestep: `TimeStep` object corresponding the first timestep returned by the environment\n            after a reset.\n    \"\"\"\n\n    state = self.generator(key)\n\n    # Create the action mask and update the state\n    state.action_mask = self._compute_action_mask(state.walls, state.agent_position)\n\n    # Generate the observation from the environment state.\n    observation = self._observation_from_state(state)\n\n    # Return a restart timestep whose step type is FIRST.\n    timestep = restart(observation)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/maze/#jumanji.environments.routing.maze.env.Maze.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>If an action is invalid, the agent does not move, i.e. the episode does not automatically terminate.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>(int32) specifying which action to take: [0,1,2,3] correspond to [Up, Right, Down, Left]. If an invalid action is taken, i.e. there is a wall blocking the action, then no action (no-op) is taken.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the next timestep to be observed.</p> Source code in <code>jumanji/environments/routing/maze/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"\n    Run one timestep of the environment's dynamics.\n\n    If an action is invalid, the agent does not move, i.e. the episode does not\n    automatically terminate.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: (int32) specifying which action to take: [0,1,2,3] correspond to\n            [Up, Right, Down, Left]. If an invalid action is taken, i.e. there is a wall\n            blocking the action, then no action (no-op) is taken.\n\n    Returns:\n        state: the next state of the environment.\n        timestep: the next timestep to be observed.\n    \"\"\"\n    # If the chosen action is invalid, i.e. blocked by a wall, overwrite it to no-op.\n    action = jax.lax.select(state.action_mask[action], action, 4)\n\n    # Take the action in the environment:  up, right, down, or left\n    # Remember the walls coordinates: (0,0) is top left.\n    agent_position = jax.lax.switch(\n        action,\n        [\n            lambda position: Position(position.row - 1, position.col),  # Up\n            lambda position: Position(position.row, position.col + 1),  # Right\n            lambda position: Position(position.row + 1, position.col),  # Down\n            lambda position: Position(position.row, position.col - 1),  # Left\n            lambda position: position,  # No-op\n        ],\n        state.agent_position,\n    )\n\n    # Generate action mask to keep in the state for the next step and\n    # to provide to the agent in the observation.\n    action_mask = self._compute_action_mask(state.walls, agent_position)\n\n    # Build the state.\n    state = State(\n        agent_position=agent_position,\n        target_position=state.target_position,\n        walls=state.walls,\n        action_mask=action_mask,\n        key=state.key,\n        step_count=state.step_count + 1,\n    )\n    # Generate the observation from the environment state.\n    observation = self._observation_from_state(state)\n\n    # Check if the episode terminates (i.e. done is True).\n    no_actions_available = ~jnp.any(action_mask)\n    target_reached = state.agent_position == state.target_position\n    time_limit_exceeded = state.step_count &gt;= self.time_limit\n\n    done = no_actions_available | target_reached | time_limit_exceeded\n\n    # Compute the reward.\n    reward = jnp.array(state.agent_position == state.target_position, float)\n\n    # Return either a MID or a LAST timestep depending on done.\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n    return state, timestep\n</code></pre>"},{"location":"api/environments/minesweeper/","title":"Minesweeper","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>A JAX implementation of the minesweeper game.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>board: jax array (int32) of shape (num_rows, num_cols):     each cell contains -1 if not yet explored, or otherwise the number of mines in     the 8 adjacent squares.</li> <li>action_mask: jax array (bool) of shape (num_rows, num_cols):     indicates which actions are valid (not yet explored squares).</li> <li>num_mines: jax array (int32) of shape <code>()</code>, indicates the number of mines to locate.</li> <li>step_count: jax array (int32) of shape ():     specifies how many timesteps have elapsed since environment reset.</li> </ul> </li> <li> <p>action:     multi discrete array containing the square to explore (row and col).</p> </li> <li> <p>reward: jax array (float32):     Configurable function of state and action. By default:         1 for every timestep where a valid action is chosen that doesn't reveal a mine,         0 for revealing a mine or selecting an already revealed square             (and terminate the episode).</p> </li> <li> <p>episode termination:     Configurable function of state, next_state, and action. By default:         Stop the episode if a mine is explored, an invalid action is selected         (exploring an already explored square), or the board is solved.</p> </li> <li> <p>state: <code>State</code></p> <ul> <li>board: jax array (int32) of shape (num_rows, num_cols):     each cell contains -1 if not yet explored, or otherwise the number of mines in     the 8 adjacent squares.</li> <li>step_count: jax array (int32) of shape ():     specifies how many timesteps have elapsed since environment reset.</li> <li>flat_mine_locations: jax array (int32) of shape (num_rows * num_cols,):     indicates the (flat) locations of all the mines on the board.     Will be of length num_mines.</li> <li>key: jax array (int32) of shape (2,) used for seeding the sampling of mine placement     on reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Minesweeper\nenv = Minesweeper()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiate a <code>Minesweeper</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> to generate problem instances on environment reset. Implemented options are [<code>SamplingGenerator</code>]. Defaults to <code>SamplingGenerator</code>. The generator will have attributes:     - num_rows: number of rows, i.e. height of the board. Defaults to 10.     - num_cols: number of columns, i.e. width of the board. Defaults to 10.     - num_mines: number of mines generated. Defaults to 10.</p> <code>None</code> <code>reward_function</code> <code>Optional[RewardFn]</code> <p><code>RewardFn</code> whose <code>__call__</code> method computes the reward of an environment transition based on the given current state and selected action. Implemented options are [<code>DefaultRewardFn</code>]. Defaults to <code>DefaultRewardFn</code>, giving a reward of 1.0 for revealing an empty square, 0.0 for revealing a mine, and 0.0 for an invalid action (selecting an already revealed square).</p> <code>None</code> <code>done_function</code> <code>Optional[DoneFn]</code> <p><code>DoneFn</code> whose <code>__call__</code> method computes the done signal given the current state, action taken, and next state. Implemented options are [<code>DefaultDoneFn</code>]. Defaults to <code>DefaultDoneFn</code>, ending the episode on solving the board, revealing a mine, or picking an invalid action.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> to support rendering and animation methods. Implemented options are [<code>MinesweeperViewer</code>]. Defaults to <code>MinesweeperViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_function: Optional[RewardFn] = None,\n    done_function: Optional[DoneFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiate a `Minesweeper` environment.\n\n    Args:\n        generator: `Generator` to generate problem instances on environment reset.\n            Implemented options are [`SamplingGenerator`]. Defaults to `SamplingGenerator`.\n            The generator will have attributes:\n                - num_rows: number of rows, i.e. height of the board. Defaults to 10.\n                - num_cols: number of columns, i.e. width of the board. Defaults to 10.\n                - num_mines: number of mines generated. Defaults to 10.\n        reward_function: `RewardFn` whose `__call__` method computes the reward of an\n            environment transition based on the given current state and selected action.\n            Implemented options are [`DefaultRewardFn`]. Defaults to `DefaultRewardFn`, giving\n            a reward of 1.0 for revealing an empty square, 0.0 for revealing a mine, and\n            0.0 for an invalid action (selecting an already revealed square).\n        done_function: `DoneFn` whose `__call__` method computes the done signal given the\n            current state, action taken, and next state.\n            Implemented options are [`DefaultDoneFn`]. Defaults to `DefaultDoneFn`, ending the\n            episode on solving the board, revealing a mine, or picking an invalid action.\n        viewer: `Viewer` to support rendering and animation methods.\n            Implemented options are [`MinesweeperViewer`]. Defaults to `MinesweeperViewer`.\n    \"\"\"\n    self.reward_function = reward_function or DefaultRewardFn(\n        revealed_empty_square_reward=1.0,\n        revealed_mine_reward=0.0,\n        invalid_action_reward=0.0,\n    )\n    self.done_function = done_function or DefaultDoneFn()\n    self.generator = generator or UniformSamplingGenerator(\n        num_rows=10, num_cols=10, num_mines=10\n    )\n    self.num_rows = self.generator.num_rows\n    self.num_cols = self.generator.num_cols\n    self.num_mines = self.generator.num_mines\n    super().__init__()\n    self._viewer = viewer or MinesweeperViewer(num_rows=self.num_rows, num_cols=self.num_cols)\n</code></pre>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. An action consists of the height and width of the square to be explored.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p><code>specs.MultiDiscreteArray</code> object.</p>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>Minesweeper</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are: - board: BoundedArray (int32) of shape (num_rows, num_cols). - action_mask: BoundedArray (bool) of shape (num_rows, num_cols). - num_mines: BoundedArray (int32) of shape (). - step_count: BoundedArray (int32) of shape ().</p>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the board based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>a list of <code>State</code> objects representing the sequence of states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the board based on the sequence of states.\n\n    Args:\n        states: a list of `State` objects representing the sequence of states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup. Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the board.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the current state to be rendered.</p> required Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the board.\n\n    Args:\n        state: the current state to be rendered.\n    \"\"\"\n    return self._viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>needed for placing mines.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> corresponding to the new state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: needed for placing mines.\n\n    Returns:\n        state: `State` corresponding to the new state of the environment,\n        timestep: `TimeStep` corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n    state = self.generator(key)\n    observation = self._state_to_observation(state=state)\n    timestep = restart(observation=observation)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/minesweeper/#jumanji.environments.logic.minesweeper.env.Minesweeper.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p><code>Array</code> containing the row and column of the square to be explored.</p> required <p>Returns:</p> Name Type Description <code>next_state</code> <code>State</code> <p><code>State</code> corresponding to the next state of the environment,</p> <code>next_timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the timestep returned by the environment.</p> Source code in <code>jumanji/environments/logic/minesweeper/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: `Array` containing the row and column of the square to be explored.\n\n    Returns:\n        next_state: `State` corresponding to the next state of the environment,\n        next_timestep: `TimeStep` corresponding to the timestep returned by the environment.\n    \"\"\"\n    board = state.board.at[tuple(action)].set(count_adjacent_mines(state=state, action=action))\n    step_count = state.step_count + 1\n    next_state = State(\n        board=board,\n        step_count=step_count,\n        key=state.key,\n        flat_mine_locations=state.flat_mine_locations,\n    )\n    reward = self.reward_function(state, action)\n    done = self.done_function(state, next_state, action)\n    next_observation = self._state_to_observation(state=next_state)\n    next_timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_observation,\n    )\n    return next_state, next_timestep\n</code></pre>"},{"location":"api/environments/mmst/","title":"MMST","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>The <code>MMST</code> (Multi Minimum Spanning Tree) environment consists of a random connected graph with groups of nodes (same node types) that needs to be connected. The goal of the environment is to connect all nodes of the same type together without using the same utility nodes (nodes that do not belong to any group of nodes).</p> <p>Note: routing problems are randomly generated and may not be solvable!</p> <p>Requirements: The total number of nodes should be at least 20% more than the number of nodes we want to connect to guarantee we have enough remaining nodes to create a path with all the nodes we want to connect. An exception will be raised if the number of nodes is not greater than (0.8 x num_agents x num_nodes_per_agent).</p> <ul> <li> <p>observation: Observation</p> <ul> <li>node_types: jax array (int) of shape (num_nodes):     the component type of each node (-1 represents utility nodes).</li> <li>adj_matrix: jax array (bool) of shape (num_nodes, num_nodes):     adjacency matrix of the graph.</li> <li>positions: jax array (int) of shape (num_agents,):     the index of the last visited node.</li> <li>step_count: jax array (int) of shape ():     integer to keep track of the number of steps.</li> <li>action_mask: jax array (bool) of shape (num_agent, num_nodes):     binary mask (False/True &lt;--&gt; invalid/valid action).</li> </ul> </li> <li> <p>reward: float</p> </li> <li> <p>action: jax array (int) of shape (num_agents,): [0,1,..., num_nodes-1]     Each agent selects the next node to which it wants to connect.</p> </li> <li> <p>state: State</p> <ul> <li>node_type: jax array (int) of shape (num_nodes,).     the component type of each node (-1 represents utility nodes).</li> <li>adj_matrix: jax array (bool) of shape (num_nodes, num_nodes):     adjacency matrix of the graph.</li> <li>connected_nodes: jax array (int) of shape (num_agents, time_limit).     we only count each node visit once.</li> <li>connected_nodes_index: jax array (int) of shape (num_agents, num_nodes).</li> <li>position_index: jax array (int) of shape (num_agents,).</li> <li>node_edges: jax array (int) of shape (num_agents, num_nodes, num_nodes).</li> <li>positions: jax array (int) of shape (num_agents,).     the index of the last visited node.</li> <li>action_mask: jax array (bool) of shape (num_agent, num_nodes).     binary mask (False/True &lt;--&gt; invalid/valid action).</li> <li>finished_agents: jax array (bool) of shape (num_agent,).</li> <li>nodes_to_connect: jax array (int) of shape (num_agents, num_nodes_per_agent).</li> <li>step_count: step counter.</li> <li>time_limit: the number of steps allowed before an episode terminates.</li> <li>key: PRNG key for random sample.</li> </ul> </li> <li> <p>constants definitions:</p> <ul> <li> <p>Nodes</p> <ul> <li>INVALID_NODE = -1: used to check if an agent selects an invalid node.     A node may be invalid if its has no edge with the current node or if it is a     utility node already selected by another agent.</li> <li>UTILITY_NODE = -1: utility node (belongs to no agent).</li> <li>EMPTY_NODE = -1: used for padding.     state.connected_nodes stores the path (all the nodes) visited by an agent. Hence     it has size equal to the step limit. We use this constant to initialise this array     since 0 represents the first node.</li> <li>DUMMY_NODE = -10: used for tie-breaking if multiple agents select the same node.</li> </ul> </li> <li> <p>Edges</p> <ul> <li>EMPTY_EDGE = -1: used for masking edges array.    state.node_edges is the graph's adjacency matrix, but we don't represent it    using 0s and 1s, we use the node values instead, i.e <code>A_ij = j</code> or <code>A_ij = -1</code>.    Also edges are masked when utility nodes    are selected by an agent to make it unaccessible by other agents.</li> </ul> </li> <li> <p>Actions encoding</p> <ul> <li>INVALID_CHOICE = -1</li> <li>INVALID_TIE_BREAK = -2</li> <li>INVALID_ALREADY_TRAVERSED = -3</li> </ul> </li> </ul> </li> </ul> <pre><code>from jumanji.environments import MMST\nenv = MMST()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Create the <code>MMST</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>SplitRandomGenerator</code>]. Defaults to <code>SplitRandomGenerator(num_nodes=36, num_edges=72, max_degree=5, num_agents=3, num_nodes_per_agent=4, max_step=time_limit)</code>.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>class of type <code>RewardFn</code>, whose <code>__call__</code> is used as a reward function. Implemented options are [<code>DenseRewardFn</code>]. Defaults to <code>DenseRewardFn(reward_values=(10.0, -1.0, -1.0))</code>.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>the number of steps allowed before an episode terminates. Defaults to 70.</p> <code>70</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>MMSTViewer</code></p> <code>None</code> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    time_limit: int = 70,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Create the `MMST` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`SplitRandomGenerator`].\n            Defaults to `SplitRandomGenerator(num_nodes=36, num_edges=72, max_degree=5,\n            num_agents=3, num_nodes_per_agent=4, max_step=time_limit)`.\n        reward_fn: class of type `RewardFn`, whose `__call__` is used as a reward function.\n            Implemented options are [`DenseRewardFn`].\n            Defaults to `DenseRewardFn(reward_values=(10.0, -1.0, -1.0))`.\n        time_limit: the number of steps allowed before an episode terminates. Defaults to 70.\n        viewer: `Viewer` used for rendering. Defaults to `MMSTViewer`\n    \"\"\"\n\n    self._generator = generator or SplitRandomGenerator(\n        num_nodes=36,\n        num_edges=72,\n        max_degree=5,\n        num_agents=3,\n        num_nodes_per_agent=4,\n        max_step=time_limit,\n    )\n\n    self.num_agents = self._generator.num_agents\n    self.num_nodes = self._generator.num_nodes\n    self.num_nodes_per_agent = self._generator.num_nodes_per_agent\n\n    self._reward_fn = reward_fn or DenseRewardFn(reward_values=(10.0, -1.0, -1.0))\n\n    self._env_viewer = viewer or MMSTViewer(num_agents=self.num_agents)\n    self.time_limit = time_limit\n    super().__init__()\n</code></pre>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p>a <code>specs.MultiDiscreteArray</code> spec.</p>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>node_types: BoundedArray (int32) of shape (num_nodes,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>adj_matrix: BoundedArray (int) of shape (num_nodes, num_nodes). Represents the adjacency matrix of the graph.</li> </ul> <code>Spec[Observation]</code> <ul> <li>positions: BoundedArray (int32) of shape (num_agents). Current node position of agent.</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_agents, num_nodes,). Represents the valid actions in the current state.</li> </ul>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Calls the environment renderer to animate a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>List of states to animate.</p> required <code>interval</code> <code>int</code> <p>Time between frames in milliseconds, defaults to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>Optional path to save the animation.</p> <code>None</code> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Calls the environment renderer to animate a sequence of states.\n\n    Args:\n        states: List of states to animate.\n        interval: Time between frames in milliseconds, defaults to 200.\n        save_path: Optional path to save the animation.\n    \"\"\"\n    return self._env_viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.get_finished_agents","title":"<code>get_finished_agents(state)</code>","text":"<p>Get the done flags for each agent.</p> <p>Parameters:</p> Name Type Description Default <code>node_types</code> <p>the environment state node_types.</p> required <code>connected_nodes</code> <p>the agent specifc view of connected nodes</p> required <p>Returns:     Array : array of boolean flags in the shape (number of agents, ).</p> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def get_finished_agents(self, state: State) -&gt; chex.Array:\n    \"\"\"Get the done flags for each agent.\n\n    Args:\n        node_types: the environment state node_types.\n        connected_nodes: the agent specifc view of connected nodes\n    Returns:\n        Array : array of boolean flags in the shape (number of agents, ).\n    \"\"\"\n\n    def done_fun(nodes: chex.Array, connected_nodes: chex.Array, n_comps: int) -&gt; jnp.bool_:\n        connects = jnp.isin(nodes, connected_nodes)\n        return jnp.sum(connects) == n_comps\n\n    finished_agents = jax.vmap(done_fun, in_axes=(0, 0, None))(\n        state.nodes_to_connect,\n        state.connected_nodes,\n        self.num_nodes_per_agent,\n    )\n\n    return finished_agents\n</code></pre>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.render","title":"<code>render(state)</code>","text":"<p>Render the environment for a given state.</p> <p>Returns:</p> Type Description <code>Array</code> <p>Array of rgb pixel values in the shape (width, height, rgb).</p> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def render(self, state: State) -&gt; chex.Array:\n    \"\"\"Render the environment for a given state.\n\n    Returns:\n        Array of rgb pixel values in the shape (width, height, rgb).\n    \"\"\"\n    return self._env_viewer.render(state)\n</code></pre>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the problem and the different start nodes.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: used to randomly generate the problem and the different start nodes.\n\n    Returns:\n         state: State object corresponding to the new state of the environment.\n         timestep: TimeStep object corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n\n    key, problem_key = jax.random.split(key)\n    state = self._generator(problem_key)\n    extras = self._get_extras(state)\n    timestep = restart(observation=self._state_to_observation(state), extras=extras)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/mmst/#jumanji.environments.routing.mmst.env.MMST.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the index of the next node to visit.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>state, timestep: Tuple[State, TimeStep] containing the next state of the environment, as well as the timestep to be observed.</p> Source code in <code>jumanji/environments/routing/mmst/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the index of the next node to visit.\n\n    Returns:\n        state, timestep: Tuple[State, TimeStep] containing the next state of the\n           environment, as well as the timestep to be observed.\n    \"\"\"\n\n    def step_agent_fn(\n        connected_nodes: chex.Array,\n        conn_index: chex.Array,\n        action: chex.Array,\n        node: int,\n        indices: chex.Array,\n        agent_id: int,\n    ) -&gt; Tuple[chex.Array, ...]:\n        is_invalid_choice = jnp.any(action == INVALID_CHOICE) | jnp.any(\n            action == INVALID_TIE_BREAK\n        )\n        is_valid = (~is_invalid_choice) &amp; (node != INVALID_NODE)\n        connected_nodes, conn_index, new_node, indices = jax.lax.cond(\n            is_valid,\n            self._update_conected_nodes,\n            lambda *_: (\n                connected_nodes,\n                conn_index,\n                state.positions[agent_id],\n                indices,\n            ),\n            connected_nodes,\n            conn_index,\n            node,\n            indices,\n        )\n\n        return connected_nodes, conn_index, new_node, indices\n\n    key, step_key = jax.random.split(state.key)\n    action, next_nodes = self._trim_duplicated_invalid_actions(state, action, step_key)\n\n    connected_nodes = jnp.zeros_like(state.connected_nodes)\n    connected_nodes_index = jnp.zeros_like(state.connected_nodes_index)\n    agents_pos = jnp.zeros_like(state.positions)\n    position_index = jnp.zeros_like(state.position_index)\n\n    for agent in range(self.num_agents):\n        conn_nodes_i, conn_nodes_id, pos_i, pos_ind = step_agent_fn(\n            state.connected_nodes[agent],\n            state.connected_nodes_index[agent],\n            action[agent],\n            next_nodes[agent],\n            state.position_index[agent],\n            agent,\n        )\n\n        connected_nodes = connected_nodes.at[agent].set(conn_nodes_i)\n        connected_nodes_index = connected_nodes_index.at[agent].set(conn_nodes_id)\n        agents_pos = agents_pos.at[agent].set(pos_i)\n        position_index = position_index.at[agent].set(pos_ind)\n\n    active_node_edges = update_active_edges(\n        self.num_agents, state.node_edges, agents_pos, state.node_types\n    )\n\n    state = State(\n        node_types=state.node_types,\n        adj_matrix=state.adj_matrix,\n        nodes_to_connect=state.nodes_to_connect,\n        connected_nodes=connected_nodes,\n        connected_nodes_index=connected_nodes_index,\n        position_index=position_index,\n        positions=agents_pos,\n        node_edges=active_node_edges,\n        action_mask=make_action_mask(\n            self.num_agents,\n            self.num_nodes,\n            active_node_edges,\n            agents_pos,\n            state.finished_agents,\n        ),\n        finished_agents=state.finished_agents,  # Not updated yet.\n        step_count=state.step_count,\n        key=key,\n    )\n\n    state, timestep = self._state_to_timestep(state, action)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/pac_man/","title":"PacMan","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>A JAX implementation of the 'PacMan' game where a single agent must navigate a maze to collect pellets and avoid 4 heuristic agents. The game takes place on a 31x28 grid where the player can move in 4 directions (left, right, up, down) and collect pellets to gain points. The goal is to collect all of the pellets on the board without colliding with one of the heuristic agents. Using the AsciiGenerator the environment will always generate the same maze as long as the same Ascii diagram is in use.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>player_locations: current 2D position of agent.</li> <li>grid: jax array (int) of the ingame maze with walls.</li> <li>ghost_locations: jax array (int) of ghost positions.</li> <li>power_up_locations: jax array (int) of power-pellet locations</li> <li>pellet_locations: jax array (int) of pellets.</li> <li>action_mask: jax array (bool) defining current actions.</li> <li>score: (int32) of total points aquired.</li> </ul> </li> <li> <p>action: jax array (int) of shape () specifiying which action to take [0,1,2,3,4]     corresponding to [up, right, down, left, no-op. If there is an invalid action     taken, i.e. there is a wall blocking the action, then no action (no-op) is taken.</p> </li> <li> <p>reward: jax array (float32) of shape (): 10 per pellet collected, 20 for a power pellet     and 200 for each unique ghost eaten.</p> </li> <li> <p>episode termination (if any):</p> <ul> <li>agent has collected all pellets.</li> <li>agent killed by ghost.</li> <li>timer has elapsed.</li> </ul> </li> <li> <p>state: State:</p> <ul> <li>key: jax array (uint32) of shape(2,).</li> <li>grid: jax array (int)) of shape (31,28) of the ingame maze with walls.</li> <li>pellets: int tracking the number of pellets.</li> <li>frightened_state_time: jax array (int) of shape ()     tracks number of steps for the scatter state.</li> <li>pellet_locations: jax array (int) of pellets of shape (316,2).</li> <li>power_up_locations: jax array (int) of power-pellet locations of shape (4,2).</li> <li>player_locations: current 2D position of agent.</li> <li>ghost_locations: jax array (int) of ghost positions of shape (4,2).</li> <li>initial_player_locations: starting 2D position of agent.</li> <li>initial_ghost_positions: jax array (int) of ghost positions of shape (4,2).</li> <li>ghost_init_targets: jax array (int) of ghost positions.     used to direct ghosts on respawn.</li> <li>old_ghost_locations: jax array (int) of shape (4,2) of ghost positions from last step.     used to prevent ghost backtracking.</li> <li>ghost_init_steps: jax array (int) of shape (4,2) number of initial ghost steps.     used to determine per ghost initialisation.</li> <li>ghost_actions: jax array (int) of shape (4,).</li> <li>last_direction: int tracking the last direction of the player.</li> <li>dead: bool used to track player death.</li> <li>visited_index: jax array (int) of visited locations of shape (320,2).     used to prevent repeated pellet points.</li> <li>ghost_starts: jax array (int) of shape (4,2)     used to reset ghost positions if eaten</li> <li>scatter_targets: jax array (int) of shape (4,2)     target locations for ghosts when scatter behavior is active.</li> <li>step_count: (int32) of total steps taken from reset till current timestep.</li> <li>ghost_eaten: jax array (bool)of shape (4,) tracking if ghost has been eaten before.</li> <li>score: (int32) of total points aquired.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import pac_man\nenv = PacMan()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>PacMan</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. Implemented options are [<code>AsciiGenerator</code>].</p> <code>None</code> <code>time_limit</code> <code>Optional[int]</code> <p>the time_limit of an episode, i.e. the maximum number of environment steps before the episode terminates. By default, set to 1000.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>PacManViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    viewer: Optional[Viewer[State]] = None,\n    time_limit: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Instantiates a `PacMan` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            Implemented options are [`AsciiGenerator`].\n        time_limit: the time_limit of an episode, i.e. the maximum number of environment steps\n            before the episode terminates. By default, set to 1000.\n        viewer: `Viewer` used for rendering. Defaults to `PacManViewer`.\n    \"\"\"\n\n    self.generator = generator or AsciiGenerator(DEFAULT_MAZE)\n    self.x_size = self.generator.x_size\n    self.y_size = self.generator.y_size\n    self.pellet_spaces = self.generator.pellet_spaces\n    super().__init__()\n    self._viewer = viewer or PacManViewer(\"Pacman\", render_mode=\"human\")\n    self.time_limit = 1000 or time_limit\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>5 actions: [0,1,2,3,4] -&gt; [Up, Right, Down, Left, No-op].</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>a <code>specs.DiscreteArray</code> spec object.</p>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>PacMan</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing all the specifications for all the <code>Observation</code> fields:</p> <code>Spec[Observation]</code> <ul> <li>player_locations: tree of BoundedArray (int32) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>grid: BoundedArray (int)) of the ingame maze with walls.</li> </ul> <code>Spec[Observation]</code> <ul> <li>ghost_locations: jax array (int) of ghost positions.</li> </ul> <code>Spec[Observation]</code> <ul> <li>power_up_locations: jax array (int) of power-pellet locations</li> </ul> <code>Spec[Observation]</code> <ul> <li>pellet_locations: jax array (int) of pellet locations.</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: jax array (bool) defining current actions.</li> </ul> <code>Spec[Observation]</code> <ul> <li>frightened_state_time: int counting time remaining in scatter mode.</li> </ul> <code>Spec[Observation]</code> <ul> <li>score: (int) of total score obtained by player.</li> </ul>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>Maze</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `Maze` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.check_power_up","title":"<code>check_power_up(state)</code>","text":"<p>Check if the player is on a power-up location and update the power-up locations array accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'state` object corresponding to the new state of the environment</p> required <p>Returns:</p> Name Type Description <code>power_up_locations</code> <code>Array</code> <p>locations of the remaining power-ups</p> <code>eat</code> <code>Numeric</code> <p>a bool indicating if the player can eat the ghosts</p> <code>reward</code> <code>Numeric</code> <p>an int of the reward gained from collecting power-ups</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def check_power_up(self, state: State) -&gt; Tuple[chex.Array, chex.Numeric, chex.Numeric]:\n    \"\"\"\n    Check if the player is on a power-up location and update the power-up\n    locations array accordingly.\n\n    Args:\n        state: 'state` object corresponding to the new state of the environment\n\n    Returns:\n        power_up_locations: locations of the remaining power-ups\n        eat: a bool indicating if the player can eat the ghosts\n        reward: an int of the reward gained from collecting power-ups\n    \"\"\"\n\n    power_up_locations = jnp.array(state.power_up_locations)\n\n    player_space = state.player_locations\n    player_loc_x = player_space.x\n    player_loc_y = player_space.y\n    player_loc = jnp.array([player_loc_y, player_loc_x])\n\n    # Check if player and power_up position are shared\n    on_powerup = (player_loc == power_up_locations).all(axis=-1).any()\n    eat = on_powerup.astype(int)\n    mask = (player_loc == power_up_locations).all(axis=-1)\n    invert_mask = ~mask\n    invert_mask = invert_mask.reshape(4, 1)\n\n    # Mask out collected power-ups\n    power_up_locations = power_up_locations * invert_mask\n\n    # Assign reward for power-up\n    reward = eat * 50.0\n\n    return power_up_locations, eat, reward\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.check_rewards","title":"<code>check_rewards(state)</code>","text":"<p>Process the state of the game to compute rewards, updated pellet spaces, and remaining number of pellets.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'State` object corresponding to the current state of the environment</p> required <p>Returns:</p> Name Type Description <code>rewards</code> <code>int</code> <p>an integer representing the reward earned by the player in the current state</p> <code>pellet_spaces</code> <code>Array</code> <p>a 2D jax array showing the location of all remaining cookies</p> <code>num_cookies</code> <code>int</code> <p>an integer counting the remaining cookies on the map.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def check_rewards(self, state: State) -&gt; Tuple[int, chex.Array, int]:\n    \"\"\"\n    Process the state of the game to compute rewards, updated pellet spaces, and remaining\n    number of pellets.\n\n    Args:\n        state: 'State` object corresponding to the current state of the environment\n\n    Returns:\n        rewards: an integer representing the reward earned by the player in the current state\n        pellet_spaces: a 2D jax array showing the location of all remaining cookies\n        num_cookies: an integer counting the remaining cookies on the map.\n    \"\"\"\n\n    # Get the locations of the pellets and the player\n    pellet_spaces = jnp.array(state.pellet_locations)\n    player_space = state.player_locations\n    ps = jnp.array([player_space.y, player_space.x])\n\n    # Get the number of pellets on the map\n    num_pellets = state.pellets\n\n    # Check if player has eaten a pellet in this step\n    ate_pellet = jnp.any(jnp.all(ps == pellet_spaces, axis=-1))\n\n    # Reduce number of pellets on map if eaten, add reward and remove eaten pellet\n    num_pellets -= ate_pellet.astype(int)\n    rewards = ate_pellet * 10.0\n    mask = jnp.logical_not(jnp.all(ps == pellet_spaces, axis=-1))\n    pellet_spaces = pellet_spaces * mask[..., None]\n\n    return rewards, pellet_spaces, num_pellets\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.check_wall_collisions","title":"<code>check_wall_collisions(state, new_player_pos)</code>","text":"<p>Check if the new player position collides with a wall.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'State` object corresponding to the new state of the environment.</p> required <code>new_player_pos</code> <code>Position</code> <p>the position of the player after the last action.</p> required <p>Returns:</p> Name Type Description <code>collision</code> <code>Any</code> <p>a boolean indicating if the player has moved into a wall.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def check_wall_collisions(self, state: State, new_player_pos: Position) -&gt; Any:\n    \"\"\"\n    Check if the new player position collides with a wall.\n\n    Args:\n        state: 'State` object corresponding to the new state of the environment.\n        new_player_pos: the position of the player after the last action.\n\n    Returns:\n        collision: a boolean indicating if the player has moved into a wall.\n    \"\"\"\n\n    grid = state.grid\n    location_value = grid[new_player_pos.x, new_player_pos.y]\n\n    collision = jax.lax.cond(\n        location_value == 1,\n        lambda x: new_player_pos,\n        lambda x: state.player_locations,\n        0,\n    )\n    return collision\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.player_step","title":"<code>player_step(state, action, steps=1)</code>","text":"<p>Compute the new position of the player based on the given state and action.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'state` object corresponding to the new state of the environment.</p> required <code>action</code> <code>int</code> <p>an integer between 0 and 4 representing the player's chosen action.</p> required <code>steps</code> <code>int</code> <p>how many steps ahead of current position to search.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>new_pos</code> <code>Position</code> <p>a <code>Position</code> object representing the new position of the player.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def player_step(self, state: State, action: int, steps: int = 1) -&gt; Position:\n    \"\"\"\n    Compute the new position of the player based on the given state and action.\n\n    Args:\n        state: 'state` object corresponding to the new state of the environment.\n        action: an integer between 0 and 4 representing the player's chosen action.\n        steps: how many steps ahead of current position to search.\n\n    Returns:\n        new_pos: a `Position` object representing the new position of the player.\n    \"\"\"\n\n    position = state.player_locations\n\n    move_left = lambda position: (position.y, position.x - steps)\n    move_up = lambda position: (position.y - steps, position.x)\n    move_right = lambda position: (position.y, position.x + steps)\n    move_down = lambda position: (position.y + steps, position.x)\n    no_op = lambda position: (position.y, position.x)\n\n    new_pos_row, new_pos_col = jax.lax.switch(\n        action, [move_left, move_up, move_right, move_down, no_op], position\n    )\n\n    new_pos = Position(x=new_pos_col % self.x_size, y=new_pos_row % self.y_size)\n    return new_pos\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>state</code> object containing the current environment state.</p> required Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Render the given state of the environment.\n\n    Args:\n        state: `state` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment by calling the instance generator for a new instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>A PRNGKey to use for random number generation.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment after a reset.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding the first timestep returned by the environment after a reset.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def reset(self, key: PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment by calling the instance generator for a new instance.\n\n    Args:\n        key: A PRNGKey to use for random number generation.\n\n    Returns:\n        state: `State` object corresponding to the new state of the environment after a reset.\n        timestep: `TimeStep` object corresponding the first timestep returned by the environment\n            after a reset.\n    \"\"\"\n\n    state = self.generator(key)\n\n    # Generate observation\n    obs = self._observation_from_state(state)\n\n    # Return a restart timestep of step type is FIRST.\n    timestep = restart(observation=obs)\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/pac_man/#jumanji.environments.routing.pac_man.env.PacMan.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>If an action is invalid, the agent does not move, i.e. the episode does not automatically terminate.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>(int32) specifying which action to take: [0,1,2,3,4] correspond to [Up, Right, Down, Left, No-op]. If an invalid action is taken, i.e. there is a wall blocking the action, then no action (no-op) is taken.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the new state of the environment.</p> <code>TimeStep[Observation]</code> <p>the next timestep to be observed.</p> Source code in <code>jumanji/environments/routing/pac_man/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    If an action is invalid, the agent does not move, i.e. the episode does not\n    automatically terminate.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: (int32) specifying which action to take: [0,1,2,3,4] correspond to\n            [Up, Right, Down, Left, No-op]. If an invalid action is taken, i.e. there is a wall\n            blocking the action, then no action (no-op) is taken.\n\n    Returns:\n        state: the new state of the environment.\n        the next timestep to be observed.\n    \"\"\"\n\n    # Collect updated state based on environment dynamics\n    updated_state, collision_rewards = self._update_state(state, action)\n\n    # Create next_state from updated state\n    next_state = updated_state.replace(step_count=state.step_count + 1)  # type: ignore\n\n    # Check if episode terminates\n    num_pellets = next_state.pellets\n    dead = next_state.dead\n    time_limit_exceeded = next_state.step_count &gt;= self.time_limit\n    all_pellets_found = num_pellets == 0\n    dead = next_state.dead == 1\n    done = time_limit_exceeded | dead | all_pellets_found\n\n    reward = jnp.asarray(collision_rewards)\n    # Generate observation from the state\n    observation = self._observation_from_state(next_state)\n\n    # Return either a MID or a LAST timestep depending on done.\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/rubiks_cube/","title":"RubiksCube","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>A JAX implementation of the Rubik's Cube with a configurable cube size (by default, 3) and number of scrambles at reset.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>cube: jax array (int8) of shape (6, cube_size, cube_size):     each cell contains the index of the corresponding colour of the sticker in the scramble.</li> <li>step_count: jax array (int32) of shape ():     specifies how many timesteps have elapsed since environment reset.</li> </ul> </li> <li> <p>action:     multi discrete array containing the move to perform (face, depth, and direction).</p> </li> <li> <p>reward: jax array (float) of shape ():     by default, 1.0 if cube is solved, otherwise 0.0.</p> </li> <li> <p>episode termination:     if either the cube is solved or a time limit is reached.</p> </li> <li> <p>state: <code>State</code></p> <ul> <li>cube: jax array (int8) of shape (6, cube_size, cube_size):     each cell contains the index of the corresponding colour of the sticker in the scramble.</li> <li>step_count: jax array (int32) of shape ():     specifies how many timesteps have elapsed since environment reset.</li> <li>key: jax array (uint) of shape (2,) used for seeding the sampling for scrambling on     reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import RubiksCube\nenv = RubiksCube()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiate a <code>RubiksCube</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> used to generate problem instances on environment reset. Implemented options are [<code>ScramblingGenerator</code>]. Defaults to <code>ScramblingGenerator</code>, with 100 scrambles on reset. The generator will contain an attribute <code>cube_size</code>, corresponding to the number of cubies to an edge, and defaulting to 3.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>the number of steps allowed before an episode terminates. Defaults to 200.</p> <code>200</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p><code>RewardFn</code> whose <code>__call__</code> method computes the reward given the new state. Implemented options are [<code>SparseRewardFn</code>]. Defaults to <code>SparseRewardFn</code>, giving a reward of 1.0 if the cube is solved or otherwise 0.0.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> to support rendering and animation methods. Implemented options are [<code>RubiksCubeViewer</code>]. Defaults to <code>RubiksCubeViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    time_limit: int = 200,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiate a `RubiksCube` environment.\n\n    Args:\n        generator: `Generator` used to generate problem instances on environment reset.\n            Implemented options are [`ScramblingGenerator`]. Defaults to `ScramblingGenerator`,\n            with 100 scrambles on reset.\n            The generator will contain an attribute `cube_size`, corresponding to the number of\n            cubies to an edge, and defaulting to 3.\n        time_limit: the number of steps allowed before an episode terminates. Defaults to 200.\n        reward_fn: `RewardFn` whose `__call__` method computes the reward given the new state.\n            Implemented options are [`SparseRewardFn`]. Defaults to `SparseRewardFn`, giving a\n            reward of 1.0 if the cube is solved or otherwise 0.0.\n        viewer: `Viewer` to support rendering and animation methods.\n            Implemented options are [`RubiksCubeViewer`]. Defaults to `RubiksCubeViewer`.\n    \"\"\"\n    if time_limit &lt;= 0:\n        raise ValueError(\n            f\"The time_limit must be positive, but received time_limit={time_limit}\"\n        )\n    self.time_limit = time_limit\n    self.reward_function = reward_fn or SparseRewardFn()\n    self.generator = generator or ScramblingGenerator(\n        cube_size=3,\n        num_scrambles_on_reset=100,\n    )\n    super().__init__()\n    self._viewer = viewer or RubiksCubeViewer(\n        sticker_colors=DEFAULT_STICKER_COLORS, cube_size=self.generator.cube_size\n    )\n</code></pre>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. An action is composed of 3 elements that range in: 6 faces, each with cube_size//2 possible depths, and 3 possible directions.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p><code>MultiDiscreteArray</code> object.</p>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>RubiksCube</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing all the specifications for all the <code>Observation</code> fields: - cube: BoundedArray (jnp.int8) of shape (num_faces, cube_size, cube_size). - step_count: BoundedArray (jnp.int32) of shape ().</p>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the cube based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>a list of <code>State</code> objects representing the sequence of game states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the cube based on the sequence of states.\n\n    Args:\n        states: a list of `State` objects representing the sequence of game states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the cube.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the current state to be rendered.</p> required Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the cube.\n\n    Args:\n        state: the current state to be rendered.\n    \"\"\"\n    return self._viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>needed for scramble.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: needed for scramble.\n\n    Returns:\n        state: `State` corresponding to the new state of the environment.\n        timestep: `TimeStep` corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n    state = self.generator(key)\n    observation = self._state_to_observation(state=state)\n    timestep = restart(observation=observation)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/rubiks_cube/#jumanji.environments.logic.rubiks_cube.env.RubiksCube.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p><code>Array</code> of shape (3,) indicating the face to move, depth of the move, and the amount to move by.</p> required <p>Returns:</p> Name Type Description <code>next_state</code> <code>State</code> <p><code>State</code> corresponding to the next state of the environment.</p> <code>next_timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the timestep returned by the environment.</p> Source code in <code>jumanji/environments/logic/rubiks_cube/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: `Array` of shape (3,) indicating the face to move, depth of the move, and the\n            amount to move by.\n\n    Returns:\n        next_state: `State` corresponding to the next state of the environment.\n        next_timestep: `TimeStep` corresponding to the timestep returned by the environment.\n    \"\"\"\n    flattened_action = flatten_action(\n        unflattened_action=action, cube_size=self.generator.cube_size\n    )\n    cube = rotate_cube(\n        cube=state.cube,\n        flattened_action=flattened_action,\n    )\n    step_count = state.step_count + 1\n    next_state = State(\n        cube=cube,\n        step_count=step_count,\n        key=state.key,\n    )\n    reward = self.reward_function(state=next_state)\n    solved = is_solved(cube)\n    done = (step_count &gt;= self.time_limit) | solved\n    next_observation = self._state_to_observation(state=next_state)\n    next_timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_observation,\n    )\n    return next_state, next_timestep\n</code></pre>"},{"location":"api/environments/rware/","title":"Rware","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>A JAX implementation of the 'Robotic warehouse' environment: https://github.com/semitable/robotic-warehouse which is described in the paper [1].</p> <p>Creates a grid world where multiple agents (robots) are supposed to collect shelves, bring them to a goal and then return them.</p> <p>Below is an example warehouse floor grid: the grid layout is instantiated using three arguments</p> <ul> <li>shelf_rows: number of vertical shelf clusters</li> <li>shelf_columns: odd number of horizontal shelf clusters</li> <li>column_height: height of each cluster</li> </ul> <p>A cluster is a set of grouped shelves (two cells wide) represented below as</p> <pre><code>                XX\n</code></pre> <p>Shelf cluster -&gt;    XX    (this cluster is of height 3)                     XX</p> <p>Grid Layout:</p> <pre><code>           shelf columns (here set to 3, i.e.\n             v  v  v      shelf_columns=3, must be an odd number)\n            ----------\n         &gt;  -XX-XX-XX-        ^\n</code></pre> <p>Shelf Row 1 -&gt;  -XX-XX-XX-  Column Height (here set to 3, i.e.              &gt;  -XX-XX-XX-        v        column_height=3)                 ----------                 -XX----XX-   &lt;                 -XX----XX-   &lt;- Shelf Row 2 (here set to 2, i.e.                 -XX----XX-   &lt;              shelf_rows=2)                 ----------                 ----GG----</p> <ul> <li>G: is the goal positions where agents are rewarded if they successfully deliver a requested shelf (i.e toggle the load action inside the goal position while carrying a requested shelf).</li> </ul> <p>The final grid size will be - height: (column_height + 1) * shelf_rows + 2 - width: (2 + 1) * shelf_columns + 1</p> <p>The bottom-middle column is removed to allow for agents to queue in front of the goal positions</p> <ul> <li> <p>action: jax array (int) of shape (num_agents,) containing the action for each agent.     (0: noop, 1: forward, 2: left, 3: right, 4: toggle_load)</p> </li> <li> <p>reward: jax array (int) of shape (), global reward shared by all agents, +1     for every successful delivery of a requested shelf to the goal position.</p> </li> <li> <p>episode termination:</p> <ul> <li>The number of steps is greater than the limit.</li> <li>Any agent selects an action which causes two agents to collide.</li> </ul> </li> <li> <p>state: State</p> <ul> <li>grid: an array representing the warehouse floor as a 2D grid with two separate channels     one for the agents, and one for the shelves</li> <li>agents: a pytree of Agent type with per agent leaves: [position, direction, is_carrying]</li> <li>shelves: a pytree of Shelf type with per shelf leaves: [position, is_requested]</li> <li>request_queue: the queue of requested shelves (by ID).</li> <li>step_count: an integer representing the current step of the episode.</li> <li>action_mask: an array of shape (num_agents, 5) containing the valid actions     for each agent.</li> <li>key: a pseudorandom number generator key.</li> </ul> </li> </ul> <p>[1] Papoudakis et al., Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms     in Cooperative Tasks (2021)</p> <pre><code>from jumanji.environments import RobotWarehouse\nenv = RobotWarehouse()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates an <code>RobotWarehouse</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p>callable to instantiate environment instances. Defaults to <code>RandomGenerator</code> with parameters: <code>shelf_rows = 2</code>, <code>shelf_columns = 3</code>, <code>column_height = 8</code>, <code>num_agents = 4</code>, <code>sensor_range = 1</code>, <code>request_queue_size = 8</code>.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>the maximum step limit allowed within the environment. Defaults to 500.</p> <code>500</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p>viewer to render the environment. Defaults to <code>RobotWarehouseViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    time_limit: int = 500,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates an `RobotWarehouse` environment.\n\n    Args:\n        generator: callable to instantiate environment instances.\n            Defaults to `RandomGenerator` with parameters:\n            `shelf_rows = 2`,\n            `shelf_columns = 3`,\n            `column_height = 8`,\n            `num_agents = 4`,\n            `sensor_range = 1`,\n            `request_queue_size = 8`.\n        time_limit: the maximum step limit allowed within the environment.\n            Defaults to 500.\n        viewer: viewer to render the environment. Defaults to `RobotWarehouseViewer`.\n    \"\"\"\n\n    # default generator is: robot_warehouse-tiny-4ag-easy (in original implementation)\n    self._generator = generator or RandomGenerator(\n        column_height=8,\n        shelf_rows=2,\n        shelf_columns=3,\n        num_agents=4,\n        sensor_range=1,\n        request_queue_size=8,\n    )\n\n    self.goals: List[Tuple[int, int]] = []\n    self.grid_size = self._generator.grid_size\n    self.request_queue_size = self._generator.request_queue_size\n\n    self.num_agents = self._generator.num_agents\n    self.sensor_range = self._generator.sensor_range\n    self.highways = self._generator.highways\n    self.shelf_ids = self._generator.shelf_ids\n    self.not_in_queue_size = self._generator.not_in_queue_size\n\n    self.agent_ids = jnp.arange(self.num_agents)\n    self.directions = jnp.array([d.value for d in Direction])\n    self.num_obs_features = utils.calculate_num_observation_features(self.sensor_range)\n    self.goals = self._generator.goals\n    self.time_limit = time_limit\n    super().__init__()\n\n    # create viewer for rendering environment\n    self._viewer = viewer or RobotWarehouseViewer(self.grid_size, self.goals, \"RobotWarehouse\")\n</code></pre>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. 5 actions: [0,1,2,3,4] -&gt; [No Op, Forward, Left, Right, Toggle_load].</p> <p>Since this is a multi-agent environment, the environment expects an array of actions. This array is of shape (num_agents,).</p>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specification of the observation of the <code>RobotWarehouse</code> environment. Returns:     Spec for the <code>Observation</code>, consisting of the fields:         - agents_view: Array (int32) of shape (num_agents, num_obs_features).         - action_mask: BoundedArray (bool) of shape (num_agent, 5).         - step_count: BoundedArray (int32) of shape ().</p>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animation from a sequence of RobotWarehouse states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>Animation object that can be saved as a GIF, MP4, or rendered with HTML.</p> Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animation from a sequence of RobotWarehouse states.\n\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None,\n            the plot will not be saved.\n\n    Returns:\n        Animation object that can be saved as a GIF, MP4, or rendered with HTML.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the RobotWarehouse environment.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>is the current environment state to be rendered.</p> required <code>save_path</code> <p>the path where the image should be saved. If it is None, the plot</p> required Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the RobotWarehouse environment.\n\n    Args:\n        state: is the current environment state to be rendered.\n        save_path: the path where the image should be saved. If it is None, the plot\n        will not be stored.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to reset the environment since it is stochastic.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: random key used to reset the environment since it is stochastic.\n\n    Returns:\n        state: State object corresponding to the new state of the environment.\n        timestep: TimeStep object corresponding the first timestep returned by the environment.\n    \"\"\"\n    # create environment state\n    state = self._generator(key)\n\n    # collect first observations and create timestep\n    agents_view = self._make_observations(state.grid, state.agents, state.shelves)\n    observation = Observation(\n        agents_view=agents_view,\n        action_mask=state.action_mask,\n        step_count=state.step_count,\n    )\n    timestep = restart(observation=observation)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/rware/#jumanji.environments.routing.robot_warehouse.env.RobotWarehouse.step","title":"<code>step(state, action)</code>","text":"<p>Perform an environment step.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p>Array containing the action to take. - 0 no op - 1 move forward - 2 turn left - 3 turn right - 4 toggle load</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding the timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/robot_warehouse/env.py</code> <pre><code>def step(\n    self,\n    state: State,\n    action: chex.Array,\n) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Perform an environment step.\n\n    Args:\n        state: State object containing the dynamics of the environment.\n        action: Array containing the action to take.\n            - 0 no op\n            - 1 move forward\n            - 2 turn left\n            - 3 turn right\n            - 4 toggle load\n\n    Returns:\n        state: State object corresponding to the next state of the environment.\n        timestep: TimeStep object corresponding the timestep returned by the environment.\n    \"\"\"\n\n    # unpack state\n    key = state.key\n    grid = state.grid\n    agents = state.agents\n    shelves = state.shelves\n    request_queue = state.request_queue\n\n    # check for invalid action -&gt; turn into noops\n    actions = utils.get_valid_actions(action, state.action_mask)\n\n    # update agents, shelves and grid\n    def update_state_scan(\n        carry_info: Tuple[chex.Array, chex.Array, chex.Array, int], action: int\n    ) -&gt; Tuple[Tuple[chex.Array, chex.Array, chex.Array, int], None]:\n        grid, agents, shelves, agent_id = carry_info\n        grid, agents, shelves = self._update_state(grid, agents, shelves, action, agent_id)\n        return (grid, agents, shelves, agent_id + 1), None\n\n    (grid, agents, shelves, _), _ = jax.lax.scan(\n        update_state_scan, (grid, agents, shelves, 0), actions\n    )\n\n    # check for agent collisions\n    collisions = jax.vmap(functools.partial(utils.is_collision, grid))(agents, self.agent_ids)\n    collision = jnp.any(collisions)\n\n    # compute shared reward for all agents and update request queue\n    # if a requested shelf has been successfully delivered to the goal\n    reward = jnp.array(0, dtype=jnp.float32)\n\n    def update_reward_and_request_queue_scan(\n        carry_info: Tuple[chex.PRNGKey, chex.Array, chex.Array, chex.Array, chex.Array],\n        goal: chex.Array,\n    ) -&gt; Tuple[Tuple[chex.PRNGKey, chex.Array, chex.Array, chex.Array, chex.Array], None]:\n        key, reward, request_queue, grid, shelves = carry_info\n        (\n            key,\n            reward,\n            request_queue,\n            shelves,\n        ) = self._update_reward_and_request_queue(\n            key, reward, request_queue, grid, shelves, goal\n        )\n        carry_info = (key, reward, request_queue, grid, shelves)\n        return carry_info, None\n\n    update_info, _ = jax.lax.scan(\n        update_reward_and_request_queue_scan,\n        (key, reward, request_queue, grid, shelves),\n        self.goals,\n    )\n    key, reward, request_queue, grid, shelves = update_info\n\n    # construct timestep and check environment termination\n    steps = state.step_count + 1\n    horizon_reached = steps &gt;= self.time_limit\n    done = collision | horizon_reached\n\n    # compute next observation\n    agents_view = self._make_observations(grid, agents, shelves)\n    action_mask = utils.compute_action_mask(grid, agents)\n    next_observation = Observation(\n        agents_view=agents_view,\n        action_mask=action_mask,\n        step_count=steps,\n    )\n\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_observation,\n    )\n    next_state = State(\n        grid=grid,\n        agents=agents,\n        shelves=shelves,\n        request_queue=request_queue,\n        step_count=steps,\n        action_mask=action_mask,\n        key=key,\n    )\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/search_and_rescue/","title":"SearchAndRescue","text":"<p>               Bases: <code>Environment</code></p> <p>A multi-agent search environment</p> <p>Environment modelling a collection of agents collectively searching for a set of targets on a 2d environment. Agents are rewarded (individually) for coming within a fixed range of a target that has not already been detected. Agents visualise their local environment (i.e. the location of other agents and targets) via a simple segmented view model. The environment area is a uniform square space with wrapped boundaries.</p> <p>An episode will terminate if all targets have been located by the team of searching agents.</p> <ul> <li> <p>observation: <code>Observation</code>     searcher_views: jax array (float) of shape (num_searchers, channels, num_vision)         Individual local views of positions of other agents and targets, where         channels can be used to differentiate between agents and targets types.         Each entry in the view indicates the distance to another agent/target         along a ray from the agent, and is -1.0 if nothing is in range along the ray.         The view model can be customised by implementing the  <code>ObservationFn</code> interface.     targets_remaining: (float) Number of targets remaining to be found from         the total scaled to the range [0, 1] (i.e. a value of 1.0 indicates         all the targets are still to be found).     step: (int) current simulation step.     positions: jax array (float) of shape (num_searchers, 2) search agent positions.</p> </li> <li> <p>action: jax array (float) of shape (num_searchers, 2)     Array of individual agent actions. Each agents actions rotate and     accelerate/decelerate the agent as [rotation, acceleration] on the range     [-1, 1]. These values are then scaled to update agent velocities within     given parameters (i.e. a value of -+1 is the maximum acceleration/rotation).</p> </li> <li> <p>reward: jax array (float) of shape (num_searchers,)     Arrays of individual agent rewards. A reward of +1 is granted when an agent     comes into contact range with a target that has not yet been found, and     the target is within the searchers view cone. It is possible for multiple     agents to newly find the same target within a given step, by default     in this case the reward is split between the locating agents. By default,     rewards granted linearly decrease over time, with zero reward granted     at the environment time-limit. These defaults can be modified by flags     in <code>IndividualRewardFn</code>, or further customised by  implementing the <code>RewardFn</code>     interface.</p> </li> <li> <p>state: <code>State</code></p> <ul> <li>searchers: <code>AgentState</code><ul> <li>pos: jax array (float) of shape (num_searchers, 2) in the range [0, env_size].</li> <li>heading: jax array (float) of shape (num_searcher,) in     the range [0, 2\u03c0].</li> <li>speed: jax array (float) of shape (num_searchers,) in the     range [min_speed, max_speed].</li> </ul> </li> <li>targets: <code>TargetState</code><ul> <li>pos: jax array (float) of shape (num_targets, 2) in the range [0, env_size].</li> <li>vel:  jax array (float) of shape (num_targets, 2).</li> <li>found: jax array (bool) of shape (num_targets,) flag indicating if     target has been located by an agent.</li> </ul> </li> <li>key: jax array (uint32) of shape (2,)</li> <li>step: int representing the current simulation step.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import SearchAndRescue\n\nenv = SearchAndRescue()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>SearchAndRescue</code> environment</p> <p>Parameters:</p> Name Type Description Default <code>target_contact_range</code> <code>float</code> <p>Range at which a searchers will 'find' a target.</p> <code>0.02</code> <code>searcher_max_rotate</code> <code>float</code> <p>Maximum rotation searcher agents can turn within a step. Should be a value from [0,1] representing a fraction of \u03c0-radians.</p> <code>0.25</code> <code>searcher_max_accelerate</code> <code>float</code> <p>Magnitude of the maximum acceleration/deceleration a searcher agent can apply within a step.</p> <code>0.005</code> <code>searcher_min_speed</code> <code>float</code> <p>Minimum speed a searcher agent can move at.</p> <code>0.005</code> <code>searcher_max_speed</code> <code>float</code> <p>Maximum speed a searcher agent can move at.</p> <code>0.02</code> <code>time_limit</code> <code>int</code> <p>Maximum number of environment steps allowed for search.</p> <code>400</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>SearchAndRescueViewer</code>.</p> <code>None</code> <code>target_dynamics</code> <code>Optional[TargetDynamics]</code> <p>Target object dynamics model, implemented as a <code>TargetDynamics</code> interface. Defaults to <code>RandomWalk</code>.</p> <code>None</code> <code>generator</code> <code>Optional[Generator]</code> <p>Initial state <code>Generator</code> instance. Defaults to <code>RandomGenerator</code> with 50 targets and 2 searchers, with targets uniformly distributed across the environment.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>Reward aggregation function. Defaults to <code>IndividualRewardFn</code> where agents split rewards if they locate a target simultaneously, and rewards linearly decrease to zero over time.</p> <code>None</code> <code>observation</code> <code>Optional[ObservationFn]</code> <p>Agent observation view generation function. Defaults to <code>AgentAndTargetObservationFn</code> where all targets (found and unfound) and other searching agents are included in the generated view.</p> <code>None</code> Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def __init__(\n    self,\n    target_contact_range: float = 0.02,\n    searcher_max_rotate: float = 0.25,\n    searcher_max_accelerate: float = 0.005,\n    searcher_min_speed: float = 0.005,\n    searcher_max_speed: float = 0.02,\n    time_limit: int = 400,\n    viewer: Optional[Viewer[State]] = None,\n    target_dynamics: Optional[TargetDynamics] = None,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    observation: Optional[ObservationFn] = None,\n) -&gt; None:\n    \"\"\"Instantiates a `SearchAndRescue` environment\n\n    Args:\n        target_contact_range: Range at which a searchers will 'find' a target.\n        searcher_max_rotate: Maximum rotation searcher agents can\n            turn within a step. Should be a value from [0,1]\n            representing a fraction of \u03c0-radians.\n        searcher_max_accelerate: Magnitude of the maximum\n            acceleration/deceleration a searcher agent can apply within a step.\n        searcher_min_speed: Minimum speed a searcher agent can move at.\n        searcher_max_speed: Maximum speed a searcher agent can move at.\n        time_limit: Maximum number of environment steps allowed for search.\n        viewer: `Viewer` used for rendering. Defaults to `SearchAndRescueViewer`.\n        target_dynamics: Target object dynamics model, implemented as a\n            `TargetDynamics` interface. Defaults to `RandomWalk`.\n        generator: Initial state `Generator` instance. Defaults to `RandomGenerator`\n            with 50 targets and 2 searchers, with targets uniformly distributed\n            across the environment.\n        reward_fn: Reward aggregation function. Defaults to `IndividualRewardFn` where\n            agents split rewards if they locate a target simultaneously, and\n            rewards linearly decrease to zero over time.\n        observation: Agent observation view generation function. Defaults to\n            `AgentAndTargetObservationFn` where all targets (found and unfound)\n            and other searching agents are included in the generated view.\n    \"\"\"\n\n    self.target_contact_range = target_contact_range\n\n    self.searcher_params = AgentParams(\n        max_rotate=searcher_max_rotate,\n        max_accelerate=searcher_max_accelerate,\n        min_speed=searcher_min_speed,\n        max_speed=searcher_max_speed,\n    )\n    self.time_limit = time_limit\n    self._target_dynamics = target_dynamics or RandomWalk(acc_std=0.0001, vel_max=0.002)\n    self.generator = generator or RandomGenerator(num_targets=40, num_searchers=2)\n    self._viewer = viewer or SearchAndRescueViewer()\n    self._reward_fn = reward_fn or IndividualRewardFn()\n    self._observation_fn = observation or AgentAndTargetObservationFn(\n        num_vision=128,\n        searcher_vision_range=0.4,\n        target_vision_range=0.1,\n        view_angle=0.4,\n        agent_radius=target_contact_range,\n        env_size=self.generator.env_size,\n    )\n    super().__init__()\n</code></pre>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>2d array of individual agent actions. Each agents action is an array representing [rotation, acceleration] in the range [-1, 1].</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>BoundedArray</code> <p>Action array spec</p>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Local searcher agent views representing the distance to the closest neighbouring agents and targets in the environment.</p> <p>Returns:</p> Name Type Description <code>observation_spec</code> <code>Spec[Observation]</code> <p>Search-and-rescue observation spec</p>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.reward_spec","title":"<code>reward_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the reward spec.</p> <p>Array of individual rewards for each agent.</p> <p>Returns:</p> Name Type Description <code>reward_spec</code> <code>BoundedArray</code> <p>Reward array spec.</p>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.animate","title":"<code>animate(states, interval=100, save_path=None)</code>","text":"<p>Create an animation from a sequence of environment states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds.</p> <code>100</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>Animation that can be saved as a GIF, MP4, or rendered with HTML.</p> Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 100,\n    save_path: Optional[str] = None,\n) -&gt; FuncAnimation:\n    \"\"\"Create an animation from a sequence of environment states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive\n            timesteps.\n        interval: delay between frames in milliseconds.\n        save_path: the path where the animation file should be saved. If it\n            is None, the plot will not be saved.\n\n    Returns:\n        Animation that can be saved as a GIF, MP4, or rendered with HTML.\n    \"\"\"\n    return self._viewer.animate(states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.render","title":"<code>render(state)</code>","text":"<p>Render a frame of the environment for a given state using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object.</p> required Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def render(self, state: State) -&gt; None:\n    \"\"\"Render a frame of the environment for a given state using matplotlib.\n\n    Args:\n        state: State object.\n    \"\"\"\n    self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.reset","title":"<code>reset(key)</code>","text":"<p>Initialise searcher and target initial states.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>Random key used to reset the environment.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>Initial environment state.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep with individual search agent views.</p> Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Initialise searcher and target initial states.\n\n    Args:\n        key: Random key used to reset the environment.\n\n    Returns:\n        state: Initial environment state.\n        timestep: TimeStep with individual search agent views.\n    \"\"\"\n    state = self.generator(key, self.searcher_params)\n    timestep = restart(observation=self._state_to_observation(state), shape=(self.num_agents,))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/search_and_rescue/#jumanji.environments.swarms.search_and_rescue.env.SearchAndRescue.step","title":"<code>step(state, actions)</code>","text":"<p>Environment update.</p> <p>Update searcher velocities and consequently their positions, mark found targets, and generate rewards and local observations.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>Environment state.</p> required <code>actions</code> <code>Array</code> <p>2d array of searcher steering actions.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>Updated searcher and target positions and velocities.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>Transition timestep with individual agent local observations.</p> Source code in <code>jumanji/environments/swarms/search_and_rescue/env.py</code> <pre><code>def step(self, state: State, actions: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Environment update.\n\n    Update searcher velocities and consequently their positions,\n    mark found targets, and generate rewards and local observations.\n\n    Args:\n        state: Environment state.\n        actions: 2d array of searcher steering actions.\n\n    Returns:\n        state: Updated searcher and target positions and velocities.\n        timestep: Transition timestep with individual agent local observations.\n    \"\"\"\n    key, target_key = jax.random.split(state.key, num=2)\n    searchers = update_state(\n        self.generator.env_size, self.searcher_params, state.searchers, actions\n    )\n    targets = self._target_dynamics(target_key, state.targets, self.generator.env_size)\n\n    # Searchers return an array of flags of any targets they are in range of,\n    #  and that have not already been located, result shape here is (n-searcher, n-targets)\n    targets_found = spatial(\n        utils.searcher_detect_targets,\n        reduction=esquilax.reductions.logical_or((self.generator.num_targets,)),\n        i_range=self.target_contact_range,\n        dims=self.generator.env_size,\n    )(\n        self._observation_fn.view_angle,\n        searchers,\n        (jnp.arange(self.generator.num_targets), targets),\n        pos=searchers.pos,\n        pos_b=targets.pos,\n        env_size=self.generator.env_size,\n        n_targets=self.generator.num_targets,\n    )\n\n    rewards = self._reward_fn(targets_found, state.step, self.time_limit)\n\n    targets_found = jnp.any(targets_found, axis=0)\n    # Targets need to remain found if they already have been\n    targets_found = jnp.logical_or(targets_found, state.targets.found)\n\n    state = State(\n        searchers=searchers,\n        targets=TargetState(pos=targets.pos, vel=targets.vel, found=targets_found),\n        key=key,\n        step=state.step + 1,\n    )\n    observation = self._state_to_observation(state)\n    observation = jax.lax.stop_gradient(observation)\n    timestep = jax.lax.cond(\n        jnp.logical_or(state.step &gt;= self.time_limit, jnp.all(targets_found)),\n        partial(termination, shape=(self.num_agents,)),\n        partial(transition, shape=(self.num_agents,)),\n        rewards,\n        observation,\n    )\n    return state, timestep\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/","title":"SlidingTilePuzzle","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Environment for the Sliding Tile Puzzle problem.</p> <p>The problem is a combinatorial optimization task where the goal is to move the empty tile around in order to arrange all the tiles in order. See more info: https://en.wikipedia.org/wiki/Sliding_puzzle.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>puzzle: jax array (int32) of shape (N, N), representing the current state of the puzzle.</li> <li>empty_tile_position: Tuple of int32, representing the position of the empty tile.</li> <li>action_mask: jax array (bool) of shape (4,), indicating which actions are valid     in the current state of the environment.</li> </ul> </li> <li> <p>action: int32, representing the direction to move the empty tile     (up, down, left, right)</p> </li> <li> <p>reward: float, a dense reward is provided based on the arrangement of the tiles.     It equals the negative sum of the boolean difference between     the current state of the puzzle and the goal state (correctly arranged tiles).     Each incorrectly placed tile contributes -1 to the reward.</p> </li> <li> <p>episode termination: if the puzzle is solved.</p> </li> <li> <p>state: <code>State</code></p> <ul> <li>puzzle: jax array (int32) of shape (N, N), representing the current state of the puzzle.</li> <li>empty_tile_position: Tuple of int32, representing the position of the empty tile.</li> <li>key: jax array (uint32) of shape (2,), random key used to generate random numbers     at each step and for auto-reset.</li> </ul> </li> </ul> <p>Instantiate a <code>SlidingTilePuzzle</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p>callable to instantiate environment instances. Defaults to <code>RandomWalkGenerator</code> which generates shuffled puzzles with a size of 5x5.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>RewardFn whose <code>__call__</code> method computes the reward of an environment transition. The function must compute the reward based on the current state, the chosen action and the next state. Implemented options are [<code>DenseRewardFn</code>, <code>SparseRewardFn</code>]. Defaults to <code>DenseRewardFn</code>.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>maximum number of steps before the episode is terminated, default to 500.</p> <code>500</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p>environment viewer for rendering.</p> <code>None</code> Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    time_limit: int = 500,\n    viewer: Optional[Viewer[State]] = None,\n) -&gt; None:\n    \"\"\"Instantiate a `SlidingTilePuzzle` environment.\n\n    Args:\n        generator: callable to instantiate environment instances.\n            Defaults to `RandomWalkGenerator` which generates shuffled puzzles with\n            a size of 5x5.\n        reward_fn: RewardFn whose `__call__` method computes the reward of an environment\n            transition. The function must compute the reward based on the current state,\n            the chosen action and the next state.\n            Implemented options are [`DenseRewardFn`, `SparseRewardFn`].\n            Defaults to `DenseRewardFn`.\n        time_limit: maximum number of steps before the episode is terminated, default to 500.\n        viewer: environment viewer for rendering.\n    \"\"\"\n    self.generator = generator or RandomWalkGenerator(grid_size=5, num_random_moves=200)\n    self.reward_fn = reward_fn or DenseRewardFn()\n    self.time_limit = time_limit\n    super().__init__()\n\n    # Create viewer used for rendering\n    self._env_viewer = viewer or SlidingTilePuzzleViewer(name=\"SlidingTilePuzzle\")\n    self.solved_puzzle = self.generator.make_solved_puzzle()\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the puzzle board based on the sequence of game states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>is a list of <code>State</code> objects representing the sequence of game states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the puzzle board based on the sequence of game states.\n\n    Args:\n        states: is a list of `State` objects representing the sequence of game states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n        will not be stored.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._env_viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._env_viewer.close()\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the puzzle board.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>is the current game state to be rendered.</p> required Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Renders the current state of the puzzle board.\n\n    Args:\n        state: is the current game state to be rendered.\n    \"\"\"\n    return self._env_viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment to an initial state.</p> Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment to an initial state.\"\"\"\n    key, subkey = jax.random.split(key)\n    state = self.generator(subkey)\n    action_mask = self._get_valid_actions(state.empty_tile_position)\n    obs = Observation(\n        puzzle=state.puzzle,\n        empty_tile_position=state.empty_tile_position,\n        action_mask=action_mask,\n        step_count=state.step_count,\n    )\n    timestep = restart(observation=obs, extras=self._get_extras(state))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/sliding_tile_puzzle/#jumanji.environments.logic.sliding_tile_puzzle.env.SlidingTilePuzzle.step","title":"<code>step(state, action)</code>","text":"<p>Updates the environment state after the agent takes an action.</p> Source code in <code>jumanji/environments/logic/sliding_tile_puzzle/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Updates the environment state after the agent takes an action.\"\"\"\n    (updated_puzzle, updated_empty_tile_position) = self._move_empty_tile(\n        state.puzzle, state.empty_tile_position, action\n    )\n    # Check if the puzzle is solved\n    done = jnp.array_equal(updated_puzzle, self.solved_puzzle)\n\n    # Update the action mask\n    action_mask = self._get_valid_actions(updated_empty_tile_position)\n\n    next_state = State(\n        puzzle=updated_puzzle,\n        empty_tile_position=updated_empty_tile_position,\n        key=state.key,\n        step_count=state.step_count + 1,\n    )\n    obs = Observation(\n        puzzle=updated_puzzle,\n        empty_tile_position=updated_empty_tile_position,\n        action_mask=action_mask,\n        step_count=next_state.step_count,\n    )\n\n    reward = self.reward_fn(state, action, next_state, self.solved_puzzle)\n    extras = self._get_extras(next_state)\n\n    timestep = jax.lax.cond(\n        done | (next_state.step_count &gt;= self.time_limit),\n        lambda: termination(\n            reward=reward,\n            observation=obs,\n            extras=extras,\n        ),\n        lambda: transition(\n            reward=reward,\n            observation=obs,\n            extras=extras,\n        ),\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/snake/","title":"Snake","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>A JAX implementation of the 'Snake' game.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>grid: jax array (float) of shape (num_rows, num_cols, 5)     feature maps that include information about the fruit, the snake head, its body and     tail.<ul> <li>body: 2D map with 1. where a body cell is present, else 0.</li> <li>head: 2D map with 1. where the snake's head is located, else 0.</li> <li>tail: 2D map with 1. where the snake's tail is located, else 0.</li> <li>fruit: 2D map with 1. where the fruit is located, else 0.</li> <li>norm_body_state: 2D map with a float between 0. and 1. for each body cell in the     decreasing order from head to tail.</li> </ul> </li> <li>step_count: jax array (int32) of shape ()     current number of steps in the episode.</li> <li>action_mask: jax array (bool) of shape (4,)     array specifying which directions the snake can move in from its current position.</li> </ul> </li> <li> <p>action: jax array (int32) of shape()     [0,1,2,3] -&gt; [Up, Right, Down, Left].</p> </li> <li> <p>reward: jax array (float) of shape ()     1.0 if a fruit is eaten, otherwise 0.0.</p> </li> <li> <p>episode termination:</p> <ul> <li>if no action can be performed, i.e. the snake is surrounded.</li> <li>if the time limit is reached.</li> <li>if an invalid action is taken, the snake exits the grid or bumps into itself.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>body: jax array (bool) of shape (num_rows, num_cols)     array indicating the snake's body cells.</li> <li>body_state: jax array (int32) of shape (num_rows, num_cols)     array ordering the snake's body cells, in decreasing order from head to tail.</li> <li>head_position: <code>Position</code> (int32) of shape ()     position of the snake's head on the 2D grid.</li> <li>tail: jax array (bool) of shape (num_rows, num_cols)     array indicating the snake's tail.</li> <li>fruit_position: <code>Position</code> (int32) of shape ()     position of the fruit on the 2D grid.</li> <li>length: jax array (int32) of shape ()     current length of the snake.</li> <li>step_count: jax array (int32) of shape ()     current number of steps in the episode.</li> <li>action_mask: jax array (bool) of shape (4,)     array specifying which directions the snake can move in from its current position.</li> <li>key: jax array (uint32) of shape (2,)     random key used to sample a new fruit when one is eaten and used for auto-reset.</li> </ul> </li> </ul> <pre><code>from jumanji.environments import Snake\nenv = Snake()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>Snake</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>number of rows of the 2D grid. Defaults to 12.</p> <code>12</code> <code>num_cols</code> <code>int</code> <p>number of columns of the 2D grid. Defaults to 12.</p> <code>12</code> <code>time_limit</code> <code>int</code> <p>time_limit of an episode, i.e. number of environment steps before the episode ends. Defaults to 4000.</p> <code>4000</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>SnakeViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def __init__(\n    self,\n    num_rows: int = 12,\n    num_cols: int = 12,\n    time_limit: int = 4000,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates a `Snake` environment.\n\n    Args:\n        num_rows: number of rows of the 2D grid. Defaults to 12.\n        num_cols: number of columns of the 2D grid. Defaults to 12.\n        time_limit: time_limit of an episode, i.e. number of environment steps before\n            the episode ends. Defaults to 4000.\n        viewer: `Viewer` used for rendering. Defaults to `SnakeViewer`.\n    \"\"\"\n    self.num_rows = num_rows\n    self.num_cols = num_cols\n    self.board_shape = (num_rows, num_cols)\n    self.time_limit = time_limit\n    super().__init__()\n    self._viewer = viewer or SnakeViewer()\n</code></pre>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. 4 actions: [0,1,2,3] -&gt; [Up, Right, Down, Left].</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>a <code>specs.DiscreteArray</code> spec.</p>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>grid: BoundedArray (float) of shape (num_rows, num_cols, 5).</li> </ul> <code>Spec[Observation]</code> <ul> <li>step_count: DiscreteArray (num_values = time_limit) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (4,).</li> </ul>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Create an animation from a sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of <code>State</code> corresponding to subsequent timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>Animation object that can be saved as a GIF, MP4, or rendered with HTML.</p> Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Create an animation from a sequence of states.\n\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        Animation object that can be saved as a GIF, MP4, or rendered with HTML.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.render","title":"<code>render(state)</code>","text":"<p>Render frames of the environment for a given state using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>State object containing the current dynamics of the environment.</p> required Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def render(self, state: State) -&gt; None:\n    \"\"\"Render frames of the environment for a given state using matplotlib.\n\n    Args:\n        state: State object containing the current dynamics of the environment.\n    \"\"\"\n    self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to sample the snake and fruit positions.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> object corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: random key used to sample the snake and fruit positions.\n\n    Returns:\n         state: `State` object corresponding to the new state of the environment.\n         timestep: `TimeStep` object corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n    key, snake_key, fruit_key = jax.random.split(key, 3)\n    # Sample Snake's head position.\n    head_coordinates = jax.random.randint(\n        snake_key,\n        shape=(2,),\n        minval=jnp.zeros(2, int),\n        maxval=jnp.array(self.board_shape),\n    )\n    head_position = Position(*tuple(head_coordinates))\n\n    body = jnp.zeros(self.board_shape, bool).at[tuple(head_position)].set(True)\n    tail = body\n    body_state = body.astype(jnp.int32)\n    fruit_position = self._sample_fruit_coord(body, fruit_key)\n    state = State(\n        key=key,\n        body=body,\n        body_state=body_state,\n        head_position=head_position,\n        tail=tail,\n        fruit_position=fruit_position,\n        length=jnp.array(1, jnp.int32),\n        step_count=jnp.array(0, jnp.int32),\n        action_mask=self._get_action_mask(head_position, body_state),\n    )\n    timestep = restart(observation=self._state_to_observation(state))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/snake/#jumanji.environments.routing.snake.env.Snake.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Numeric</code> <p>Array containing the action to take: - 0: move up. - 1: move to the right. - 2: move down. - 3: move to the left.</p> required <p>Returns:</p> Type Description <code>Tuple[State, TimeStep[Observation]]</code> <p>state, timestep: next state of the environment and timestep to be observed.</p> Source code in <code>jumanji/environments/routing/snake/env.py</code> <pre><code>def step(self, state: State, action: chex.Numeric) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: Array containing the action to take:\n            - 0: move up.\n            - 1: move to the right.\n            - 2: move down.\n            - 3: move to the left.\n\n    Returns:\n        state, timestep: next state of the environment and timestep to be observed.\n    \"\"\"\n    is_valid = state.action_mask[action]\n    key, fruit_key = jax.random.split(state.key)\n\n    head_position = self._update_head_position(state.head_position, action)\n\n    fruit_eaten = head_position == state.fruit_position\n\n    length = state.length + fruit_eaten\n\n    body_state_without_head = jax.lax.select(\n        fruit_eaten,\n        state.body_state,\n        jnp.clip(state.body_state - 1, 0),\n    )\n    body_state = body_state_without_head.at[tuple(head_position)].set(length)\n\n    body = body_state &gt; 0\n\n    tail = body_state == 1\n\n    fruit_position = jax.lax.cond(\n        fruit_eaten,\n        self._sample_fruit_coord,\n        lambda *_: state.fruit_position,\n        body,\n        fruit_key,\n    )\n    step_count = state.step_count + 1\n    next_state = State(\n        key=key,\n        body=body,\n        body_state=body_state,\n        head_position=head_position,\n        tail=tail,\n        fruit_position=fruit_position,\n        length=length,\n        step_count=state.step_count + 1,\n        action_mask=self._get_action_mask(head_position, body_state),\n    )\n\n    snake_completed = jnp.all(body)\n    done = ~is_valid | snake_completed | (step_count &gt;= self.time_limit)\n\n    reward = jnp.asarray(fruit_eaten, float)\n    observation = self._state_to_observation(next_state)\n\n    timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/sokoban/","title":"Sokoban","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>A JAX implementation of the 'Sokoban' game from deepmind.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>grid: jax array (uint8) of shape (num_rows, num_cols, 2)     Array that includes information about the agent, boxes, and     targets in the game.</li> <li>step_count: jax array (int32) of shape ()     current number of steps in the episode.</li> </ul> </li> <li> <p>action: jax array (int32) of shape ()     [0,1,2,3] -&gt; [Up, Right, Down, Left].</p> </li> <li> <p>reward: jax array (float) of shape ()     A reward of 1.0 is given for each box placed on a target and -1     when removed from a target and -0.1 for each timestep.     10 is awarded when all boxes are on targets.</p> </li> <li> <p>episode termination:</p> <ul> <li>if the time limit is reached.</li> <li>if all boxes are on targets.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>key: jax array (uint32) of shape (2,) used for auto-reset</li> <li>fixed_grid: jax array (uint8) of shape (num_rows, num_cols)     array indicating the walls and targets in the level.</li> <li>variable_grid: jax array (uint8) of shape (num_rows, num_cols)     array indicating the current location of the agent and boxes.</li> <li>agent_location: jax array (int32) of shape (2,)     the agent's current location.</li> <li>step_count: jax array (int32) of shape ()     current number of steps in the episode.</li> </ul> </li> </ul> <p><pre><code>from jumanji.environments import Sokoban\nfrom jumanji.environments.routing.sokoban.generator import\nHuggingFaceDeepMindGenerator,\n\nenv_train = Sokoban(\n    generator=HuggingFaceDeepMindGenerator(\n        dataset_name=\"unfiltered-train\",\n        proportion_of_files=1,\n    )\n)\n\nenv_test = Sokoban(\n    generator=HuggingFaceDeepMindGenerator(\n        dataset_name=\"unfiltered-test\",\n        proportion_of_files=1,\n    )\n)\n\n# Train...\n</code></pre> key_train = jax.random.PRNGKey(0) state, timestep = jax.jit(env_train.reset)(key_train) env_train.render(state) action = env_train.action_spec.generate_value() state, timestep = jax.jit(env_train.step)(state, action) env_train.render(state) ```</p> <p>Instantiates a <code>Sokoban</code> environment with a specific generator, time limit, and viewer.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance (an initial state). Implemented options are [<code>ToyGenerator</code>, <code>DeepMindGenerator</code>, and <code>HuggingFaceDeepMindGenerator</code>]. Defaults to <code>HuggingFaceDeepMindGenerator</code> with <code>dataset_name=\"unfiltered-train\", proportion_of_files=1</code>.</p> <code>None</code> <code>time_limit</code> <code>int</code> <p>int, max steps for the environment, defaults to 120.</p> <code>120</code> <code>viewer</code> <code>Optional[Viewer]</code> <p>'Viewer' object, used to render the environment.</p> <code>None</code> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer] = None,\n    time_limit: int = 120,\n) -&gt; None:\n    \"\"\"\n    Instantiates a `Sokoban` environment with a specific generator,\n    time limit, and viewer.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment\n         instance (an initial state). Implemented options are [`ToyGenerator`,\n         `DeepMindGenerator`, and `HuggingFaceDeepMindGenerator`].\n         Defaults to `HuggingFaceDeepMindGenerator` with\n         `dataset_name=\"unfiltered-train\", proportion_of_files=1`.\n        time_limit: int, max steps for the environment, defaults to 120.\n        viewer: 'Viewer' object, used to render the environment.\n        If not provided, defaults to`BoxViewer`.\n    \"\"\"\n\n    self.num_rows = GRID_SIZE\n    self.num_cols = GRID_SIZE\n    self.shape = (self.num_rows, self.num_cols)\n    self.time_limit = time_limit\n\n    super().__init__()\n\n    self.generator = generator or HuggingFaceDeepMindGenerator(\n        \"unfiltered-train\",\n        proportion_of_files=1,\n    )\n\n    self._viewer = viewer or BoxViewer(\n        name=\"Sokoban\",\n        grid_combine=self.grid_combine,\n    )\n    self.reward_fn = reward_fn or DenseReward()\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action specification for the Sokoban environment. There are 4 actions: [0,1,2,3] -&gt; [Up, Right, Down, Left].</p> <p>Returns:</p> Type Description <code>DiscreteArray</code> <p>specs.DiscreteArray: Discrete action specifications.</p>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the specifications of the observation of the <code>Sokoban</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>specs.Spec[Observation]: The specifications of the observations.</p>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns a printable representation of the Sokoban environment.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string representation of the Sokoban environment.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Returns a printable representation of the Sokoban environment.\n\n    Returns:\n        str: A string representation of the Sokoban environment.\n    \"\"\"\n    return \"\\n\".join(\n        [\n            \"Bokoban environment:\",\n            f\" - num_rows: {self.num_rows}\",\n            f\" - num_cols: {self.num_cols}\",\n            f\" - time_limit: {self.time_limit}\",\n            f\" - generator: {self.generator}\",\n        ]\n    )\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the Sokoban environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>Sequence of 'State' object</p> required <code>interval</code> <code>int</code> <p>int, The interval between frames in the animation.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>str The path where to save the animation. If not</p> <code>None</code> <p>Returns:</p> Name Type Description <code>animation</code> <code>FuncAnimation</code> <p>'matplotlib.animation.FuncAnimation'.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"\n    Creates an animated gif of the Sokoban environment based on the\n    sequence of states.\n\n    Args:\n        states: Sequence of 'State' object\n        interval: int, The interval between frames in the animation.\n        Defaults to 200.\n        save_path: str The path where to save the animation. If not\n        provided, the animation is not saved.\n\n    Returns:\n        animation: 'matplotlib.animation.FuncAnimation'.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.check_space","title":"<code>check_space(grid, location, value)</code>","text":"<p>Checks if a specific location in the grid contains a given value.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols) The grid to check.</p> required <code>location</code> <code>Array</code> <p>Tuple size 2 of Array (int32) shape () containing the x</p> required <code>value</code> <code>int</code> <p>int The value to look for.</p> required <p>Returns:</p> Name Type Description <code>present</code> <code>Array</code> <p>Array (bool) shape () indicating whether the location</p> <code>Array</code> <p>in the grid contains the given value or not.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def check_space(\n    self,\n    grid: chex.Array,\n    location: chex.Array,\n    value: int,\n) -&gt; chex.Array:\n    \"\"\"\n    Checks if a specific location in the grid contains a given value.\n\n    Args:\n        grid: Array (uint8) shape (num_rows, num_cols) The grid to check.\n        location: Tuple size 2 of Array (int32) shape () containing the x\n        and y coodinate of the location to check in the grid.\n        value: int The value to look for.\n\n    Returns:\n        present: Array (bool) shape () indicating whether the location\n        in the grid contains the given value or not.\n    \"\"\"\n\n    return grid[tuple(location)] == value\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.detect_noop_action","title":"<code>detect_noop_action(variable_grid, fixed_grid, action, agent_location)</code>","text":"<p>Masks actions to -1 that have no effect on the variable grid. Determines if there is space in the destination square or if there is a box in the destination square, it determines if the box destination square is valid.</p> <p>Parameters:</p> Name Type Description Default <code>variable_grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols).</p> required <code>action</code> <code>Array</code> <p>Array (int32) shape () The action to check.</p> required <p>Returns:</p> Name Type Description <code>updated_action</code> <code>Array</code> <p>Array (int32) shape () The updated action after</p> <code>Array</code> <p>detecting noop action.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def detect_noop_action(\n    self,\n    variable_grid: chex.Array,\n    fixed_grid: chex.Array,\n    action: chex.Array,\n    agent_location: chex.Array,\n) -&gt; chex.Array:\n    \"\"\"\n    Masks actions to -1 that have no effect on the variable grid.\n    Determines if there is space in the destination square or if\n    there is a box in the destination square, it determines if the box\n    destination square is valid.\n\n    Args:\n        variable_grid: Array (uint8) shape (num_rows, num_cols).\n        fixed_grid Array (uint8) shape (num_rows, num_cols) .\n        action: Array (int32) shape () The action to check.\n\n    Returns:\n        updated_action: Array (int32) shape () The updated action after\n        detecting noop action.\n    \"\"\"\n\n    new_location = agent_location + MOVES[action].squeeze()\n\n    valid_destination = self.check_space(fixed_grid, new_location, WALL) | ~self.in_grid(\n        new_location\n    )\n\n    updated_action = jax.lax.select(\n        valid_destination,\n        jnp.full(shape=(), fill_value=NOOP, dtype=jnp.int32),\n        jax.lax.select(\n            self.check_space(variable_grid, new_location, BOX),\n            self.update_box_push_action(\n                fixed_grid,\n                variable_grid,\n                new_location,\n                action,\n            ),\n            action,\n        ),\n    )\n\n    return updated_action\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.grid_combine","title":"<code>grid_combine(variable_grid, fixed_grid)</code>","text":"<p>Combines the variable grid and fixed grid into one single grid representation of the current Sokoban state required for visual representation of the Sokoban state. Takes care of two possible overlaps of fixed and variable entries (an agent on a target or a box on a target), introducing two additional encodings.</p> <p>Parameters:</p> Name Type Description Default <code>variable_grid</code> <code>Array</code> <p>Array (uint8) of shape (num_rows, num_cols).</p> required <code>fixed_grid</code> <code>Array</code> <p>Array (uint8) of shape (num_rows, num_cols).</p> required <p>Returns:</p> Name Type Description <code>full_grid</code> <code>Array</code> <p>Array (uint8) of shape (num_rows, num_cols, 2).</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def grid_combine(self, variable_grid: chex.Array, fixed_grid: chex.Array) -&gt; chex.Array:\n    \"\"\"\n    Combines the variable grid and fixed grid into one single grid\n    representation of the current Sokoban state required for visual\n    representation of the Sokoban state. Takes care of two possible\n    overlaps of fixed and variable entries (an agent on a target or a box\n    on a target), introducing two additional encodings.\n\n    Args:\n        variable_grid: Array (uint8) of shape (num_rows, num_cols).\n        fixed_grid: Array (uint8) of shape (num_rows, num_cols).\n\n    Returns:\n        full_grid: Array (uint8) of shape (num_rows, num_cols, 2).\n    \"\"\"\n\n    mask_target_agent = jnp.logical_and(\n        fixed_grid == TARGET,\n        variable_grid == AGENT,\n    )\n\n    mask_target_box = jnp.logical_and(\n        fixed_grid == TARGET,\n        variable_grid == BOX,\n    )\n\n    single_grid = jnp.where(\n        mask_target_agent,\n        TARGET_AGENT,\n        jnp.where(\n            mask_target_box,\n            TARGET_BOX,\n            jnp.maximum(variable_grid, fixed_grid),\n        ),\n    ).astype(jnp.uint8)\n\n    return single_grid\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.in_grid","title":"<code>in_grid(coordinates)</code>","text":"<p>Checks if given coordinates are within the grid size.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols) The</p> required <p>Returns:     in_grid: Array (bool) shape () Boolean indicating whether the     coordinates are within the grid.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def in_grid(self, coordinates: chex.Array) -&gt; chex.Array:\n    \"\"\"\n    Checks if given coordinates are within the grid size.\n\n    Args:\n        coordinates: Array (uint8) shape (num_rows, num_cols) The\n        coordinates to check.\n    Returns:\n        in_grid: Array (bool) shape () Boolean indicating whether the\n        coordinates are within the grid.\n    \"\"\"\n    return jnp.all((0 &lt;= coordinates) &amp; (coordinates &lt; GRID_SIZE))\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.level_complete","title":"<code>level_complete(state)</code>","text":"<p>Checks if the sokoban level is complete.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object representing the current state of the environment.</p> required <p>Returns:</p> Name Type Description <code>complete</code> <code>Array</code> <p>Boolean indicating whether the level is complete</p> <code>Array</code> <p>or not.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def level_complete(self, state: State) -&gt; chex.Array:\n    \"\"\"\n    Checks if the sokoban level is complete.\n\n    Args:\n        state: `State` object representing the current state of the environment.\n\n    Returns:\n        complete: Boolean indicating whether the level is complete\n        or not.\n    \"\"\"\n    return self.reward_fn.count_targets(state) == N_BOXES\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.move_agent","title":"<code>move_agent(variable_grid, action, current_location)</code>","text":"<p>Executes the movement of the agent specified by the action and executes the movement of a box if present at the destination.</p> <p>Parameters:</p> Name Type Description Default <code>variable_grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols)</p> required <code>action</code> <code>Array</code> <p>Array (int32) shape () The action to take.</p> required <code>current_location</code> <code>Array</code> <p>Array (int32) shape (2,)</p> required <p>Returns:</p> Name Type Description <code>next_variable_grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols)</p> <code>next_location</code> <code>Array</code> <p>Array (int32) shape (2,)</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def move_agent(\n    self,\n    variable_grid: chex.Array,\n    action: chex.Array,\n    current_location: chex.Array,\n) -&gt; Tuple[chex.Array, chex.Array]:\n    \"\"\"\n    Executes the movement of the agent specified by the action and\n    executes the movement of a box if present at the destination.\n\n    Args:\n        variable_grid: Array (uint8) shape (num_rows, num_cols)\n        action: Array (int32) shape () The action to take.\n        current_location: Array (int32) shape (2,)\n\n    Returns:\n        next_variable_grid: Array (uint8) shape (num_rows, num_cols)\n        next_location: Array (int32) shape (2,)\n    \"\"\"\n\n    next_location = current_location + MOVES[action]\n    box_location = next_location + MOVES[action]\n\n    # remove agent from current location\n    next_variable_grid = variable_grid.at[tuple(current_location)].set(EMPTY)\n\n    # either move agent or move agent and box\n\n    next_variable_grid = jax.lax.select(\n        self.check_space(variable_grid, next_location, BOX),\n        next_variable_grid.at[tuple(next_location)].set(AGENT).at[tuple(box_location)].set(BOX),\n        next_variable_grid.at[tuple(next_location)].set(AGENT),\n    )\n\n    return next_variable_grid, next_location\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of Sokoban.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'State' object , the current state to be rendered.</p> required Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def render(self, state: State) -&gt; None:\n    \"\"\"\n    Renders the current state of Sokoban.\n\n    Args:\n        state: 'State' object , the current state to be rendered.\n    \"\"\"\n\n    self._viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment by calling the instance generator for a new instance.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>random key used to sample new Sokoban problem.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> object corresponding to the new state of the</p> <code>TimeStep[Observation]</code> <p>environment after a reset.</p> <code>timestep</code> <code>Tuple[State, TimeStep[Observation]]</code> <p><code>TimeStep</code> object corresponding the first timestep</p> <code>Tuple[State, TimeStep[Observation]]</code> <p>returned by the environment after a reset.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"\n    Resets the environment by calling the instance generator for a\n    new instance.\n\n    Args:\n        key: random key used to sample new Sokoban problem.\n\n    Returns:\n        state: `State` object corresponding to the new state of the\n        environment after a reset.\n        timestep: `TimeStep` object corresponding the first timestep\n        returned by the environment after a reset.\n    \"\"\"\n\n    generator_key, key = jax.random.split(key)\n\n    state = self.generator(generator_key)\n\n    timestep = restart(\n        self._state_to_observation(state),\n        extras=self._get_extras(state),\n    )\n\n    return state, timestep\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.step","title":"<code>step(state, action)</code>","text":"<p>Executes one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>'State' object representing the current state of the</p> required <code>action</code> <code>Array</code> <p>Array (int32) of shape (). - 0: move up. - 1: move down. - 2: move left. - 3: move right.</p> required <p>Returns:</p> Type Description <code>State</code> <p>state, timestep: next state of the environment and timestep to be</p> <code>TimeStep[Observation]</code> <p>observed.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"\n    Executes one timestep of the environment's dynamics.\n\n    Args:\n        state: 'State' object representing the current state of the\n        environment.\n        action: Array (int32) of shape ().\n            - 0: move up.\n            - 1: move down.\n            - 2: move left.\n            - 3: move right.\n\n    Returns:\n        state, timestep: next state of the environment and timestep to be\n        observed.\n    \"\"\"\n\n    # switch to noop if action will have no impact on variable grid\n    action = self.detect_noop_action(\n        state.variable_grid, state.fixed_grid, action, state.agent_location\n    )\n\n    next_variable_grid, next_agent_location = jax.lax.cond(\n        jnp.all(action == NOOP),\n        lambda: (state.variable_grid, state.agent_location),\n        lambda: self.move_agent(state.variable_grid, action, state.agent_location),\n    )\n\n    next_state = State(\n        key=state.key,\n        fixed_grid=state.fixed_grid,\n        variable_grid=next_variable_grid,\n        agent_location=next_agent_location,\n        step_count=state.step_count + 1,\n    )\n\n    target_reached = self.level_complete(next_state)\n    time_limit_exceeded = next_state.step_count &gt;= self.time_limit\n\n    done = jnp.logical_or(target_reached, time_limit_exceeded)\n\n    reward = jnp.asarray(self.reward_fn(state, action, next_state), float)\n\n    observation = self._state_to_observation(next_state)\n\n    extras = self._get_extras(next_state)\n\n    timestep = jax.lax.cond(\n        done,\n        lambda: termination(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n        lambda: transition(\n            reward=reward,\n            observation=observation,\n            extras=extras,\n        ),\n    )\n\n    return next_state, timestep\n</code></pre>"},{"location":"api/environments/sokoban/#jumanji.environments.routing.sokoban.env.Sokoban.update_box_push_action","title":"<code>update_box_push_action(fixed_grid, variable_grid, new_location, action)</code>","text":"<p>Masks actions to -1 if pushing the box is not a valid move. If it would be pushed out of the grid or the resulting square is either a wall or another box.</p> <p>Parameters:</p> Name Type Description Default <code>fixed_grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols) The fixed grid.</p> required <code>variable_grid</code> <code>Array</code> <p>Array (uint8) shape (num_rows, num_cols) The</p> required <code>new_location</code> <code>Array</code> <p>Array (int32) shape (2,) The new location of the agent.</p> required <code>action</code> <code>Array</code> <p>Array (int32) shape () The action to be executed.</p> required <p>Returns:</p> Name Type Description <code>updated_action</code> <code>Array</code> <p>Array (int32) shape () The updated action after</p> <code>Array</code> <p>checking if pushing the box is a valid move.</p> Source code in <code>jumanji/environments/routing/sokoban/env.py</code> <pre><code>def update_box_push_action(\n    self,\n    fixed_grid: chex.Array,\n    variable_grid: chex.Array,\n    new_location: chex.Array,\n    action: chex.Array,\n) -&gt; chex.Array:\n    \"\"\"\n    Masks actions to -1 if pushing the box is not a valid move. If it\n    would be pushed out of the grid or the resulting square\n    is either a wall or another box.\n\n    Args:\n        fixed_grid: Array (uint8) shape (num_rows, num_cols) The fixed grid.\n        variable_grid: Array (uint8) shape (num_rows, num_cols) The\n        variable grid.\n        new_location: Array (int32) shape (2,) The new location of the agent.\n        action: Array (int32) shape () The action to be executed.\n\n    Returns:\n        updated_action: Array (int32) shape () The updated action after\n        checking if pushing the box is a valid move.\n    \"\"\"\n\n    return jax.lax.select(\n        self.check_space(\n            variable_grid,\n            new_location + MOVES[action].squeeze(),\n            BOX,\n        )\n        | ~self.in_grid(new_location + MOVES[action].squeeze()),\n        jnp.full(shape=(), fill_value=NOOP, dtype=jnp.int32),\n        jax.lax.select(\n            self.check_space(\n                fixed_grid,\n                new_location + MOVES[action].squeeze(),\n                WALL,\n            ),\n            jnp.full(shape=(), fill_value=NOOP, dtype=jnp.int32),\n            action,\n        ),\n    )\n</code></pre>"},{"location":"api/environments/sudoku/","title":"Sudoku","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>A JAX implementation of the sudoku game.</p> <ul> <li> <p>observation: <code>Observation</code></p> <ul> <li>board: jax array (int32) of shape (9,9):     empty cells are represented by -1, and filled cells are represented by 0-8.</li> <li>action_mask: jax array (bool) of shape (9,9,9):     indicates which actions are valid.</li> </ul> </li> <li> <p>action:     multi discrete array containing the square to write a digit, and the digits     to input.</p> </li> <li> <p>reward: jax array (float32):     1 at the end of the episode if the board is valid     0 otherwise</p> </li> <li> <p>state: <code>State</code></p> <ul> <li> <p>board: jax array (int32) of shape (9,9):     empty cells are represented by -1, and filled cells are represented by 0-8.</p> </li> <li> <p>action_mask: jax array (bool) of shape (9,9,9):     indicates which actions are valid (empty cells and valid digits).</p> </li> <li> <p>key: jax array (int32) of shape (2,) used for seeding initial sudoku     configuration.</p> </li> </ul> </li> </ul> <pre><code>from jumanji.environments import Sudoku\nenv = Sudoku()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> Source code in <code>jumanji/environments/logic/sudoku/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    super().__init__()\n    if generator is None:\n        file_path = os.path.dirname(os.path.abspath(__file__))\n        database_file = DATABASES[\"mixed\"]\n        database = jnp.load(os.path.join(file_path, \"data\", database_file))\n\n    self._generator = generator or DatabaseGenerator(database=database)\n    self._reward_fn = reward_fn or SparseRewardFn()\n    self._viewer = viewer or SudokuViewer()\n</code></pre>"},{"location":"api/environments/sudoku/#jumanji.environments.logic.sudoku.env.Sudoku.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. An action is composed of 3 integers: the row index, the column index and the value to be placed in the cell.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>MultiDiscreteArray</code> <p><code>MultiDiscreteArray</code> object.</p>"},{"location":"api/environments/sudoku/#jumanji.environments.logic.sudoku.env.Sudoku.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec containing the board and action_mask arrays.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing all the specifications for all the <code>Observation</code> fields: - board: BoundedArray (jnp.int8) of shape (9,9). - action_mask: BoundedArray (bool) of shape (9,9,9).</p>"},{"location":"api/environments/sudoku/#jumanji.environments.logic.sudoku.env.Sudoku.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the board based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>a list of <code>State</code> objects representing the sequence of states.</p> required <code>interval</code> <code>int</code> <p>the delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/logic/sudoku/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the board based on the sequence of states.\n\n    Args:\n        states: a list of `State` objects representing the sequence of states.\n        interval: the delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states=states, interval=interval, save_path=save_path)\n</code></pre>"},{"location":"api/environments/sudoku/#jumanji.environments.logic.sudoku.env.Sudoku.render","title":"<code>render(state)</code>","text":"<p>Renders the current state of the sudoku.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>the current state to be rendered.</p> required Source code in <code>jumanji/environments/logic/sudoku/env.py</code> <pre><code>def render(self, state: State) -&gt; Any:\n    \"\"\"Renders the current state of the sudoku.\n\n    Args:\n        state: the current state to be rendered.\n    \"\"\"\n    return self._viewer.render(state=state)\n</code></pre>"},{"location":"api/environments/tetris/","title":"Tetris","text":"<p>               Bases: <code>Environment[State, MultiDiscreteArray, Observation]</code></p> <p>RL Environment for the game of Tetris. The environment has a grid where the player can place tetrominoes. The environment has the following characteristics:</p> <ul> <li>observation: <code>Observation</code><ul> <li>grid: jax array (int32) of shape (num_rows, num_cols)     representing the current state of the grid.</li> <li>tetromino: jax array (int32) of shape (4, 4)     representing the current tetromino sampled from the tetromino list.</li> <li>action_mask: jax array (bool) of shape (4,  num_cols).     For each tetromino there are 4 rotations, each one corresponds     to a line in the action_mask.     Mask of the joint action space: True if the action     (x_position and rotation degree) is feasible     for the current tetromino and grid state.</li> </ul> </li> <li> <p>action: multi discrete array of shape (2,)</p> <ul> <li>rotation_index: The degree index determines the rotation of the     tetromino: 0 corresponds to 0 degrees, 1 corresponds to 90 degrees,     2 corresponds to 180 degrees, and 3 corresponds to 270 degrees.</li> <li>x_position: int between 0 and num_cols - 1 (included).</li> </ul> </li> <li> <p>reward:     The reward is 0 if no lines was cleared by the action and a convex function of the number     of cleared lines otherwise.</p> </li> <li> <p>episode termination:     if the tetromino cannot be placed anymore (i.e., it hits the top of the grid).</p> </li> </ul> <pre><code>from jumanji.environments import Tetris\nenv = Tetris()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>Tetris</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>num_rows</code> <code>int</code> <p>number of rows of the 2D grid. Defaults to 10.</p> <code>10</code> <code>num_cols</code> <code>int</code> <p>number of columns of the 2D grid. Defaults to 10.</p> <code>10</code> <code>time_limit</code> <code>int</code> <p>time_limit of an episode, i.e. number of environment steps before the episode ends. Defaults to 400.</p> <code>400</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>TetrisViewer</code>.</p> <code>None</code> Source code in <code>jumanji/environments/packing/tetris/env.py</code> <pre><code>def __init__(\n    self,\n    num_rows: int = 10,\n    num_cols: int = 10,\n    time_limit: int = 400,\n    viewer: Optional[Viewer[State]] = None,\n) -&gt; None:\n    \"\"\"Instantiates a `Tetris` environment.\n\n    Args:\n        num_rows: number of rows of the 2D grid. Defaults to 10.\n        num_cols: number of columns of the 2D grid. Defaults to 10.\n        time_limit: time_limit of an episode, i.e. number of environment steps before\n            the episode ends. Defaults to 400.\n        viewer: `Viewer` used for rendering. Defaults to `TetrisViewer`.\n    \"\"\"\n    if num_rows &lt; 4:\n        raise ValueError(f\"The `num_rows` must be &gt;= 4, but got num_rows={num_rows}\")\n    if num_cols &lt; 4:\n        raise ValueError(f\"The `num_cols` must be &gt;= 4, but got num_cols={num_cols}\")\n    self.num_rows = num_rows\n    self.num_cols = num_cols\n    self.padded_num_rows = num_rows + 3\n    self.padded_num_cols = num_cols + 3\n    self.TETROMINOES_LIST = jnp.array(TETROMINOES_LIST, jnp.int32)\n    self.reward_list = jnp.array(REWARD_LIST, float)\n    self.time_limit = time_limit\n    super().__init__()\n    self._viewer = viewer or TetrisViewer(\n        num_rows=self.num_rows,\n        num_cols=self.num_cols,\n    )\n</code></pre>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec. An action consists of two pieces of information: the amount of rotation (number of 90-degree rotations) and the x-position of the leftmost part of the tetromino.</p> <p>Returns:</p> Type Description <code>MultiDiscreteArray</code> <p>The action spec, which is a <code>specs.MultiDiscreteArray</code> object.</p>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Specifications of the observation of the <code>Tetris</code> environment.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec containing all the specifications for all the <code>Observation</code> fields: - grid: BoundedArray (jnp.int32) of shape (num_rows, num_cols). - tetromino: BoundedArray (bool) of shape (4, 4). - action_mask: BoundedArray (bool) of shape (NUM_ROTATIONS, num_cols). - step_count: DiscreteArray (num_values = time_limit) of shape ().</p>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.animate","title":"<code>animate(states, interval=100, save_path=None)</code>","text":"<p>Create an animation from a sequence of states. Args:     states: sequence of <code>State</code> corresponding to subsequent timesteps.     interval: delay between frames in milliseconds, default to 100.     save_path: the path where the animation file should be saved. If it is None, the plot         will not be saved. Returns:     animation that can export to gif, mp4, or render with HTML.</p> Source code in <code>jumanji/environments/packing/tetris/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 100,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Create an animation from a sequence of states.\n    Args:\n        states: sequence of `State` corresponding to subsequent timesteps.\n        interval: delay between frames in milliseconds, default to 100.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n    Returns:\n        animation that can export to gif, mp4, or render with HTML.\n    \"\"\"\n\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment. Args:     state: <code>State</code> object containing the current environment state.</p> Source code in <code>jumanji/environments/packing/tetris/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment.\n    Args:\n        state: `State` object containing the current environment state.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>needed for generating new tetrominoes.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p><code>State</code> corresponding to the new state of the environment,</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/packing/tetris/env.py</code> <pre><code>def reset(self, key: chex.PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: needed for generating new tetrominoes.\n\n    Returns:\n        state: `State` corresponding to the new state of the environment,\n        timestep: `TimeStep` corresponding to the first timestep returned by the\n            environment.\n    \"\"\"\n    grid_padded = jnp.zeros(shape=(self.padded_num_rows, self.padded_num_cols), dtype=jnp.int32)\n    tetromino, tetromino_index = utils.sample_tetromino_list(key, self.TETROMINOES_LIST)\n\n    action_mask = self._calculate_action_mask(grid_padded, tetromino_index)\n    state = State(\n        grid_padded=grid_padded,\n        grid_padded_old=grid_padded,\n        tetromino_index=tetromino_index,\n        old_tetromino_rotated=tetromino,\n        new_tetromino=tetromino,\n        x_position=jnp.array(0, jnp.int32),\n        y_position=jnp.array(0, jnp.int32),\n        action_mask=action_mask,\n        full_lines=jnp.full((self.num_rows + 3), False),\n        score=jnp.array(0, float),\n        reward=jnp.array(0, float),\n        key=key,\n        is_reset=True,\n        step_count=jnp.array(0, jnp.int32),\n    )\n\n    observation = Observation(\n        grid=grid_padded[: self.num_rows, : self.num_cols],\n        tetromino=tetromino,\n        action_mask=action_mask,\n        step_count=jnp.array(0, jnp.int32),\n    )\n    timestep = restart(observation=observation)\n    return state, timestep\n</code></pre>"},{"location":"api/environments/tetris/#jumanji.environments.packing.tetris.env.Tetris.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Array</code> <p><code>chex.Array</code> containing the rotation_index and x_position of the tetromino.</p> required <p>Returns:</p> Name Type Description <code>next_state</code> <code>State</code> <p><code>State</code> corresponding to the next state of the environment,</p> <code>next_timestep</code> <code>TimeStep[Observation]</code> <p><code>TimeStep</code> corresponding to the timestep returned by the environment.</p> Source code in <code>jumanji/environments/packing/tetris/env.py</code> <pre><code>def step(self, state: State, action: chex.Array) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: `chex.Array` containing the rotation_index and x_position of the tetromino.\n\n    Returns:\n        next_state: `State` corresponding to the next state of the environment,\n        next_timestep: `TimeStep` corresponding to the timestep returned by the environment.\n    \"\"\"\n    rotation_index, x_position = action\n    tetromino_index = state.tetromino_index\n    key, sample_key = jax.random.split(state.key)\n    tetromino = self._rotate(rotation_index, tetromino_index)\n    # Place the tetromino in the selected place\n    grid_padded, y_position = utils.place_tetromino(state.grid_padded, tetromino, x_position)\n    # A line is full when it doesn't contain any 0.\n    full_lines = jnp.all(grid_padded[:, : self.num_cols] != 0, axis=1)\n    nbr_full_lines = sum(full_lines)\n    grid_padded = utils.clean_lines(grid_padded, full_lines)\n    # Generate new tetromino\n    new_tetromino, tetromino_index = utils.sample_tetromino_list(\n        sample_key, self.TETROMINOES_LIST\n    )\n    grid_padded_cliped = jnp.clip(grid_padded, a_max=1)\n    action_mask = self._calculate_action_mask(grid_padded_cliped, tetromino_index)\n    # The maximum should be bigger than 0.\n    # In case the grid is empty the color should be set 0.\n    color = jnp.array([1, grid_padded.max()])\n    colored_tetromino = tetromino * jnp.max(color)\n    is_valid = state.action_mask[tuple(action)]\n    reward = self.reward_list[nbr_full_lines] * is_valid\n    step_count = state.step_count + 1\n    next_state = State(\n        grid_padded=grid_padded,\n        grid_padded_old=state.grid_padded,\n        tetromino_index=tetromino_index,\n        old_tetromino_rotated=colored_tetromino,\n        new_tetromino=new_tetromino,\n        x_position=x_position,\n        y_position=y_position,\n        action_mask=action_mask,\n        full_lines=full_lines,\n        score=state.score + reward,\n        reward=reward,\n        key=key,\n        is_reset=False,\n        step_count=step_count,\n    )\n    next_observation = Observation(\n        grid=grid_padded_cliped[: self.num_rows, : self.num_cols],\n        tetromino=new_tetromino,\n        action_mask=action_mask,\n        step_count=jnp.array(0, jnp.int32),\n    )\n\n    tetris_completed = ~jnp.any(action_mask)\n    done = tetris_completed | ~is_valid | (step_count &gt;= self.time_limit)\n\n    next_timestep = jax.lax.cond(\n        done,\n        termination,\n        transition,\n        reward,\n        next_observation,\n    )\n    return next_state, next_timestep\n</code></pre>"},{"location":"api/environments/tsp/","title":"TSP","text":"<p>               Bases: <code>Environment[State, DiscreteArray, Observation]</code></p> <p>Traveling Salesman Problem (TSP) environment as described in [1].</p> <ul> <li> <p>observation: Observation</p> <ul> <li>coordinates: jax array (float) of shape (num_cities, 2)     the coordinates of each city.</li> <li>position: jax array (int32)  of shape ()     the index corresponding to the last visited city.</li> <li>trajectory: jax array (int32) of shape (num_cities,)     array of city indices defining the route (-1 --&gt; not filled yet).</li> <li>action_mask: jax array (bool) of shape (num_cities,)     binary mask (False/True &lt;--&gt; illegal/legal &lt;--&gt; cannot be visited/can be visited).</li> </ul> </li> <li> <p>action: jax array (int32) of shape ()     [0, ..., num_cities - 1] -&gt; city to visit.</p> </li> <li> <p>reward: jax array (float) of shape (), could be either:</p> <ul> <li>dense: the negative distance between the current city and the chosen next city to go to.     It is 0 for the first chosen city, and for the last city, it also includes the distance     to the initial city to complete the tour.</li> <li>sparse: the negative tour length at the end of the episode. The tour length is defined     as the sum of the distances between consecutive cities. It is computed by starting at     the first city and ending there, after visiting all the cities. In both cases, the reward is a large negative penalty of <code>-num_cities * sqrt(2)</code> if the action is invalid, i.e. a previously selected city is selected again.</li> </ul> </li> <li> <p>episode termination:</p> <ul> <li>if no action can be performed, i.e. all cities have been visited.</li> <li>if an invalid action is taken, i.e. an already visited city is chosen.</li> </ul> </li> <li> <p>state: <code>State</code></p> <ul> <li>coordinates: jax array (float) of shape (num_cities, 2)     the coordinates of each city.</li> <li>position: int32     the identifier (index) of the last visited city.</li> <li>visited_mask: jax array (bool) of shape (num_cities,)     binary mask (False/True &lt;--&gt; not visited/visited).</li> <li>trajectory: jax array (int32) of shape (num_cities,)     the identifiers of the cities that have been visited (-1 means that no city has been     visited yet at that time in the sequence).</li> <li>num_visited: int32     number of cities that have been visited.</li> </ul> </li> </ul> <p>[1] Kwon Y., Choo J., Kim B., Yoon I., Min S., Gwon Y. (2020). \"POMO: Policy Optimization     with Multiple Optima for Reinforcement Learning\".</p> <pre><code>from jumanji.environments import TSP\nenv = TSP()\nkey = jax.random.PRNGKey(0)\nstate, timestep = jax.jit(env.reset)(key)\nenv.render(state)\naction = env.action_spec.generate_value()\nstate, timestep = jax.jit(env.step)(state, action)\nenv.render(state)\n</code></pre> <p>Instantiates a <code>TSP</code> environment.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Optional[Generator]</code> <p><code>Generator</code> whose <code>__call__</code> instantiates an environment instance. The default option is 'UniformGenerator' which randomly generates TSP instances with 20 cities sampled from a uniform distribution.</p> <code>None</code> <code>reward_fn</code> <code>Optional[RewardFn]</code> <p>RewardFn whose <code>__call__</code> method computes the reward of an environment transition. The function must compute the reward based on the current state, the chosen action and the next state. Implemented options are [<code>DenseReward</code>, <code>SparseReward</code>]. Defaults to <code>DenseReward</code>.</p> <code>None</code> <code>viewer</code> <code>Optional[Viewer[State]]</code> <p><code>Viewer</code> used for rendering. Defaults to <code>TSPViewer</code> with \"human\" render mode.</p> <code>None</code> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def __init__(\n    self,\n    generator: Optional[Generator] = None,\n    reward_fn: Optional[RewardFn] = None,\n    viewer: Optional[Viewer[State]] = None,\n):\n    \"\"\"Instantiates a `TSP` environment.\n\n    Args:\n        generator: `Generator` whose `__call__` instantiates an environment instance.\n            The default option is 'UniformGenerator' which randomly generates\n            TSP instances with 20 cities sampled from a uniform distribution.\n        reward_fn: RewardFn whose `__call__` method computes the reward of an environment\n            transition. The function must compute the reward based on the current state,\n            the chosen action and the next state.\n            Implemented options are [`DenseReward`, `SparseReward`]. Defaults to `DenseReward`.\n        viewer: `Viewer` used for rendering. Defaults to `TSPViewer` with \"human\" render mode.\n    \"\"\"\n\n    self.generator = generator or UniformGenerator(\n        num_cities=20,\n    )\n    self.num_cities = self.generator.num_cities\n    super().__init__()\n    self.reward_fn = reward_fn or DenseReward()\n    self._viewer = viewer or TSPViewer(name=\"TSP\", render_mode=\"human\")\n</code></pre>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.action_spec","title":"<code>action_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the action spec.</p> <p>Returns:</p> Name Type Description <code>action_spec</code> <code>DiscreteArray</code> <p>a <code>specs.DiscreteArray</code> spec.</p>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.observation_spec","title":"<code>observation_spec</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the observation spec.</p> <p>Returns:</p> Type Description <code>Spec[Observation]</code> <p>Spec for the <code>Observation</code> whose fields are:</p> <code>Spec[Observation]</code> <ul> <li>coordinates: BoundedArray (float) of shape (num_cities,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>position: DiscreteArray (num_values = num_cities) of shape ().</li> </ul> <code>Spec[Observation]</code> <ul> <li>trajectory: BoundedArray (int32) of shape (num_cities,).</li> </ul> <code>Spec[Observation]</code> <ul> <li>action_mask: BoundedArray (bool) of shape (num_cities,).</li> </ul>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.animate","title":"<code>animate(states, interval=200, save_path=None)</code>","text":"<p>Creates an animated gif of the <code>TSP</code> environment based on the sequence of states.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Sequence[State]</code> <p>sequence of environment states corresponding to consecutive timesteps.</p> required <code>interval</code> <code>int</code> <p>delay between frames in milliseconds, default to 200.</p> <code>200</code> <code>save_path</code> <code>Optional[str]</code> <p>the path where the animation file should be saved. If it is None, the plot will not be saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>FuncAnimation</code> <p>animation.FuncAnimation: the animation object that was created.</p> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def animate(\n    self,\n    states: Sequence[State],\n    interval: int = 200,\n    save_path: Optional[str] = None,\n) -&gt; matplotlib.animation.FuncAnimation:\n    \"\"\"Creates an animated gif of the `TSP` environment based on the sequence of states.\n\n    Args:\n        states: sequence of environment states corresponding to consecutive timesteps.\n        interval: delay between frames in milliseconds, default to 200.\n        save_path: the path where the animation file should be saved. If it is None, the plot\n            will not be saved.\n\n    Returns:\n        animation.FuncAnimation: the animation object that was created.\n    \"\"\"\n    return self._viewer.animate(states, interval, save_path)\n</code></pre>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.close","title":"<code>close()</code>","text":"<p>Perform any necessary cleanup.</p> <p>Environments will automatically :meth:<code>close()</code> themselves when garbage collected or when the program exits.</p> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Perform any necessary cleanup.\n\n    Environments will automatically :meth:`close()` themselves when\n    garbage collected or when the program exits.\n    \"\"\"\n    self._viewer.close()\n</code></pre>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.render","title":"<code>render(state)</code>","text":"<p>Render the given state of the environment. This rendering shows the layout of the cities and the tour so far.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>current environment state.</p> required <p>Returns:</p> Name Type Description <code>rgb_array</code> <code>Optional[NDArray]</code> <p>the RGB image of the state as an array.</p> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def render(self, state: State) -&gt; Optional[NDArray]:\n    \"\"\"Render the given state of the environment. This rendering shows the layout of the cities\n    and the tour so far.\n\n    Args:\n        state: current environment state.\n\n    Returns:\n        rgb_array: the RGB image of the state as an array.\n    \"\"\"\n    return self._viewer.render(state)\n</code></pre>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.reset","title":"<code>reset(key)</code>","text":"<p>Resets the environment.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>PRNGKey</code> <p>used to randomly generate the coordinates.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>State object corresponding to the new state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>TimeStep object corresponding to the first timestep returned by the environment.</p> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def reset(self, key: PRNGKey) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Resets the environment.\n\n    Args:\n        key: used to randomly generate the coordinates.\n\n    Returns:\n        state: State object corresponding to the new state of the environment.\n        timestep: TimeStep object corresponding to the first timestep returned\n            by the environment.\n    \"\"\"\n    state = self.generator(key)\n    timestep = restart(observation=self._state_to_observation(state))\n    return state, timestep\n</code></pre>"},{"location":"api/environments/tsp/#jumanji.environments.routing.tsp.env.TSP.step","title":"<code>step(state, action)</code>","text":"<p>Run one timestep of the environment's dynamics.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p><code>State</code> object containing the dynamics of the environment.</p> required <code>action</code> <code>Numeric</code> <p><code>Array</code> containing the index of the next position to visit.</p> required <p>Returns:</p> Name Type Description <code>state</code> <code>State</code> <p>the next state of the environment.</p> <code>timestep</code> <code>TimeStep[Observation]</code> <p>the timestep to be observed.</p> Source code in <code>jumanji/environments/routing/tsp/env.py</code> <pre><code>def step(self, state: State, action: chex.Numeric) -&gt; Tuple[State, TimeStep[Observation]]:\n    \"\"\"Run one timestep of the environment's dynamics.\n\n    Args:\n        state: `State` object containing the dynamics of the environment.\n        action: `Array` containing the index of the next position to visit.\n\n    Returns:\n        state: the next state of the environment.\n        timestep: the timestep to be observed.\n    \"\"\"\n    is_valid = ~state.visited_mask[action]\n    next_state = jax.lax.cond(\n        is_valid,\n        self._update_state,\n        lambda *_: state,\n        state,\n        action,\n    )\n\n    reward = self.reward_fn(state, action, next_state, is_valid)\n    observation = self._state_to_observation(next_state)\n\n    # Terminate if all cities have been visited or the action is invalid\n    is_done = (next_state.num_visited == self.num_cities) | ~is_valid\n    timestep = jax.lax.cond(\n        is_done,\n        termination,\n        transition,\n        reward,\n        observation,\n    )\n    return next_state, timestep\n</code></pre>"},{"location":"environments/bin_pack/","title":"BinPack Environment","text":"<p>We provide here an implementation of the 3D bin packing problem. In this problem, the goal of the agent is to efficiently pack a set of boxes (items) of different sizes into a single container with as little empty space as possible. Since there is only 1 bin, this formulation is equivalent to the 3D-knapsack problem.</p>"},{"location":"environments/bin_pack/#observation","title":"Observation","text":"<p>The observation given to the agent provides information on the available empty space (called EMSs), the items that still need to be packed, and information on what actions are valid at this point. The full observation is as follows:</p> <ul> <li> <p><code>ems</code>: <code>EMS</code> tree of jax arrays (float if <code>normalize_dimensions</code> else int32) each of shape     <code>(obs_num_ems,)</code>, coordinates of all EMSs at the current timestep.</p> </li> <li> <p><code>ems_mask</code>: jax array (bool) of shape <code>(obs_num_ems,)</code>, indicates the EMSs that are valid.</p> </li> <li> <p><code>items</code>: <code>Item</code> tree of jax arrays (float if <code>normalize_dimensions</code> else int32) each of shape     <code>(max_num_items,)</code>, characteristics of all items for this instance.</p> </li> <li> <p><code>items_mask</code>: jax array (bool) of shape <code>(max_num_items,)</code>, indicates the items that are valid.</p> </li> <li> <p><code>items_placed</code>: jax array (bool) of shape <code>(max_num_items,)</code>, indicates the items that have been     placed so far.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(obs_num_ems, max_num_items)</code>, mask of the joint action     space: <code>True</code> if the action <code>[ems_id, item_id]</code> is valid.</p> </li> </ul>"},{"location":"environments/bin_pack/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> of 2 integer values representing the ID of an EMS (space) and the ID of an item. For instance, <code>[1, 5]</code> will place item 5 in EMS 1.</p>"},{"location":"environments/bin_pack/#reward","title":"Reward","text":"<p>The reward could be either:</p> <ul> <li> <p>Dense: normalized volume (relative to the container volume) of the item packed by taking     the chosen action. The computed reward is equivalent to the increase in volume utilization     of the container due to packing the chosen item. If the action is invalid, the reward is 0.0     instead.</p> </li> <li> <p>Sparse: computed only at the end of the episode (otherwise, returns 0.0). Returns the volume     utilization of the container (between 0.0 and 1.0). If the action is invalid, the action is     ignored and the reward is still returned as the current container utilization.</p> </li> </ul>"},{"location":"environments/bin_pack/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>BinPack-v2</code>, 3D bin-packing problem with a solvable random generator that generates up to 20 items maximum, that can handle 40 EMSs maximum that are given in the observation.</li> </ul>"},{"location":"environments/cleaner/","title":"Cleaner Environment","text":"<p>We provide here a JAX jit-able implementation of the Multi-Agent Cleaning environment.</p> <p>In this environment, multiple agents must cooperatively clean the floor of a room with complex indoor barriers (black). At the beginning of an episode, the whole floor is dirty (green). Every time an agent (red) visits a dirty tile, it is cleaned (white).</p> <p>The goal is to clean as many tiles as possible in a given time budget.</p> <p>A new maze is randomly generated using a recursive division method for each new episode. Agents always start in the top left corner of the maze.</p>"},{"location":"environments/cleaner/#observation","title":"Observation","text":"<p>The observation seen by the agent is a <code>NamedTuple</code> containing the following:</p> <ul> <li> <p><code>grid</code>: jax array (int8) of shape <code>(num_rows, num_cols)</code>, array representing the grid, each tile is     either dirty (0), clean (1), or a wall (2).</p> </li> <li> <p><code>agents_locations</code>: jax array (int) of shape <code>(num_agents, 2)</code>, array specifying the x and y     coordinates of every agent.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_agents, 4)</code>, array specifying, for each agent,     which action (up, right, down, left) is legal.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, number of steps elapsed in the current episode.</p> </li> </ul>"},{"location":"environments/cleaner/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> containing an integer value in <code>[0, 1, 2, 3]</code> for each agent. Each agent can take one of four actions: up (<code>0</code>), right (<code>1</code>), down (<code>2</code>), or left (<code>3</code>).</p> <p>The episode terminates if any agent meets one of the following conditions:</p> <ul> <li> <p>An invalid action is taken, or</p> </li> <li> <p>An action is blocked by a wall.</p> </li> </ul> <p>In both cases, the agent's position remains unchanged.</p>"},{"location":"environments/cleaner/#reward","title":"Reward","text":"<p>The reward is global and shared among the agents. It is equal to the number of tiles which were cleaned during the time step, minus a penalty (0.5 by default) to encourage agents to clean the maze faster.</p>"},{"location":"environments/cleaner/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Cleaner-v0</code>, a room of size 10x10 with 3 agents.</li> </ul>"},{"location":"environments/connector/","title":"Connector Environment","text":"<p>The <code>Connector</code> environment contains multiple agents spawned in a grid world with each agent representing a start and end position that need to be connected. The main goal of the environment is to connect each start and end position in as few steps as possible. However, when an agent moves it leaves behind a path, which is impassable by all agents. Thus, agents need to cooperate in order to allow each other to connect to their own targets without overlapping.</p> <p>An episode ends when all agents have connected to their targets or no agents can make any further moves due to being blocked.</p>"},{"location":"environments/connector/#observation","title":"Observation","text":"<p>At each step observation contains 3 items: a grid, an action mask for each agent and the episode step count.</p> <ul> <li> <p><code>grid</code>: jax array (int32) of shape <code>(grid_size, grid_size)</code>, a 2D matrix that represents pairs    of points that need to be connected. Each agent has three types of points: position,    target and path which are represented by different numbers on the grid. The    position of an agent has to connect to its target, leaving a path behind    it as it moves across the grid forming its route. Each agent connects to only 1 target.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_agents, 5)</code>, indicates which actions each agent    can take.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, represents how many steps have been taken in    the environment since the last reset.</p> </li> </ul>"},{"location":"environments/connector/#encoding","title":"Encoding","text":"<p>Each agent has 3 components represented in the observation space: position, target, and path. Each agent in the environment will have an integer representing their components.</p> <ul> <li> <p>Positions are encoded starting from 2 in multiples of 3: 2, 5, 8, \u2026</p> </li> <li> <p>Targets are encoded starting from 3 in multiples of 3: 3, 6, 9, \u2026</p> </li> <li> <p>Paths appear in the location of the head once it moves, starting from 1 in multiples of 3: 1, 4, 7, \u2026</p> </li> </ul> <p>Every group of 3 corresponds to 1 agent: (1,2,3), (4,5,6), \u2026</p> <p>Example: <pre><code>Agent1[path=1, position=2, target=3]\nAgent2[path=4, position=5, target=6]\nAgent3[path=7, position=8, target=9]\n</code></pre></p> <p>For example, on a 6x6 grid, a possible observation is shown below.</p> <pre><code>[[ 2  0  3  0  0  0]\n [ 1  0  4  4  4  0]\n [ 1  0  5  9  0  0]\n [ 1  0  0  0  0  0]\n [ 0  0  0  8  0  0]\n [ 0  0  6  7  7  7]]\n</code></pre>"},{"location":"environments/connector/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> of shape <code>(num_agents,)</code> of integer values in the range of <code>[0, 4]</code>. Each value corresponds to an agent moving in 1 of 4 cardinal directions or taking the no-op action. That is, [0, 1, 2, 3, 4] -&gt; [No Op, Up, Right, Down, Left].</p>"},{"location":"environments/connector/#reward","title":"Reward","text":"<p>The reward is dense: +1.0 per agent that connects at that step and -0.03 per agent that has not connected yet.</p> <p>Rewards are provided in the shape <code>(num_agents,)</code> so that each agent can have a reward.</p>"},{"location":"environments/connector/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Connector-v2</code>, grid size of 10 and 10 agents.</li> </ul>"},{"location":"environments/cvrp/","title":"Capacitated Vehicle Routing Problem (CVRP) Environment","text":"<p>We provide here a Jax JIT-able implementation of the capacitated vehicle routing problem (CVRP) which is a specific type of VRP.</p> <p>CVRP is a classic combinatorial optimization problem. Given a set of nodes with specific demands, a depot node, and a vehicle with limited capacity, the goal is to determine the shortest route between the nodes such that each node (excluding depot) is visited exactly once and has its demand covered. The problem is NP-complete, thus there is no known algorithm both correct and fast (i.e., that runs in polynomial time) for any instance of the problem.</p> <p>A new problem instance is generated by resetting the environment. The problem instance contains coordinates for each node sampled from a uniform distribution between 0 and 1, and each node (except for depot) has a specific demand which is an integer value sampled from a uniform distribution between 1 and the maximum demand (which is a parameter of the CVRP environment). The number of nodes with demand is a parameter of the environment.</p>"},{"location":"environments/cvrp/#observation","title":"Observation","text":"<p>The observation given to the agent provides information on the problem layout, the visited/unvisited cities and the current position of the agent as well as the current capacity.</p> <ul> <li> <p><code>coordinates</code>: jax array (float) of shape <code>(num_nodes + 1, 2)</code>, array of coordinates of each city node and the depot node.</p> </li> <li> <p><code>demands</code>: jax array (float) of shape <code>(num_nodes + 1,)</code>, array of the demands of each city node and the depot node whose demand is set to 0.</p> </li> <li> <p><code>unvisited_nodes</code>: jax array (bool) of shape <code>(num_nodes + 1,)</code>, array denoting which nodes remain to be visited.</p> </li> <li> <p><code>position</code>: jax array (int32) of shape <code>()</code>, identifier (index) of the current visited node (city or depot).</p> </li> <li> <p><code>trajectory</code>: jax array (int32) of shape <code>(2 * num_nodes,)</code>, identifiers of the nodes that have been visited (set to <code>DEPOT_IDX</code> if not filled yet).</p> </li> <li> <p><code>capacity</code>: jax array (float) of shape <code>()</code>, current capacity of the vehicle.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_nodes + 1,)</code>, array denoting which actions are possible (True) and which are not (False).</p> </li> </ul>"},{"location":"environments/cvrp/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in the range of <code>[0, num_nodes]</code>. An action is the index of the next node to visit, and an action value of 0 corresponds to visiting the depot.</p>"},{"location":"environments/cvrp/#reward","title":"Reward","text":"<p>The reward could be either:</p> <ul> <li> <p>Dense: the negative distance between the current node and the chosen next node to go to.     For the last node, it also includes the distance to the depot to complete the tour.</p> </li> <li> <p>Sparse: the negative tour length at the end of the episode. The tour length is defined     as the sum of the distances between consecutive nodes.</p> </li> </ul> <p>In both cases, the reward is a large negative penalty of <code>-2 * num_nodes * sqrt(2)</code> if the action is invalid, e.g. a previously selected node other than the depot is selected again.</p>"},{"location":"environments/cvrp/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>CVRP-v1</code>: CVRP problem with 20 randomly generated nodes, a maximum capacity of 30, a maximum demand for each node of 10 and a dense reward function.</li> </ul>"},{"location":"environments/flat_pack/","title":"FlatPack Environment","text":"<p>We provide here a Jax JIT-able implementation of a packing environment named flat pack. The goal of the agent is to place all the available blocks on an empty 2D grid. Each time an episode resets a new set of blocks is created and the grid is emptied. Blocks are randomly shuffled and rotated and all have shape (3, 3).</p>"},{"location":"environments/flat_pack/#observation","title":"Observation","text":"<p>The observation given to the agent gives a view of the current state of the grid as well as all blocks that can be placed.</p> <ul> <li> <p><code>current_grid</code>: jax array (float32) of shape <code>(num_rows, num_cols)</code> with values in the range     <code>[0, num_blocks]</code> (corresponding to the number of each block). This grid will have zeros     where no blocks have been placed and numbers corresponding to each block where that particular     block has been placed.</p> </li> <li> <p><code>blocks</code>: jax array (float32) of shape <code>(num_blocks, 3, 3)</code> of all possible blocks in     that can fit in the current grid. These blocks are shuffled, rotated and will always have shape <code>(3, 3)</code>.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_blocks, 4, num_rows-2, num_cols-2)</code>, representing     which actions are possible given the current state of the grid. The first index indicates the     number of blocks associated with a given grid. The second index indicates the number of times a block may be rotated.     The third and fourth indices indicate the row and column coordinate of where a blocks top left-most corner may be placed     respectively. Blocks are placed by an agent by specifying the row and column coordinate on the grid where the top left corner     of the selected block should be placed. These values will always be <code>num_rows-2</code> and <code>num_cols-2</code>     respectively to make it impossible for an agent to place a block outside the current grid.</p> </li> </ul>"},{"location":"environments/flat_pack/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code>, specifically a tuple of an index between 0 and <code>num_blocks - 1</code>, an index between 0 and 4 (since there are 4 possible rotations), an index between 0 and <code>num_rows-2</code> (the possible row coordinates for placing a block) and an index between 0 and <code>num_cols-2</code> (the possible column coordinates for placing a block). An action thus consists of four pieces of information:</p> <ul> <li> <p>Block to place,</p> </li> <li> <p>Number of 90 degree rotations to make to a chosen block ({0, 90, 180, 270} degrees),</p> </li> <li> <p>Row coordinate for placing the rotated block's top left corner,</p> </li> <li> <p>Column coordinate for placing the rotated block's top left corner.</p> </li> </ul>"},{"location":"environments/flat_pack/#reward","title":"Reward","text":"<p>The reward function is configurable, but by default is a fully dense reward giving the sum of the number of non-zero cells in a placed block normalised by the total number of cells in the grid at each timestep. The episode terminates if either the grid is filled or <code>num_blocks</code> steps have been taken by an agent.</p>"},{"location":"environments/flat_pack/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>FlatPack-v0</code>, a flat pack environment grid with 11 rows and 11 columns containing 5 row blocks and 5 column blocks     for a total of 25 blocks that can be placed on the grid. This version has a dense reward.</li> </ul>"},{"location":"environments/game_2048/","title":"2048 Environment","text":"<p>We provide here a Jax JIT-able implementation of the game 2048.</p> <p>2048 is a popular single-player puzzle game that is played on a 4x4 grid. The game board consists of cells, each containing a power of 2, and the objective is to reach a score of at least 2048 by merging cells together. The player can shift the entire grid in one of the four directions (up, down, right, left) to combine cells of the same value. When two adjacent cells have the same value, they merge into a single cell with a value equal to the sum of the two cells. The game ends when the player is no longer able to make any further moves. The ultimate goal is to achieve the highest-valued tile possible, with the hope of surpassing 2048. With each move, the player must carefully plan and strategize to reach the highest score possible.</p>"},{"location":"environments/game_2048/#observation","title":"Observation","text":"<p>The observation in the game 2048 includes information about the board, the action mask, and the step count.</p> <ul> <li> <p><code>board</code>: jax array (int32) of shape <code>(board_size, board_size)</code>, representing the current game     state. Each nonzero element in the array corresponds to a game tile and holds an exponent of 2.     The actual value of the tile is obtained by raising 2 to the power of said exponent.</p> <ul> <li> <p>Here is an example of a random observation of the game board:     <pre><code>[[ 2 0 1 4]\n [ 5 3 0 2]\n [ 0 2 3 2]\n [ 1 2 0 0]]\n</code></pre></p> </li> <li> <p>This array can be converted into the actual game board:     <pre><code>[[  4 0 2 16]\n [ 32 8 0  4]\n [  0 4 8  4]\n [  2 4 0  0]]\n</code></pre></p> </li> </ul> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(4,)</code>, indicating which actions are valid in the current state of the environment. The actions include moving the tiles up, right, down, or left. For example, an action mask <code>[False, True, False, False]</code> means that the only valid action is to move the tiles rightward.</p> </li> </ul>"},{"location":"environments/game_2048/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in <code>[0, 1, 2, 3]</code>. Specifically, these four actions correspond to: up (0), right (1), down (2), or left (3).</p>"},{"location":"environments/game_2048/#reward","title":"Reward","text":"<p>Taking an action in 2048 only returns a reward when two tiles of equal value are merged into a new tile containing their sum (i.e. twice each of their values). The cumulative reward in an episode is the sum of the values of all newly created tiles. For example, if a player merges two 512-value tiles to create a new 1024-value tile, and then merges two 256-value tiles to create a new 512-value tile, the total reward from these actions is 1536 (i.e., 1024 + 512).</p>"},{"location":"environments/game_2048/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Game2048-v1</code>, the default settings for 2048 with a board of size 4x4.</li> </ul>"},{"location":"environments/graph_coloring/","title":"Graph Coloring Environment","text":"<p>We provide here a Jax JIT-able implementation of the Graph Coloring environment.</p> <p>Graph coloring is a combinatorial optimization problem where the objective is to assign a color to each vertex of a graph in such a way that no two adjacent vertices share the same color. The problem is usually formulated as minimizing the number of colors used. The <code>GraphColoring</code> environment is an episodic, single-agent setting that allows for the exploration of graph coloring algorithms and reinforcement learning methods.</p>"},{"location":"environments/graph_coloring/#observation","title":"Observation","text":"<p>The observation in the <code>GraphColoring</code> environment includes information about the graph, the colors assigned to the vertices, the action mask, and the current node index.</p> <ul> <li><code>graph</code>: jax array (bool) of shape <code>(num_nodes, num_nodes)</code>, representing the adjacency matrix of the graph.</li> <li> <p>For example, a random observation of the graph adjacency matrix:</p> <pre><code>```[[False,  True, False,  True],\n[ True, False,  True, False],\n[False,  True, False,  True],\n[ True, False,  True, False]]```\n</code></pre> </li> <li> <p><code>colors</code>: a JAX array (int32) of shape <code>(num_nodes,)</code>, representing the current color assignments for the vertices. Initially, all elements are set to -1, indicating that no colors have been assigned yet.</p> </li> <li> <p>For example, an initial color assignment:     <code>[-1, -1, -1, -1]</code></p> </li> <li> <p><code>action_mask</code>: a JAX array of boolean values, shaped <code>(num_colors,)</code>, which indicates the valid actions in the current state of the environment. Each position in the array corresponds to a color. True at a position signifies that the corresponding color can be used to color a node, while False indicates the opposite.</p> </li> <li> <p>For example, for 4 number of colors available:     <code>[True, False, True, False]</code></p> </li> <li> <p><code>current_node_index</code>: an integer representing the current node being colored.</p> </li> <li>For example, an initial current_node_index might be 0.</li> </ul>"},{"location":"environments/graph_coloring/#action","title":"Action","text":"<p>The action space is a DiscreteArray of integer values in <code>[0, 1, ..., num_colors - 1]</code>. Each action corresponds to assigning a color to the current node.</p>"},{"location":"environments/graph_coloring/#reward","title":"Reward","text":"<p>The reward in the <code>GraphColoring</code> environment is given as follows:</p> <ul> <li><code>sparse reward</code>: a reward is provided at the end of the episode and equals the negative of the number of unique colors used to color all vertices in the graph.</li> </ul> <p>The agent's goal is to find a valid coloring using as few colors as possible while avoiding conflicts with adjacent nodes.</p>"},{"location":"environments/graph_coloring/#episode-termination","title":"Episode Termination","text":"<p>The goal of the agent is to find a valid coloring using as few colors as possible. An episode in the graph coloring environment can terminate under two conditions:</p> <ol> <li> <p>All nodes have been assigned a color: the environment iteratively assigns colors to nodes. When all nodes have a color assigned (i.e., there are no nodes with a color value of -1), the episode ends. This is the natural termination condition and ideally the one we'd like the agent to achieve.</p> </li> <li> <p>Invalid action is taken: an action is considered invalid if it tries to assign a color to a node that is not within the allowed color set for that node at that time. The allowed color set for each node is updated after every action. If an invalid action is attempted, the episode immediately terminates and the agent receives a large negative reward. This encourages the agent to learn valid actions and discourages it from making invalid actions.</p> </li> </ol>"},{"location":"environments/graph_coloring/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>GraphColoring-v0</code>: The default settings for the <code>GraphColoring</code> problem with a configurable number of nodes and edge_probability. The default number of nodes is 20, and the default edge probability is 0.8.</li> </ul>"},{"location":"environments/job_shop/","title":"JobShop Environment","text":"<p>We provide here a JAX jit-able implementation of the job shop scheduling problem. It is NP-hard and one of the most well-known combinatorial optimisation problems. The problem formulation is:</p> <ul> <li> <p><code>N</code> jobs, each consisting of a sequence of operations, need to be scheduled on <code>M</code> machines.</p> </li> <li> <p>For each job, its operations must be processed in order. This is called the precedence constraints.</p> </li> <li> <p>Only one operation in a job can be processed at any given time.</p> </li> <li> <p>A machine can only work on one operation at a time.</p> </li> <li> <p>Once started, an operation must run to completion.</p> </li> </ul> <p>The goal of the agent is to determine the schedule that minimises the time needed to process all the jobs. The length of the schedule is also known as its makespan.</p>"},{"location":"environments/job_shop/#observation","title":"Observation","text":"<p>The observation seen by the agent is a <code>NamedTuple</code> containing the following:</p> <ul> <li> <p><code>ops_machine_ids</code>: jax array (int32) of shape <code>(num_jobs, max_num_ops)</code>. For each job, it     specifies the machine each op must be processed on. Note that a <code>-1</code> corresponds to padded ops     since not all jobs have the same number of ops.</p> </li> <li> <p><code>ops_durations</code>: jax array (int32) of shape <code>(num_jobs, max_num_ops)</code>. For each job, it specifies     the processing time of each operation. Note that a <code>-1</code> corresponds to padded ops since not all     jobs have the same number of ops.</p> </li> <li> <p><code>ops_mask</code>: jax array (bool) of shape <code>(num_jobs, max_num_ops)</code>. For each job, indicates which     operations remain to be scheduled. False if the op has been scheduled or if the op was added     for padding, True otherwise. The first True in each row (i.e. each job) identifies the next     operation for that job.</p> </li> <li> <p><code>machines_job_ids</code>: jax array (int32) of shape <code>(num_machines,)</code>. For each machine, it specifies     the job currently being processed. Note that <code>-1</code> means no-op in which case the remaining time     until available is always 0.</p> </li> <li> <p><code>machines_remaining_times</code>: jax array (int32) of shape <code>(num_machines,)</code>. For each machine, it     specifies the number of time steps until available.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of <code>(num_machines, num_jobs + 1)</code>. For each machine, it indicates     which jobs (or no-op) can legally be scheduled. The last column corresponds to no-op.</p> </li> </ul>"},{"location":"environments/job_shop/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> containing an integer value in <code>[0, 1, ..., num_jobs]</code> for each machine. Thus, an action consists of the following: for each machine, decide which job (or no-op) to schedule at the current time step. The action is represented as a 1-dimensional array of length <code>num_machines</code>. For example, suppose we have <code>M=5</code> machines and there are <code>N=10</code> jobs. A legal action might be <pre><code>action = [4, 7, 0, 10, 10]\n</code></pre> This action represents scheduling</p> <ul> <li> <p>Job 4 on Machine 0,</p> </li> <li> <p>Job 7 on Machine 1,</p> </li> <li> <p>Job 0 on Machine 2,</p> </li> <li> <p>No-op on Machine 3,</p> </li> <li> <p>No-op on Machine 4.</p> </li> </ul> <p>As such, the action is multidimensional and can be thought of as each machine (each agent) deciding which job (or no-op) to schedule. Importantly, the action space is a product of the marginal action space of each agent (machine).</p> <p>The rationale for having a no-op is the following:</p> <ul> <li> <p>A machine might be busy processing an operation, in which case a no-op is the only allowed action for that machine.</p> </li> <li> <p>There might not be any jobs that can be scheduled on a machine.</p> </li> <li> <p>There may be scenarios where waiting to schedule a job via one or more no-op(s) ultimately minimizes the makespan.</p> </li> </ul>"},{"location":"environments/job_shop/#reward","title":"Reward","text":"<p>The reward setting is dense: a reward of <code>-1</code> is given each time step if none of the termination criteria are met. An episode will terminate in any of the three scenarios below:</p> <ul> <li> <p>Finished schedule: all operations (and thus all jobs) every job have been processed.</p> </li> <li> <p>Illegal action: the agent ignores the action mask and takes an illegal action.</p> </li> <li> <p>Simultaneously idle: all machines are inactive at the same time.</p> </li> </ul> <p>If all machines are simultaneously idle or the agent selects an invalid action, this is reflected in a large penalty in the reward. This would be <code>-num_jobs * max_num_ops * max_op_duration</code> which is a upper bound on the makespan, corresponding to if every job had <code>max_num_ops</code> operations and every operation had a processing time of <code>max_op_duration</code>.</p>"},{"location":"environments/job_shop/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>JobShop-v0</code>: job-shop scheduling problem with 20 jobs, 10 machines, a maximum of 8 operations per job, and a max operation duration of 6 timesteps per operation.</li> </ul>"},{"location":"environments/knapsack/","title":"Knapskack Environment","text":"<p>We provide here a Jax JIT-able implementation of the knapskack problem.</p> <p>The knapsack problem is a famous problem in combinatorial optimization. The goal is to determine, given a set of items, each with a weight and a value, which items to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.</p> <p>The decision problem form of the knapsack problem is NP-complete, thus there is no known algorithm both correct and fast (polynomial-time) in all cases.</p> <p>When the environment is reset, a new problem instance is generated, by sampling weights and values from a uniform distribution between 0 and 1. The weight limit of the knapsack is a parameter of the environment. A trajectory terminates when no further item can be added to the knapsack or the chosen action is invalid.</p>"},{"location":"environments/knapsack/#observation","title":"Observation","text":"<p>The observation given to the agent provides information regarding the weights and the values of all the items, as well as, which items have been packed into the knapsack.</p> <ul> <li> <p><code>weights</code>: jax array (float) of shape <code>(num_items,)</code>, array of weights of the items to be packed into the knapsack.</p> </li> <li> <p><code>values</code>: jax array (float) of shape <code>(num_items,)</code>, array of values of the items to be packed into the knapsack.</p> </li> <li> <p><code>packed_items</code>: jax array (bool) of shape <code>(num_items,)</code>, array of binary values denoting which items are already packed into the knapsack.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_items,)</code>, array of binary values denoting which items can be packed into the knapsack.</p> </li> </ul>"},{"location":"environments/knapsack/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in the range of <code>[0, num_items-1]</code>. An action is the index of the next item to pack.</p>"},{"location":"environments/knapsack/#reward","title":"Reward","text":"<p>The reward can be either:</p> <ul> <li> <p>Dense: the value of the item to pack at the current timestep.</p> </li> <li> <p>Sparse: the sum of the values of the items packed in the bag at the end of the episode.</p> </li> </ul> <p>In both cases, the reward is 0 if the action is invalid, i.e. an item that was previously selected is selected again or has a weight larger than the bag capacity.</p>"},{"location":"environments/knapsack/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Knapsack-v1</code>: Knapsack problem with 50 randomly generated items, a total budget of 12.5 and a dense reward function.</li> </ul>"},{"location":"environments/lbf/","title":"# Level-Based Foraging Environment","text":"<p>We provide a JAX jit-able implementation of the Level-Based Foraging environment.</p> <p>The Level-Based Foraging (LBF) represents a mixed cooperative-competitive environment that emphasises coordination between agents. As illustrated above, agents are placed within a grid world and assigned different levels.</p> <p>To collect food, agents must be adjacent to it and the cumulative level of participating agents must meet or exceed the food's designated level. Agents receive points based on the level of the collected food and their own level.</p>"},{"location":"environments/lbf/#observation","title":"Observation","text":"<p>The observation seen by the agent is a <code>NamedTuple</code> containing the following:</p> <ul> <li> <p><code>agents_view</code>: jax array (int32) of shape <code>(num_agents, num_obs_features)</code>, array representing the agent's view of other agents     and food.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_agents, 6)</code>, array specifying, for each agent,     which action (noop, up, down, left, right, load) is legal.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, number of steps elapsed in the current episode.</p> </li> </ul>"},{"location":"environments/lbf/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> containing an integer value in <code>[0, 1, 2, 3, 4, 5]</code> for each agent. Each agent can take one of five actions: noop (<code>0</code>), up (<code>1</code>), down (<code>2</code>), turn left (<code>3</code>), turn right (<code>4</code>), or pick up food (<code>5</code>).</p> <p>The episode terminates under the following conditions:</p> <ul> <li> <p>An invalid action is taken, or</p> </li> <li> <p>An agent collides with another agent.</p> </li> </ul>"},{"location":"environments/lbf/#reward","title":"Reward","text":"<p>The reward is equal to the sum of the levels of collected food divided by the level of the agents that collected them.</p>"},{"location":"environments/lbf/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>LevelBasedForaging-v0</code>, a grid with 2 agents each with a field of view equal to the grid size (full observation case), with 2 food items and forcing the cooperation between agents.</li> </ul>"},{"location":"environments/maze/","title":"Maze Environment","text":"<p>We provide here a Jax JIT-able implementation of a 2D maze problem. The maze is a size-configurable 2D matrix where each cell represents either free space (white) or wall (black).</p> <p>The goal is for the agent (green) to reach the single target cell (red). It is a sparse reward problem, where the agent receives a reward of 0 at every step and a reward of 1 for reaching the target. The agent may choose to move one space up, right, down, or left: (\"N\", \u201cE\u201d, \"S\",  \"W\"). If the way is blocked by a wall, it will remain at the same position.</p> <p>Each maze is randomly generated using a recursive division function. By default, a new maze, initial agent position and target position are generated each time the environment is reset.</p>"},{"location":"environments/maze/#observation","title":"Observation","text":"<p>As an observation, the agent has access to the current maze configuration in the array named <code>walls</code>. It also has access to its current position <code>agent_position</code>, the target's <code>target_position</code>, the number of steps <code>step_count</code> elapsed in the current episode and the action mask <code>action_mask</code>.</p> <ul> <li> <p><code>agent_position</code>: Position(row, col) (int32) each of shape <code>()</code>, agent position in the maze.</p> </li> <li> <p><code>target_position</code>: Position(row, col) (int32) each of shape <code>()</code>, target position in the maze.</p> </li> <li> <p><code>walls</code>: jax array (bool) of shape <code>(num_rows, num_cols)</code>, indicates whether a grid cell is a wall.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, number of steps elapsed in the current episode.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(4,)</code>, binary values denoting whether each action is possible.</p> </li> </ul> <p>An example 5x5 observation <code>walls</code> array, is shown below. 1 represents a wall, and 0 represents free space.</p> <pre><code>[0, 1, 0, 0, 0],\n[0, 1, 0, 1, 1],\n[0, 1, 0, 0, 0],\n[0, 0, 0, 1, 1],\n[0, 0, 0, 0, 0]\n</code></pre>"},{"location":"environments/maze/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in the range of [0, 3]. I.e. the agent can take one of four actions: up (<code>0</code>), right (<code>1</code>), down (<code>2</code>), or left (<code>3</code>). If an invalid action is taken, or an action is blocked by a wall, a no-op is performed and the agent's position remains unchanged.</p>"},{"location":"environments/maze/#reward","title":"Reward","text":"<p>Maze is a sparse reward problem, where the agent receives a reward of 0 at every step and a reward of 1 for reaching the target position. An episode ends when the agent reaches the target position, or after a set number of steps (by default, this is twice the number of cells in the maze, i.e. <code>step_limit=2*num_rows*num_cols</code>).</p>"},{"location":"environments/maze/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Maze-v0</code>, maze with 10 rows and 10 cols.</li> </ul>"},{"location":"environments/minesweeper/","title":"Minesweeper Environment","text":"<p>We provide here a Jax JIT-able implementation of the Minesweeper game.</p>"},{"location":"environments/minesweeper/#observation","title":"Observation","text":"<p>The observation given to the agent consists of:</p> <ul> <li> <p><code>board</code>: jax array (int32) of shape <code>(num_rows, num_cols)</code>:     each cell contains <code>-1</code> if not yet explored, or otherwise the number of mines in     the 8 adjacent squares.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_rows, num_cols)</code>:     indicates which actions are valid (not yet explored squares). This can also be determined from     the board which will have an entry of <code>-1</code> in all of these positions.</p> </li> <li> <p><code>num_mines</code>: jax array (int32) of shape <code>()</code>, indicates the number of mines to locate.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>:     specifies how many timesteps have elapsed since environment reset.</p> </li> </ul>"},{"location":"environments/minesweeper/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> of integer values representing coordinates of the square to explore, e.g. <code>[3, 6]</code> for the cell located on the third row and sixth column. If either a mined square or an already explored square is selected, the episode terminates (the latter are termed invalid actions).</p> <p>Also, exploring a square will reveal only the contents of that square. This differs slightly from the usual implementation of the game, which automatically and recursively reveals neighbouring squares if there are no adjacent mines.</p>"},{"location":"environments/minesweeper/#reward","title":"Reward","text":"<p>The reward is configurable, but default to <code>+1</code> for exploring a new square that does not contain a mine, and <code>0</code> otherwise (which also terminates the episode). The episode also terminates if the board is solved.</p>"},{"location":"environments/minesweeper/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Minesweeper-v0</code>, the classic game on a 10x10 grid with 10 mines to locate.</li> </ul>"},{"location":"environments/mmst/","title":"MMST Environment","text":"<p>The multi minimum spanning tree (mmst) environment consists of a random connected graph with groups of nodes (same node types) that needs to be connected. The goal of the environment is to connect all nodes of the same type together without using the same utility nodes (nodes that do not belong to any group of nodes) in the shortest time possible.</p> <p>An episode ends when all group of nodes are connected or the maximum number of steps is reached.</p> <p>Note:</p> <p>This environment can be treated as a multi agent problem with each agent atempting to connect one group of node. In this implementation, we treat the problem as single agent that outputs multiple actions per nodes.</p>"},{"location":"environments/mmst/#observation","title":"Observation","text":"<p>At each step observation contains 4 items: a node_types, an adjacency matrix for the graph, an action mask for each group of nodes (agent) and current node positon of each agent.</p> <ul> <li> <p><code>node_types</code>: Array representing the types of nodes in the problem.         For example, if we have 12 nodes, their indices are 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.         Let's consider we have 2 agents. Agent 0 wants to connect nodes (0, 1, 9),         and agent 1 wants to connect nodes (3, 5, 8).         The remaining nodes are considered utility nodes.         Therefore, in the state view, the node_types are         represented as [0, 0, -1, 1, -1, 1, 0, -1, 1, 0, -1, -1].         When generating the problem, each agent starts from one of its nodes.         So, if agent 0 starts on node 1 and agent 1 on node 3,         the connected_nodes array will have values [1, -1, ...] and [3, -1, ...] respectively.         The agent's observation is represented using the following rules:         - Each agent should see its connected nodes on the path as 0.         - Nodes that the agent still needs to connect are represented as 1.         - The next agent's nodes are represented by 2 and 3, the next by 4 and 5, and so on.         - Utility unconnected nodes are represented by -1.         For the 12 node example mentioned above,         the expected observation view node_types will have the following values:         node_types = jnp.array(             [                 [1, 0, -1, 2, -1, 3, 1, -1, 3, 1, -1, -1],                 [3, 2, -1, 0, -1, 1, 3, -1, 1, 3, -1, -1],             ],             dtype=jnp.int32,         )         Note: to make the environment single agent, we use the first agent's observation.</p> </li> <li> <p><code>adj_matrix</code>: Adjacency matrix representing the connections between nodes.</p> </li> <li> <p><code>positions</code>: Current node positions of the agents.         In our current problem, this will be represented as jnp.array([1, 3]).</p> </li> <li> <p><code>step_count</code>: integer to keep track of the number of steps.</p> </li> <li> <p><code>action_mask</code>: Binary mask indicating the validity of each action.         Given the current node on which the agent is located,         this mask determines if there is a valid edge to every other node.</p> </li> </ul>"},{"location":"environments/mmst/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> of shape <code>(num_agents,)</code> of integer values in the range of <code>[0, num_nodes-1]</code>. During every step, an agent picks the next node it wants to move to. An action is invalid if the agent picks a node it has no edge to or the node is a utility node already been used by another agent.</p>"},{"location":"environments/mmst/#reward","title":"Reward","text":"<p>At every step, an agent receives a reward of 10.0 if it gets a valid connection, a reward of -1.0 if it does not connect and an extra penalty of -1.0 if it chooses an invalid action.</p> <p>The total step reward is the sum of rewards per agent.</p>"},{"location":"environments/mmst/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>MMST-v0</code>, 3 agents, 36 nodes, 72 edges, 4 nodes to connect per agent and step limit of 70.</li> </ul>"},{"location":"environments/multi_cvrp/","title":"Multi Agent Capacitated Vehicle Routing Problem - <code>MultiCVRP</code> Environment","text":"<p>We provide here a Jax JIT-able implementation of the multi-agent capacitated vehicle routing problem (MultiCVRP) which is specified in MVRPSTW.</p> <p>This environment introduces the problem of routing multiple agents in a coordinated manner, specifically in the context of collecting items from various locations. Each agent controls one vehicle. The problem, called the multi-agent capacitated vehicle routing problem (MultiCVRP), entails directing a group of agents to different locations on a map. They need to collectively go to each node and return items to the depot location. To make the problem a bit more realistic we consider the multi-vehicle routing problem with soft time windows (MVRPSTW). In this formulation, each location on the map also has a soft time window in which the items must be collected. If the items are collected outside this window a penalty is provided to the agents.</p> <p>A new problem instance is generated by resetting the environment. The problem instance contains coordinates for each node sampled from a uniform distribution inside the map boundries, and each node (except for depot) has a specific demand which is an integer value sampled from a uniform distribution between 1 and the maximum demand. The number of nodes with demand is a parameter of the environment.</p>"},{"location":"environments/multi_cvrp/#observation","title":"Observation","text":"<p>Each agent receives information on the coordinates, demands, time windows and penalty coefficients of all the customer nodes. Futhermore the agents receive positions, local times and vehicle capacity information on all vehicles. Lastly an action mask is also provided to each agent.</p> <ul> <li><code>node_coordinates</code>: jax array (float32) of shape <code>(num_vehicles, num_customers + 1, 2)</code>, shows an array of the coordinates of each customer node and the depot node.</li> <li><code>node_demands</code>: jax array (int16) of shape <code>(num_vehicles, num_customers + 1,)</code>, shows an array of the demands of each city node (and depot node where the demand is set to 0).</li> <li><code>node_time_windows</code>: jax array (float32) of shape <code>(num_vehicles, num_customers + 1, 2)</code>, shows an array of the early and late time cutoffs for each customer.</li> <li><code>node_penalty_coefs</code>: jax array (float32) of shape <code>(num_vehicles, num_customers + 1, 2)</code>, shows the early and late penalty coefficients for arriving early or late at a customer's location.</li> <li><code>other_vehicles_position</code>: jax array (int16) of shape <code>(num_vehicles, num_vehicles - 1)</code>, shows the positions of all other vehicles.</li> <li><code>other_vehicles_local_times</code>: jax array (float32) of shape <code>(num_vehicles, num_vehicles - 1)</code>, shows the local times of all other vehicles.</li> <li><code>other_vehicles_capacities</code>: jax array (int16) of shape <code>(num_vehicles, num_vehicles - 1)</code>, shows the capacities of all other vehicles.</li> <li><code>vehicle_position</code>: jax array (int16) of shape <code>(num_vehicles)</code>, shows the positions of the vehicles controlled by the agents.</li> <li><code>vehicle_local_time</code>: jax array (float32) of shape <code>(num_vehicles)</code>, shows the local times of the vehicles controlled by the agents.</li> <li><code>vehicle_capacity</code>: jax array (int16) of shape <code>(num_vehicles)</code>, shows the capacity of the vehicles controlled by the agents.</li> <li><code>action_mask</code>: jax array (bool) of shape <code>(num_vehicles, num_customers + 1,)</code>, denoting which actions are possible (True) and which are not (False).</li> </ul>"},{"location":"environments/multi_cvrp/#action","title":"Action","text":"<p>Each agent's action space is a <code>BoundedArray</code> of integer values in the range of <code>[0, num_customers]</code>. An action is the index of the next node to visit, and an action value of 0 corresponds to visiting the depot.</p>"},{"location":"environments/multi_cvrp/#reward","title":"Reward","text":"<ul> <li>Dense: The reward is equal to the sum of negative distances of the current location and next location of all the vehicles. Time penalities are added if the agents arrived early or late to specific customers. If the max step limit is reached, the episode ends with a large negative reward which is equal to the maximum negative distance reward that can be incurred.</li> <li>Sparse: The reward is 0 at every step but the last, where the reward is the negative of the length of the path chosen by all the agents combined. Time penalities are added if the agents arrived early or late to specific customers. If the max step limit is reached, the episode ends with a large negative reward which is equal to the maximum negative distance reward that can be incurred.</li> </ul>"},{"location":"environments/multi_cvrp/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>MultiCVRP-v0</code>: MultiCVRP problem with 20 customers (randomly generated), maximum capacity of 20, and maximum demand of 10 with two vehicles.</li> </ul>"},{"location":"environments/pac_man/","title":"PacMan Environment","text":"<p>We provide here a minimal Jax JIT-able implementation of the game PAC-MAN. The game is played in a 2D matrix where a cell is a free space (black), a wall (dark blue), pacman (yellow) or a ghost.</p> <p>The goal is for the agent (yellow) to collect all of the pellets (small pink blocks) on the map without touching any of the ghosts. The agent receives a reward of +10 when collecting a pellet for the first time and pellets are removed from the map after being collected.</p> <p>The power-ups (large pink blocks) trigger a 'scatter mode' which changes the colour of the ghosts to dark blue for 30 in game steps. When the ghosts are in this state, the player can touch them which causes them to return to the center of the map. This gives a reward of +200 for each unique ghost.</p> <p>The agent selects an action at each timestep (up, left, right, down, no-op) which determines the direction they wil travel for that step. However, even if an action is in an invalid direction it will still be taken as input and the player will remain stationary. If the no-op action is used the player will not stop but instead take the last action that was selected.</p> <p>The game takes place on a fixed map and the same map is generated on each reset. The generator can be used to generate new maps based on an ASCII representation of the desired map. This ASCII generator is deterministic and will always initialise to the same state as long as the same ASCII diagram is is use.</p>"},{"location":"environments/pac_man/#observation","title":"Observation","text":"<p>As an observation, the agent has access to the current maze configuration in the array named <code>grid</code>. It also has access to its current position <code>player_locations</code>, the ghosts' locations <code>ghost_locations</code>, the power-pellet locations <code>power_up_location</code>, the time left for the scatter state <code>frightened_state_time</code>, the pellet locations <code>pellet_locations</code> and the action mask <code>action_mask</code>.</p> <ul> <li> <p><code>agent_position</code>: Position(row, col) (int32) each of shape <code>()</code>, agent position in the maze.</p> </li> <li> <p><code>ghost_locations</code>: jax array (int32) of shape <code>(4,2)</code>, with the (y,x) coordinates of each ghost</p> </li> <li> <p><code>power_up_locations</code>: jax array (int32) of shape <code>(4,2)</code>, with the (y,x) coordinates of each power-pellet</p> </li> <li> <p><code>pellet_locations</code>: jax array (int32) of shape <code>(4,2)</code>, with the (y,x) coordinates of each pellet</p> </li> <li> <p><code>frightened_state_time</code>: jax array (int32) of shape <code>()</code>, number of steps left of the scatter state.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(5,)</code>, binary values denoting whether each action is possible.</p> </li> <li><code>frightened_state_time</code>: (int32) tracking the number of steps for the scatter state.</li> <li><code>score</code>: (int32) tracking the total points accumulated since the last reset.</li> </ul> <p>An example 5x5 observation <code>grid</code> array, is shown below. 1 represents a wall, and 0 represents free space.</p> <pre><code>[0, 1, 0, 0, 0],\n[0, 1, 0, 1, 1],\n[0, 1, 0, 0, 0],\n[0, 0, 0, 1, 1],\n[0, 0, 0, 0, 0]\n</code></pre>"},{"location":"environments/pac_man/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in the range of [0, 4]. I.e. the agent can take one of four actions: up (<code>0</code>), right (<code>1</code>), down (<code>2</code>), left (<code>3</code>) or no-op (<code>4</code>). If an invalid action is taken, or an action is blocked by a wall, a no-op is performed and the agent's position remains unchanged. Additionally if a no-op is performed the agent will use the last normal action used.</p>"},{"location":"environments/pac_man/#reward","title":"Reward","text":"<p>PacMan is a dense reward setting, where the agent receives a reward of +10 for each pellet collected. The agent also recieve a reward of 20 for collecting a power pellet. The game ends when the agent has collected all 316 pellets on the map or touches a ghost.</p> <p>Eating a ghost when scatter mode is enabled also awards +200 points but, points are only awarded the first time each unique ghost is eaten.</p>"},{"location":"environments/pac_man/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>PacMan-v1</code>, PacMan in a 31x28 map with simple grid observations.</li> </ul>"},{"location":"environments/robot_warehouse/","title":"RobotWarehouse Environment","text":"<p>We provide a JAX jit-able implementation of the Robotic Warehouse environment.</p> <p>The Robot Warehouse (RWARE) environment simulates a warehouse with robots moving and delivering requested goods. Real-world applications inspire the simulator, in which robots pick up shelves and deliver them to a workstation. Humans access the content of a shelf, and then robots can return them to empty shelf locations.</p> <p>The goal is to successfully deliver as many requested shelves in a given time budget.</p> <p>Once a shelf has been delivered, a new shelf is requested at random. Agents start each episode at random locations within the warehouse.</p>"},{"location":"environments/robot_warehouse/#observation","title":"Observation","text":"<p>The observation seen by the agent is a <code>NamedTuple</code> containing the following:</p> <ul> <li> <p><code>agents_view</code>: jax array (int32) of shape <code>(num_agents, num_obs_features)</code>, array representing the agent's view of other agents     and shelves.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_agents, 5)</code>, array specifying, for each agent,     which action (noop, forward, left, right, toggle_load) is legal.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, number of steps elapsed in the current episode.</p> </li> </ul>"},{"location":"environments/robot_warehouse/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> containing an integer value in <code>[0, 1, 2, 3, 4]</code> for each agent. Each agent can take one of five actions: noop (<code>0</code>), forward (<code>1</code>), turn left (<code>2</code>), turn right (<code>3</code>), or toggle_load (<code>4</code>).</p> <p>The episode terminates under the following conditions:</p> <ul> <li> <p>An invalid action is taken, or</p> </li> <li> <p>An agent collides with another agent.</p> </li> </ul>"},{"location":"environments/robot_warehouse/#reward","title":"Reward","text":"<p>The reward is global and shared among the agents. It is equal to the number of shelves which were delivered successfully during the time step (i.e., +1 for each shelf).</p>"},{"location":"environments/robot_warehouse/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>RobotWarehouse-v0</code>, a warehouse with 4 agents each with a sensor range of 1, a warehouse floor with 2 shelf rows, 3 shelf columns, a column height of 8, and a shelf request queue of 8.</li> </ul>"},{"location":"environments/rubiks_cube/","title":"Rubik's Cube Environment","text":"<p>We provide here a Jax JIT-able implementation of the Rubik's cube. The environment contains an implementation of the classic 3x3x3 cube by default, and configurably other sizes. The goal of the agent is to match all stickers on each face to a single colour. On resetting the environment the cube will be randomly scrambled with a configurable number of turns (by default 100).</p>"},{"location":"environments/rubiks_cube/#observation","title":"Observation","text":"<p>The observation given to the agent gives a view of the current state of the cube,</p> <ul> <li> <p><code>cube</code>: jax array (int8) of shape <code>(6, cube_size, cube_size)</code> whose values are in     <code>[0, 1, 2, 3, 4, 5]</code> (corresponding to the different sticker colors). The indices of the array     specify the sticker position - first the face (in the order up, front, right, back,     left, down) and then the row and column. Note that the orientation of each face is as     follows:</p> <ul> <li> <p>UP: LEFT face on the left and BACK face pointing up</p> </li> <li> <p>FRONT: LEFT face on the left and UP face pointing up</p> </li> <li> <p>RIGHT: FRONT face on the left and UP face pointing up</p> </li> <li> <p>BACK: RIGHT face on the left and UP face pointing up</p> </li> <li> <p>LEFT: BACK face on the left and UP face pointing up</p> </li> <li> <p>DOWN: LEFT face on the left and FRONT face pointing up</p> </li> </ul> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, representing the number of steps in the episode thus far.</p> </li> </ul>"},{"location":"environments/rubiks_cube/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code>, specifically a tuple of an index between 0 and 5 (since there are 6 faces), an index between 0 and <code>cube_size//2</code> (the number of possible depths), and an index between 0 and 2 (3 possible directions). An action thus consists of three pieces of information:</p> <ul> <li> <p>Face to turn,</p> </li> <li> <p>Depth of the turn (possible depths are between <code>0</code> representing the outer layer and <code>cube_size//2</code> representing the layer closest to the middle),</p> </li> <li> <p>Direction of turn (possible directions are clockwise, anti-clockwise, or a half turn).</p> </li> </ul>"},{"location":"environments/rubiks_cube/#reward","title":"Reward","text":"<p>The reward function is configurable, but by default is the fully sparse reward giving <code>+1</code> for solving the cube and otherwise <code>0</code>. The episode terminates if either the cube is solved or a configurable horizon (by default <code>200</code>) is reached.</p>"},{"location":"environments/rubiks_cube/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li> <p><code>RubiksCube-v0</code>, the standard Rubik's Cube puzzle with faces of size 3x3.</p> </li> <li> <p><code>RubiksCube-partly-scrambled-v0</code>, an easier version of the standard Rubik's Cube puzzle with faces of size 3x3 yet only 7 scrambles at reset time, making it technically maximum 7 actions away from the solution.</p> </li> </ul>"},{"location":"environments/search_and_rescue/","title":"\ud83d\ude81 Search &amp; Rescue","text":"<p>Multi-agent environment, modelling a group of agents searching a 2d environment for multiple targets. Agents are individually rewarded for finding a target that has not previously been detected.</p> <p>Each agent visualises a local region around itself, represented as a simple segmented view of locations of other agents and targets in the vicinity. The environment is updated in the following sequence:</p> <ul> <li>The velocity of searching agents are updated, and consequently their positions.</li> <li>The positions of targets are updated.</li> <li>Targets within detection range, and within an agents view cone are marked as found.</li> <li>Agents are rewarded for locating previously unfound targets.</li> <li>Local views of the environment are generated for each searching agent.</li> </ul> <p>The agents are allotted a fixed number of steps to locate the targets. The search space is a uniform square space, wrapped at the boundaries.</p> <p>Many aspects of the environment can be customised:</p> <ul> <li>Agent observations can be customised by implementing the <code>ObservationFn</code> interface.</li> <li>Rewards can be customised by implementing the <code>RewardFn</code> interface.</li> <li>Target dynamics can be customised to model various search scenarios by implementing the   <code>TargetDynamics</code> interface.</li> </ul>"},{"location":"environments/search_and_rescue/#observations","title":"Observations","text":"<ul> <li><code>searcher_views</code>: jax array (float) of shape <code>(num_searchers, channels, num_vision)</code>.   Each agent generates an independent observation, an array of values representing the distance   along a ray from the agent to the nearest neighbour or target, with  each cell representing a   ray angle (with <code>num_vision</code> rays evenly distributed over the agents field of vision).   For example if an agent sees another agent straight ahead and <code>num_vision = 5</code> then   the observation array could be</li> </ul> <pre><code>[-1.0, -1.0, 0.5, -1.0, -1.0]\n</code></pre> <p>where <code>-1.0</code> indicates there are no agents along that ray, and <code>0.5</code> is the normalised   distance to the other agent. Channels in the segmented view are used to differentiate   between different agents/targets and can be customised. By default, the view has three   channels representing other agents, found targets, and unlocated targets respectively. - <code>targets_remaining</code>: float in the range <code>[0, 1]</code>. The normalised number of targets   remaining to be detected (i.e. 1.0 when no targets have been found). - <code>step</code>: int in the range <code>[0, time_limit]</code>. The current simulation step. - <code>positions</code>: jax array (float) of shape <code>(num_searchers, 2)</code>. Agent coordinates.</p>"},{"location":"environments/search_and_rescue/#actions","title":"Actions","text":"<p>Jax array (float) of <code>(num_searchers, 2)</code> in the range <code>[-1, 1]</code>. Each entry in the array represents an update of each agents velocity in the next step. Searching agents update their velocity each step by rotating and accelerating/decelerating, where the values are <code>[rotation, acceleration]</code>. Values are clipped to the range <code>[-1, 1]</code> and then scaled by max rotation and acceleration parameters, i.e. the new values each step are given by</p> <pre><code>heading = heading + max_rotation * action[0]\n</code></pre> <p>and speed</p> <pre><code>speed = speed + max_acceleration * action[1]\n</code></pre> <p>Once applied, agent speeds are clipped to velocities within a fixed range of speeds given by the <code>min_speed</code> and <code>max_speed</code> parameters.</p>"},{"location":"environments/search_and_rescue/#rewards","title":"Rewards","text":"<p>Jax array (float) of <code>(num_searchers,)</code>. Rewards are generated for each agent individually.</p> <p>Agents are rewarded +1 for locating a target that has not already been detected. It is possible for multiple agents to newly detect the same target inside a step. By default, the reward is split between the locating agents if this is the case. By default, rewards granted linearly decrease over time, from +1 to 0 at the final step. The reward function can be customised by implementing the <code>RewardFn</code> interface.</p>"},{"location":"environments/sliding_tile_puzzle/","title":"Sliding Tile Puzzle Environment","text":"<p>This is a Jax JIT-able implementation of the classic Sliding Tile Puzzle game.</p> <p>The Sliding Tile Puzzle game is a classic puzzle that challenges a player to slide (typically flat) pieces along certain routes (usually on a board) to establish a certain end-configuration. The pieces to be moved may consist of simple shapes, or they may be imprinted with colors, patterns, sections of a larger picture (like a jigsaw puzzle), numbers, or letters.</p> <p>The puzzle is often 3\u00d73, 4\u00d74 or 5\u00d75 in size and made up of square tiles that are slid into a square base, larger than the tiles by one tile space, in a specific large configuration. Tiles are moved/arranged by sliding an adjacent tile into a position occupied by the missing tile, which creates a new space. The sliding puzzle is mechanical and requires the use of no other equipment or tools.</p>"},{"location":"environments/sliding_tile_puzzle/#observation","title":"Observation","text":"<p>The observation in the Sliding Tile Puzzle game includes information about the puzzle, the position of the empty tile, and the action mask.</p> <ul> <li> <p><code>puzzle</code>: jax array (int32) of shape <code>(grid_size, grid_size)</code>, representing the current game state. Each element in the array corresponds to a puzzle tile. The tile represented by 0 is the empty tile.</p> </li> <li> <p>Here is an example of a random observation of the game board:</p> <pre><code>```\n[[ 1 2 3 4]\n [ 5 6 7 8]\n [ 9 10 0 12]\n [ 13 14 15 11]]\n```\n</code></pre> <ul> <li>In this array, the tile represented by 0 is the empty tile that can be moved.</li> </ul> </li> <li> <p><code>empty_tile_position</code>: a tuple (int32) of shape <code>(2,)</code> representing the position of the empty tile in the grid. For example, (2, 2) would represent the third row and the third column in a zero-indexed grid.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(4,)</code>, indicating which actions are valid in the current state of the environment. The actions include moving the empty tile up, right, down, or left. For example, an action mask <code>[True, False, True, False]</code> means that the valid actions are to move the empty tile upward or downward.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, current number of steps in the episode.</p> </li> </ul>"},{"location":"environments/sliding_tile_puzzle/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in <code>[0, 1, 2, 3]</code>. Specifically, these four actions correspond to moving the empty tile: up (0), right (1), down (2), or left (3).</p>"},{"location":"environments/sliding_tile_puzzle/#reward","title":"Reward","text":"<p>The reward could be either:</p> <ul> <li> <p>DenseRewardFn: This reward function provides a dense reward based on the difference of correctly placed tiles between the current state and the next state. The reward is positive for each newly correctly placed tile and negative for each newly incorrectly placed tile.</p> </li> <li> <p>SparseRewardFn: This reward function provides a sparse reward, only rewarding when the puzzle is solved. The reward is 1 if the puzzle is solved, and 0 otherwise.</p> </li> </ul> <p>The goal in all cases is to solve the puzzle in a way that maximizes the reward.</p>"},{"location":"environments/sliding_tile_puzzle/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>SlidingTilePuzzle-v0</code>, the Sliding Tile Puzzle with a grid size of 5x5.</li> </ul>"},{"location":"environments/snake/","title":"Snake Environment \ud83d\udc0d","text":"<p>We provide here an implementation of the Snake environment from (Bonnet et al., 2021). The goal of the agent is to navigate in a grid world (by default of size 12x12) to collect as many fruits as possible without colliding with its own body (i.e. looping on itself).</p>"},{"location":"environments/snake/#observation","title":"Observation","text":"<ul> <li> <p><code>grid</code>: jax array (float) of shape <code>(num_rows, num_cols, 5)</code>, feature maps (image) that include     information about the fruit, the snake head, its body and tail.</p> </li> <li> <p><code>step_count</code>: jax array (int32) of shape <code>()</code>, current number of steps in the episode.</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(4,)</code>, array specifying which directions the snake can     move in from its current position.</p> </li> </ul>"},{"location":"environments/snake/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values: <code>[0,1,2,3]</code> -&gt; <code>[Up, Right, Down, Left]</code>.</p>"},{"location":"environments/snake/#reward","title":"Reward","text":"<p>The reward is <code>+1</code> upon collection of a fruit and <code>0</code> otherwise.</p>"},{"location":"environments/snake/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Snake-v1</code>: Snake game on a board of size 12x12 with a time limit of <code>4000</code>.</li> </ul>"},{"location":"environments/sokoban/","title":"Sokoban Environment \ud83d\udc7e","text":"<p>This is a Jax implementation of the Sokoban puzzle, a dynamic box-pushing environment where the agent's goal is to place all boxes on their targets. This version follows the rules from the DeepMind paper on Imagination Augmented Agents, with levels based on the Boxoban dataset from Guez et al., 2018[1]. The graphical assets were taken from gym-sokoban by Schrader, a diverse Sokoban library implementing many versions of the game in the OpenAI gym framework [2].</p>"},{"location":"environments/sokoban/#observation","title":"Observation","text":"<ul> <li><code>grid</code>: An Array (uint8) of shape <code>(10, 10, 2)</code>. It represents the variable grid (containing movable objects: boxes and the agent) and the fixed grid (containing fixed objects: walls and targets).</li> <li><code>step_count</code>: An Array (int32) of shape <code>()</code>, representing the current number of steps in the episode.</li> </ul>"},{"location":"environments/sokoban/#object-encodings","title":"Object Encodings","text":"Object Encoding Empty Space 0 Wall 1 Target 2 Agent 3 Box 4"},{"location":"environments/sokoban/#actions","title":"Actions","text":"<p>The agent's action space is an Array (int32) with potential values of <code>[0,1,2,3]</code> (corresponding to <code>[Up, Down, Left, Right]</code>). If the agent attempts to move into a wall, off the grid, or push a box into a wall or off the grid, the grid state remains unchanged; however, the step count is incremented by one. Chained box pushes are not allowed and will result in no action.</p>"},{"location":"environments/sokoban/#reward","title":"Reward","text":"<p>The reward function comprises: - <code>-0.1</code> for each step taken in the environment. - <code>+1</code> for each box moved onto a target location and <code>-1</code> for each box moved off a target location. - <code>+10</code> upon successful placement of all four boxes on their targets.</p>"},{"location":"environments/sokoban/#episode-termination","title":"Episode Termination","text":"<p>The episode concludes when: - The step limit of 120 is reached. - All 4 boxes are placed on targets (i.e., the problem is solved).</p>"},{"location":"environments/sokoban/#dataset","title":"Dataset","text":"<p>The Boxoban dataset offers a collection of puzzle levels. Each level features four boxes and four targets. The dataset has three levels of difficulty: 'unfiltered', 'medium', and 'hard'.</p> Dataset Split Number of Levels Unfiltered (Training) 900,000 Unfiltered (Validation) 100,000 Unfiltered (Test) 1000 Medium (Training) 450,000 Medium (Validation) 50,000 Hard 3332 <p>The dataset generation procedure and more details can be found in Guez et al., 2018 [1].</p>"},{"location":"environments/sokoban/#graphics","title":"Graphics","text":"Type Graphic Wall Floor Target Box on Target Box Off Target Agent Off Target Agent On Target"},{"location":"environments/sokoban/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Sokoban-v0</code>: Sokoban game with levels generated using DeepMind Boxoban dataset (unfiltered train).</li> </ul>"},{"location":"environments/sokoban/#references","title":"References","text":"<p>[1] Guez, A., Mirza, M., Gregor, K., Kabra, R., Racaniere, S., Weber, T., Raposo, D., Santoro, A., Orseau, L., Eccles, T., Wayne, G., Silver, D., Lillicrap, T., Valdes, V. (2018). An investigation of Model-free planning: boxoban levels. Available at https://github.com/deepmind/boxoban-levels</p> <p>[2] Schrader, M. (2018). Gym-sokoban. Available at https://github.com/mpSchrader/gym-sokoban</p>"},{"location":"environments/sudoku/","title":"Sudoku Environment","text":"<p>We provide here a Jax JIT-able implementation of the Sudoku puzzle game.</p>"},{"location":"environments/sudoku/#observation","title":"Observation","text":"<p>The observation given to the agent consists of:</p> <ul> <li><code>board</code>: jax array (int32) of shape (9,9):     empty cells are represented by -1, and filled cells are represented by 0-8.</li> <li><code>action_mask</code>: jax array (bool) of shape (9,9,9):     indicates which actions are valid.</li> </ul>"},{"location":"environments/sudoku/#action","title":"Action","text":"<p>The action space is a <code>MultiDiscreteArray</code> of integer values representing coordinates of the square to explore and the digits to write in the cell, e.g. <code>[3, 6, 8]</code> for writing the digit <code>9</code> in the cell located on the fourth row and seventh column.</p>"},{"location":"environments/sudoku/#reward","title":"Reward","text":"<p>The reward is <code>1</code> at the end of the episode if the board is correctly solved, and <code>0</code> in every other case.</p>"},{"location":"environments/sudoku/#termination","title":"Termination","text":"<p>An episode terminates when there are no more legal actions available, this could happen if the board is solved or if the agent finds itself in a dead-end.</p>"},{"location":"environments/sudoku/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Sudoku-v0</code>, the classic game on a 9x9 grid, 10000 random puzzles with mixed difficulty are included by default.</li> <li><code>Sudoku-very-easy-v0</code>, the classic game on a 9x9 grid, only 1000 very-easy random puzzles (&gt;46 clues) included by default.</li> </ul>"},{"location":"environments/sudoku/#using-custom-puzzle-instances","title":"Using custom puzzle instances","text":"<p>If one wants to include its own database of puzzles, the <code>DatabaseGenerator</code> can be initialized with any collection of puzzles using the argument <code>custom_boards</code>. Some references for databases of puzzle of various difficulties:  - https://www.kaggle.com/datasets/rohanrao/sudoku  - https://www.kaggle.com/datasets/informoney/4-million-sudoku-puzzles-easytohard</p>"},{"location":"environments/sudoku/#difficulty-level-as-a-function-of-number-of-clues","title":"Difficulty level as a function of number of clues","text":"<p> Adapted from An Algorithm for Generating only Desired Permutations for Solving Sudoku Puzzle.</p>"},{"location":"environments/tetris/","title":"Tetris Environment","text":"<p>We provide here a Jax JIT-able implementation of the game Tetris. Tetris is a popular single-player game that is played on a 2D grid by fitting falling blocks of various Tetrominoes together to create horizontal lines without any gaps. As each line is completed, it disappears, and the player earns points. If the stack of blocks reaches the top of the game grid, the game ends. The objective of Tetris is to score as many points as possible before the game ends, by clearing as many lines as possible. Tetris consists of 7 types of Tetrominoes, which are shapes that represent the letters \"I\", \"O\", \"S\", \"Z\", \"L\", \"J\", and \"T\" as shown in the image below.</p> <p> </p>"},{"location":"environments/tetris/#observation","title":"Observation","text":"<p>The observation in Tetris includes information about the grid, the Tetromino and the action mask.</p> <ul> <li> <p><code>grid</code>: jax array (int32) of shape <code>(num_rows, num_cols)</code>, representing the current grid    state. The grid is filled with zeros for the empty cells and with ones for the filled cells.</p> </li> <li> <p>Here is an example of a random observation of the grid:        <pre><code>[\n    [0, 0, 0, 0, 0, 1],\n    [0, 0, 0, 0, 1, 1],\n    [0, 0, 0, 0, 1, 1],\n    [0, 1, 0, 0, 1, 1],\n    [0, 1, 1, 1, 0, 1],\n    [0, 1, 0, 1, 1, 1],\n    [1, 1, 0, 1, 1, 1],\n]\n</code></pre></p> </li> <li> <p><code>tetromino</code>: jax array (int32) of shape <code>(4, 4)</code>, where a value of 1 indicates a filled cell and a value of 0 indicates an empty cell.</p> <ul> <li>Here is an example of an I tetromino:     <pre><code>[\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n    [1, 0, 0, 0],\n]\n</code></pre></li> <li><code>action_mask</code>: jax array (bool) of shape <code>(4, num_cols)</code>, indicating which actions are valid in the current state of the environment. Each row in the action mask corresponds to a Tetromino for a certain rotation (example: the first row for 0 degrees rotation, the second row for 90 degrees rotation, and so on).</li> <li> <p>Here is an example of an action mask that corresponds to the same grid and the tetromino examples:</p> <p><pre><code>[\n    [ True,  False,  True,  True, False, False],\n    [ True,  True, False,  False,  False, False],\n    [ True,  False,  True,  True, False, False],\n    [ True,  True, False,  False,  False, False],\n]\n</code></pre> - <code>step_count</code>: jax array (int32) of shape <code>()</code>, integer to keep track of the number of steps.</p> </li> </ul> </li> </ul>"},{"location":"environments/tetris/#action","title":"Action","text":"<p>The action space in Tetris is represented as a <code>MultiDiscreteArray</code> of two integer values. The first integer value corresponds to the selected X-position where the Tetromino will be placed, and the second integer value represents the index for the rotation degree. The rotation degree index can take four possible values: 0 for \"0 degrees\", 1 for \"90 degrees\", 2 for \"180 degrees\", and 3 for \"270 degrees\". For example, an action of [7, 2] means placing the Tetromino in the seventh column with a rotation of 180 degrees.</p>"},{"location":"environments/tetris/#reward","title":"Reward","text":"<p>Dense: the reward is based on the number of lines cleared and the reward_list <code>[0, 40, 100, 300, 1200]</code>. If no lines are cleared, the reward is 0. As the number of cleared lines increases, so does the reward, with the maximum reward of 1200 being awarded for clearing four lines at once.</p>"},{"location":"environments/tetris/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>Tetris-v0</code>, the default settings for tetris with a grid of size 10x10.</li> </ul>"},{"location":"environments/tsp/","title":"Traveling Salesman Problem (TSP) Environment","text":"<p>We provide here a Jax JIT-able implementation of the traveling salesman problem (TSP).</p> <p>TSP is a well-known combinatorial optimization problem. Given a set of cities and the distances between them, the goal is to determine the shortest route that visits each city exactly once and finishes in the starting city. The problem is NP-complete, thus there is no known algorithm both correct and fast (i.e., that runs in polynomial time) for any instance of the problem.</p> <p>When the environment is reset, a new problem instance is generated by sampling coordinates (a pair for each city) from a uniform distribution between 0 and 1. The number of cities is a parameter of the environment. A trajectory terminates when no new cities can be visited or the last action was invalid (i.e., the agent attempted to revisit a city).</p>"},{"location":"environments/tsp/#observation","title":"Observation","text":"<p>The observation given to the agent provides information on the problem layout, the visited/unvisited cities and the current position (city) of the agent.</p> <ul> <li> <p><code>coordinates</code>: jax array (float) of shape <code>(num_cities, 2)</code>, array of coordinates of each city.</p> </li> <li> <p><code>position</code>: jax array (int32) of shape <code>()</code>, identifier (index) of the last visited city.</p> </li> <li> <p><code>trajectory</code>: jax array (int32) of shape <code>(num_cities,)</code>, city indices defining the route (<code>-1</code> --&gt; not filled yet).</p> </li> <li> <p><code>action_mask</code>: jax array (bool) of shape <code>(num_cities,)</code>, binary values denoting whether a city can be visited.</p> </li> </ul>"},{"location":"environments/tsp/#action","title":"Action","text":"<p>The action space is a <code>DiscreteArray</code> of integer values in the range of <code>[0, num_cities-1]</code>. An action is the index of the next city to visit.</p>"},{"location":"environments/tsp/#reward","title":"Reward","text":"<p>The reward could be either:</p> <ul> <li> <p>Dense: the negative distance between the current city and the chosen next city to go to.     It is 0 for the first chosen city, and for the last city, it also includes the distance     to the initial city to complete the tour.</p> </li> <li> <p>Sparse: the negative tour length at the end of the episode. The tour length is defined     as the sum of the distances between consecutive cities. It is computed by starting at     the first city and ending there, after visiting all the cities.</p> </li> </ul> <p>In both cases, the reward is a large negative penalty of <code>-num_cities * sqrt(2)</code> if the action is invalid, i.e. a previously selected city is selected again.</p>"},{"location":"environments/tsp/#registered-versions","title":"Registered Versions \ud83d\udcd6","text":"<ul> <li><code>TSP-v1</code>: TSP problem with 20 randomly generated cities and a dense reward function.</li> </ul>"},{"location":"guides/advanced_usage/","title":"Advanced Usage \ud83e\uddd1\u200d\ud83d\udd2c","text":"<p>Being written in JAX, Jumanji's environments benefit from many of its features including automatic vectorization/parallelization (<code>jax.vmap</code>, <code>jax.pmap</code>) and JIT-compilation (<code>jax.jit</code>), which can be composed arbitrarily. We provide an example of this below, where we use <code>jax.vmap</code> and <code>jax.lax.scan</code> to generate a batch of rollouts in the <code>Snake</code> environment.</p> <pre><code>import jax\n\nimport jumanji\nfrom jumanji.wrappers import AutoResetWrapper\n\nenv = jumanji.make(\"Snake-v1\")  # Create a Snake environment\nenv = AutoResetWrapper(env)     # Automatically reset the environment when an episode terminates\n\nbatch_size = 7\nrollout_length = 5\nnum_actions = env.action_spec.num_values\n\nrandom_key = jax.random.PRNGKey(0)\nkey1, key2 = jax.random.split(random_key)\n\ndef step_fn(state, key):\n  action = jax.random.randint(key=key, minval=0, maxval=num_actions, shape=())\n  new_state, timestep = env.step(state, action)\n  return new_state, timestep\n\ndef run_n_steps(state, key, n):\n  random_keys = jax.random.split(key, n)\n  state, rollout = jax.lax.scan(step_fn, state, random_keys)\n  return rollout\n\n# Instantiate a batch of environment states\nkeys = jax.random.split(key1, batch_size)\nstate, timestep = jax.vmap(env.reset)(keys)\n\n# Collect a batch of rollouts\nkeys = jax.random.split(key2, batch_size)\nrollout = jax.vmap(run_n_steps, in_axes=(0, 0, None))(state, keys, rollout_length)\n\n# Shape and type of given rollout:\n# TimeStep(step_type=(7, 5), reward=(7, 5), discount=(7, 5), observation=(7, 5, 6, 6, 5), extras=None)\n</code></pre>"},{"location":"guides/registration/","title":"Environment Registry","text":"<p>Jumanji adopts the convention defined in Gym of having an environment registry and a <code>make</code> function to instantiate environments.</p>"},{"location":"guides/registration/#create-an-environment","title":"Create an environment","text":"<p>To instantiate a Jumanji registered environment, we provide the convenient function <code>jumanji.make</code>. It can be used as follows:</p> <pre><code>import jax\nimport jumanji\n\nenv = jumanji.make('BinPack-v1')\nkey = jax.random.PRNGKey(0)\nstate, timestep = env.reset(key)\n</code></pre> <p>The environment ID is composed of two parts, the environment name and its version. To get the full list of registered environments, you can use the <code>registered_environments</code> util.</p> <p>\u26a0\ufe0f Warning</p> <pre><code>Users can provide additional key-word arguments in the call to `jumanji.make(env_id, ...)`.\nThese are then passed to the class constructor. Because they can be used to overwrite the\nintended configuration of the environment when registered, we discourage users to do so.\nHowever, we are mindful of particular use cases that might require this flexibility.\n</code></pre> <p>Although the <code>make</code> function provides a unified way to instantiate environments, users can always instantiate them by importing the corresponding environment class.</p>"},{"location":"guides/registration/#register-your-environment","title":"Register your environment","text":"<p>In addition to the environments available in Jumanji, users can register their custom environment and access them through the familiar <code>jumanji.make</code> function. Assuming you created an environment by subclassing Jumanji <code>Environment</code> base class, you can register it as follows:</p> <pre><code>from jumanji import register\n\nregister(\n    id=\"CustomEnv-v0\",                            # format: (env_name)-v(version)\n    entry_point=\"path.to.your.package:CustomEnv\", # class constructor\n    kwargs={...},                                 # environment configuration\n)\n</code></pre> <p>To successfully register your environment, make sure to provide the right path to your class constructor. The <code>kwargs</code> argument is there to configurate the environment and allow you to register scenarios with a specific set of arguments. The environment ID must respect the format <code>(EnvName)-v(version)</code>, where the version number starts at <code>v0</code>.</p> <p>For examples on how to register environments, please see our jumanji/__init__.py file.</p> <pre><code>Note that Jumanji doesn't allow users to overwrite the registration of an existing environment.\n</code></pre> <p>To verify that your custom environment has been registered correctly, you can inspect the listing of registered environments using the <code>registered_environments</code> util.</p>"},{"location":"guides/training/","title":"Training","text":"<p>Jumanji provides a training script train.py to train an online agent on a specified Jumanji environment given an environment-specific network.</p>"},{"location":"guides/training/#agents","title":"Agents","text":"<p>Jumanji provides two example agents in jumanji/training/agents/ to get you started with training on Jumanji environments:</p> <ul> <li> <p>Random agent: uses the action mask to randomly sample valid actions.</p> </li> <li> <p>A2C agent: online advantage actor-critic agent that follows from [Mnih et al., 2016].</p> </li> </ul>"},{"location":"guides/training/#configuration","title":"Configuration","text":"<p>In each environment-specific config YAML file, you will see a \"training\" section like below:</p> <p><pre><code>training:\n    num_epochs: 1000\n    num_learner_steps_per_epoch: 50\n    n_steps: 20\n    total_batch_size: 64\n</code></pre> Here,</p> <ul> <li> <p><code>num_epochs</code> corresponds to the number of data points in your plots. An epoch can be thought as an iteration.</p> </li> <li> <p><code>num_learner_steps_per_epoch</code> is the number of learner steps that happen in each epoch. After every learner step, the A2C agent's policy is updated.</p> </li> <li> <p><code>n_steps</code> is the sequence length (consecutive environment steps in a batch).</p> </li> <li> <p><code>total_batch_size</code> is the number of environments that are run in parallel.</p> </li> </ul> <p>So in the above example,</p> <ul> <li> <p>64 environments are running in parallel.</p> </li> <li> <p>Each of these 64 environments run 20 environment steps. After this, the agent's policy is updated via SGD. This constitutes a single learner step.</p> </li> <li> <p>50 such learner steps are done for the epoch in question. After this, evaluation is done using the updated policy.</p> </li> <li> <p>The above procedure is done for 1000 epochs.</p> </li> </ul>"},{"location":"guides/training/#evaluation","title":"Evaluation","text":"<p>Two types of evaluation are recorded:</p> <ul> <li> <p>Stochastic evaluation (same policy used during training)</p> </li> <li> <p>Greedy evaluation (argmax over the action logits)</p> </li> </ul>"},{"location":"guides/wrappers/","title":"Wrappers","text":"<p>The <code>Wrapper</code> interface is used for extending Jumanji <code>Environment</code> to add features like auto reset and vectorised environments. Jumanji provides wrappers to convert a Jumanji <code>Environment</code> to a DeepMind or Gym environment.</p>"},{"location":"guides/wrappers/#jumanji-to-deepmind-environment","title":"Jumanji to DeepMind Environment","text":"<p>We can also convert our Jumanji environments to a DeepMind environment: <pre><code>import jumanji.wrappers\n\nenv = jumanji.make(\"Snake-6x6-v0\")\ndm_env = jumanji.wrappers.JumanjiToDMEnvWrapper(env)\n\ntimestep = dm_env.reset()\naction = dm_env.action_spec.generate_value()\nnext_timestep = dm_env.step(action)\n...\n</code></pre></p>"},{"location":"guides/wrappers/#jumanji-to-gymnasium","title":"Jumanji To Gymnasium","text":"<p>We can also convert our Jumanji environments to a Gymnasium environment! Below is an example of how to convert a Jumanji environment into a Gymnasium environment.</p> <pre><code>import jumanji.wrappers\n\nenv = jumanji.make(\"Snake-6x6-v0\")\ngym_env = jumanji.wrappers.JumanjiToGymWrapper(env)\n\nobs, info = gym_env.reset()\naction = gym_env.action_space.sample()\nobservation, reward, term, trunc, info = gym_env.step(action)\n...\n</code></pre>"},{"location":"guides/wrappers/#auto-reset-an-environment","title":"Auto-reset an Environment","text":"<p>Below is an example of how to extend the functionality of the Snake environment to automatically reset whenever the environment reaches a terminal state. The Snake game terminates when the snake hits the wall, using the <code>AutoResetWrapper</code> the environment will be reset once a terminal state has been reached.</p> <pre><code>import jax.random\n\nimport jumanji.wrappers\n\nenv = jumanji.make(\"Snake-6x6-v0\")\nenv = jumanji.wrappers.AutoResetWrapper(env)\n\nkey = jax.random.PRNGKey(0)\nstate, timestep = env.reset(key)\nprint(\"New episode\")\nfor i in range(100):\n    action = env.action_spec.generate_value()  # Returns jnp.array(0) when using Snake.\n    state, timestep = env.step(state, action)\n    if timestep.first():\n        print(\"New episode\")\n</code></pre>"}]}